{
  "hash": "1ac13bd12f51f9d33e3617d620ce51bd",
  "result": {
    "engine": "knitr",
    "markdown": "# La regressione lineare bivariata: un approccio frequentista {#sec-linear-models-biv-model-frequentist}\n\n::: callout-note\n## In questo capitolo imparerai a:\n\n- comprendere il funzionamento del modello lineare secondo l’approccio frequentista;\n- stimare i coefficienti del modello utilizzando il metodo dei minimi quadrati e interpretarli correttamente;\n- valutare la qualità del modello attraverso l’indice di determinazione ($R^2$).\n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Consulta l'appendice @sec-apx-lin-func per un'introduzione alle funzioni lineari.\n:::\n\n::: callout-important\n## Preparazione del Notebook\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(broom)\n```\n:::\n\n\n\n:::\n\n## Introduzione\n\nI modelli lineari sono stati impiegati in molteplici contesti per lungo tempo. Come descritto da @stigler1986, il metodo dei minimi quadrati, una tecnica per adattare una regressione lineare bivariata, veniva già utilizzato nel XVIII secolo per affrontare problemi di analisi dei dati in astronomia. Ad esempio, questo metodo era impiegato per determinare il moto della Luna e per riconciliare i movimenti non periodici di Giove e Saturno. All'epoca, gli astronomi erano tra i primi a sentirsi a proprio agio nell'uso di tali metodi, poiché raccoglievano personalmente le loro osservazioni e sapevano che le condizioni di raccolta dei dati erano omogenee, anche se i valori osservati potevano differire. Questo contrastava con l'approccio più cauto delle scienze sociali, dove la riluttanza a combinare dati eterogenei ritardava l'adozione dei modelli lineari [@stigler1986].\n\nIn questo capitolo verrà introdotto il modello di regressione lineare bivariata secondo l'approccio frequentista. Questo modello consente di predire una variabile continua $y$ a partire da un unico predittore continuo $x$, utilizzando una relazione lineare. La relazione tra le due variabili è descritta dall'equazione della retta di regressione:\n\n$$\ny_i = a + b x_i + e_i, \\quad i = 1, \\dots, n,\n$$\n\ndove $a$ rappresenta l'intercetta, $b$ la pendenza della retta (coefficiente di regressione) e $e_i$ l'errore residuo per ciascuna osservazione.\n\nSaranno illustrati i seguenti aspetti:\n\n1. **Stima dei coefficienti di regressione**: Come calcolare $a$ e $b$.\n2. **Interpretazione dei coefficienti**: Il significato pratico di $a$ e $b$ nella descrizione della relazione tra $x$ e $y$.\n3. **Valutazione del modello**: Come misurare la bontà di adattamento del modello ai dati osservati, attraverso indicatori come l'indice di determinazione ($R^2$) e l'analisi dei residui.\n\n## La Predizione dell'Intelligenza\n\nNella presente discussione, esamineremo i dati `kidiq` che consistono in una raccolta di dati provenienti da una survey su donne adulte americane e i loro figli, selezionati da un sotto-campione del National Longitudinal Survey of Youth [@gelman2021regression]. \n\nNello specifico, ci concentreremo sulla relazione tra il punteggio di intelligenza del bambino (`kid_score`) e quello della madre (`mom_iq`). Ci proponiamo di valutare se e in quale misura l'intelligenza della madre possa prevedere l'intelligenza del bambino. Per fare ciò, inizieremo ad importare i dati nell'ambiente R.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Caricamento dei dati\nkidiq <- rio::import(here::here(\"data\", \"kidiq.dta\"))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Anteprima dei dati\nhead(kidiq)\n#>   kid_score mom_hs mom_iq mom_work mom_age\n#> 1        65      1  121.1        4      27\n#> 2        98      1   89.4        4      25\n#> 3        85      1  115.4        4      27\n#> 4        83      1   99.4        3      25\n#> 5       115      1   92.7        4      27\n#> 6        98      0  107.9        1      18\n```\n:::\n\n\n\n\nUn diagramma a dispersione per i dati di questo campione suggerisce la presenza di un'associazione positiva tra l'intelligenza del bambino (`kid_score`) e l'intelligenza della madre (`mom_iq`).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(kidiq, aes(x = mom_iq, y = kid_score)) +\n  geom_point(alpha = 0.4) +\n  labs(x = \"QI della madre\", y = \"QI del bambino\") +\n  ggtitle(\"Diagramma a dispersione\")\n```\n\n::: {.cell-output-display}\n![](01_lin_mod_frequentist_files/figure-html/unnamed-chunk-4-1.png){width=576}\n:::\n:::\n\n\n\n\n## Stima del modello di regressione lineare\n\nCalcoliamo i coefficienti della retta di regressione utilizzando la funzione `lm`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modello di regressione lineare\nmod <- lm(kid_score ~ mom_iq, data = kidiq)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Coefficienti stimati\ncoef(mod)\n#> (Intercept)      mom_iq \n#>       25.80        0.61\n```\n:::\n\n\n\n\nCi sono però infinite rette che, in linea di principio, possono essere usate per \"approssimare\" la nube di punti nel diagramma a dispersione. È dunque necessario introdurre dei vincoli per selezionare una di queste possibili rette. Il vincolo che viene introdotto dal modello di regressione è quello di costringere la retta a passare per il punto $(\\bar{x}, \\bar{y})$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(kidiq, aes(x = mom_iq, y = kid_score)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"QI della madre\", y = \"QI del bambino\") +\n  ggtitle(\"Retta di regressione\")\n#> `geom_smooth()` using formula = 'y ~ x'\n```\n\n::: {.cell-output-display}\n![](01_lin_mod_frequentist_files/figure-html/unnamed-chunk-7-1.png){width=576}\n:::\n:::\n\n\n\n\nUna retta di regressione che passa per il punto medio $(\\bar{x}, \\bar{y})$ (che rappresenta il centro di massa dei dati) è preferibile dal punto di vista statistico poiché minimizza la somma dei quadrati degli errori residui.\n\nIl campione è costituito da $n$ coppie di osservazioni ($x, y$). Per ciascuna coppia di valori $x_i, y_i$, il modello di regressione si aspetta che il valore $y_i$ sia associato al corrispondente valore $x_i$ come indicato dalla seguente equazione:\n\n$$\n\\begin{equation}\n\\mathbb{E}(y_i) = a + b x_i .\n\\end{equation}\n$$\n\nI valori $y_i$ corrispondono, nell'esempio che stiamo discutendo, alla variabile `kid_score`. I primi 10 valori della variabile $y$ sono i seguenti:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkidiq$kid_score[0:10]\n#>  [1]  65  98  85  83 115  98  69 106 102  95\n```\n:::\n\n\n\n\nPer fare riferimento a ciascuna osservazione usiamo l'indice $i$. Quindi, ad esempio, $y_2$ è uguale a\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkidiq$kid_score[2]\n#> [1] 98\n```\n:::\n\n\n\n\nIl modello di regressione lineare bivariata, rappresentato dall'equazione $y_i = a + b x_i + e_i$, descrive la relazione tra le variabili $x$ e $y$, dove $y$ è la variabile dipendente (nel nostro esempio, la variabile `kid_score`) e $x$ è la variabile indipendente (nel nostro esempio, la variabile `mom_iq`). Il valore di $y$ è la somma di due componenti: la componente deterministica, $\\hat{y}_i = a + b x_i$, e la componente aleatoria, $e_i$. La componente deterministica rappresenta la porzione della $y$ che è prevedibile conoscendo il valore di $x$, mentre la componente aleatoria rappresenta la porzione della $y$ che non è prevedibile dal modello.\n\nIl modello lineare cerca di trovare i coefficienti $a$ e $b$ che permettono di prevedere la componente deterministica di $y$ conoscendo il valore di $x$. Tuttavia, poiché la retta è solo un'approssimazione della relazione tra $x$ e $y$, la componente deterministica rappresenta solo una stima approssimata della vera relazione tra le due variabili.\n\nPer valutare l'accuratezza del modello di regressione lineare, è necessario calcolare il residuo $e_i = y_i - (a + b x_i)$, ovvero la differenza tra il valore osservato di $y$ e il valore previsto dal modello, $\\hat{y}$. La dimensione del residuo indica quanto la componente aleatoria contribuisce al valore osservato di $y$.\n\nIl modello di regressione lineare ha tre obiettivi [@fox2015applied]: \n\n1. il primo è quello di trovare i coefficienti $a$ e $b$ che permettono di prevedere la componente deterministica di $y$ conoscendo il valore di $x$; \n2. il secondo obiettivo è quello di valutare l'accuratezza della predizione fornita dal modello di regressione lineare;\n3. infine, il terzo obiettivo è quello dell'inferenza, ovvero quello di capire quali relazioni esistono tra la relazione tra $x$ e $y$ osservata nel campione e la relazione tra le due variabili nella popolazione.\n\n## Stima dei coefficienti di regressione\n\nIn breve, stiamo cercando di descrivere una relazione tra due variabili, il QI della madre e il QI del bambino, utilizzando un modello di regressione lineare. L'equazione lineare che descrive la relazione tra le due variabili è della forma $\\hat{y}_i = a_i + bx_i$, dove $\\hat{y}_i$ rappresenta la previsione per il QI del bambino $i$-esimo, $a_i$ e $b$ sono i coefficienti di regressione che vogliamo trovare e $x_i$ è il QI della madre del bambino $i$-esimo.\n\nPer trovare i coefficienti di regressione, dobbiamo introdurre dei vincoli per limitare lo spazio delle possibili soluzioni. Il primo vincolo è che la retta di regressione deve passare per il baricentro del grafico a dispersione. Il secondo vincolo è che vogliamo minimizzare la somma dei quadrati dei residui, ovvero la differenza tra il valore osservato e il valore previsto dal modello. I coefficienti di regressione che soddisfano questi vincoli si chiamano **coefficienti dei minimi quadrati**.\n\nIl problema di trovare i coefficienti di regressione $a$ e $b$ che minimizzano la somma dei quadrati dei residui ha una soluzione analitica. Questa soluzione si ottiene trovando il punto di minimo di una superficie tridimensionale che rappresenta la somma dei quadrati dei residui. Il punto di minimo è quello per cui il piano tangente alla superficie nelle due direzioni $a$ e $b$ è piatto, cioè le derivate parziali rispetto ad $a$ e $b$ sono uguali a zero. In pratica, ciò significa risolvere un sistema di equazioni lineari con due incognite $a$ e $b$, noto come equazioni normali.\n\nLa soluzione delle equazioni normali ci fornisce i coefficienti di regressione stimati, che minimizzano la somma dei quadrati dei residui.  La formula per il coefficiente $a$ è\n\n$$\na = \\bar{y} - b \\bar{x}.\n$$\n\nLa formula per il coefficiente $b$ è\n\n$$\nb = \\frac{Cov(x, y)}{Var(x)},\n$$\n\ndove $\\bar{x}$ e $\\bar{y}$ sono le medie delle variabili $x$ e $y$, $Cov(x,y)$ è la covarianza tra $x$ e $y$ e $Var(x)$ è la varianza di $x$.\n\nQueste equazioni rappresentano la stima dei minimi quadrati dei coefficienti di regressione che ci permettono di trovare la retta che minimizza la somma dei quadrati dei residui.\n\n### Calcolo manuale dei coefficienti di regressione\n\nPer calcolare i coefficienti di regressione $a$ (intercetta) e $b$ (pendenza), utilizziamo la covarianza e la varianza:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calcolo manuale dei coefficienti\ncov_xy <- cov(kidiq$kid_score, kidiq$mom_iq)  # Covarianza tra le due variabili\nvar_x <- var(kidiq$mom_iq)  # Varianza della variabile indipendente\nb <- cov_xy / var_x  # Pendenza (coefficiente b)\nb\n#> [1] 0.61\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Intercetta (coefficiente a)\na <- mean(kidiq$kid_score) - b * mean(kidiq$mom_iq)\na\n#> [1] 25.8\n```\n:::\n\n\n\n\n### Interpretazione\n\nIl coefficiente $a$ indica l'intercetta della retta di regressione nel diagramma a dispersione. Questo valore rappresenta il punto in cui la retta di regressione interseca l'asse $y$ del sistema di assi cartesiani. Tuttavia, in questo caso specifico, il valore di $a$ non è di particolare interesse poiché corrisponde al valore della retta di regressione quando l'intelligenza della madre è pari a 0, il che non ha senso nella situazione reale. Successivamente, vedremo come è possibile trasformare i dati per fornire un'interpretazione utile del coefficiente $a$.\n\nInvece, il coefficiente $b$ indica la pendenza della retta di regressione, ovvero di quanto aumenta (se $b$ è positivo) o diminuisce (se $b$ è negativo) la retta di regressione in corrispondenza di un aumento di 1 punto della variabile $x$. Nel caso specifico del QI delle madri e dei loro figli, il coefficiente $b$ ci indica che un aumento di 1 punto del QI delle madri è associato, in media, a un aumento di 0.61 punti del QI dei loro figli.\n\nIn pratica, il modello di regressione lineare cerca di prevedere le medie dei punteggi del QI dei figli in base al QI delle madri. Ciò significa che non è in grado di prevedere esattamente il punteggio di ciascun bambino in funzione del QI della madre, ma solo una stima della media dei punteggi dei figli quando il QI delle madri aumenta o diminuisce di un punto.\n\nIl coefficiente $b$ ci dice di quanto aumenta (o diminuisce) in media il QI dei figli per ogni unità di aumento (o diminuzione) del QI della madre. Nel nostro caso, se il QI della madre aumenta di un punto, il QI dei figli aumenta in media di 0.61 punti.\n\nÈ importante comprendere che il modello statistico di regressione lineare non è in grado di prevedere il valore preciso di ogni singolo bambino, ma solo una stima della media dei punteggi del QI dei figli quando il QI delle madri aumenta o diminuisce. Questa stima è basata su una distribuzione di valori possibili che si chiama distribuzione condizionata $p(y \\mid x_i)$.\n\nUna rappresentazione grafica del valore predetto dal modello di regressione, $\\hat{y}_i = a + bx_i$ è stato fornito in precedenza. Il diagramma presenta ciascun valore $\\hat{y}_i = a + b x_i$ in funzione di $x_i$. I valori predetti dal modello di regressione sono i punti che stanno sulla retta di regressione.\n\n## Residui \n\nIn precedenza abbiamo detto che il residuo, ovvero la componente di ciascuna osservazione $y_i$ che non viene predetta dal modello di regressione, corrisponde alla *distanza verticale* tra il valore $y_i$ osservato e il valore $\\hat{y}_i$ predetto dal modello di regressione:\n\n$$\ne_i = y_i - (a + b x_i).\n$$\n\nPer fare un esempio numerico, consideriamo il punteggio osservato del QI del primo bambino.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkidiq$kid_score[1]\n#> [1] 65\n```\n:::\n\n\n\n\nIl QI della madre è\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkidiq$mom_iq[1]\n#> [1] 121\n```\n:::\n\n\n\n\nPer questo bambino, il valore predetto dal modello di regressione è\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\na + b * kidiq$mom_iq[1]\n#> [1] 99.7\n```\n:::\n\n\n\n\nL'errore che compiamo per predire il QI del bambino utilizzando il modello di regressione (ovvero, il residuo) è\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkidiq$kid_score[1] - (a + b * kidiq$mom_iq[1])\n#> [1] -34.7\n```\n:::\n\n\n\n\nPer tutte le osservazioni abbiamo\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres <- kidiq$kid_score - (a + b * kidiq$mom_iq)\n```\n:::\n\n\n\n\nÈ una proprietà del modello di regressione (calcolato con il metodo dei minimi quadrati) che la somma dei residui sia uguale a zero.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(res)\n#> [1] 1.44e-11\n```\n:::\n\n\n\n\nQuesto significa che ogni valore osservato $y_i$ viene scomposto dal modello di regressione in due componenti distinte. La componente deterministica $\\hat{y}_i$, che è predicibile da $x_i$, è data da $\\hat{y}_i = a + b x_i$. Il residuo, invece, è dato da $e_i = y_i - \\hat{y}_i$. La somma di queste due componenti, ovviamente, riproduce il valore osservato.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Creazione di un data frame con i valori calcolati\ndf <- data.frame(\n  kid_score = kidiq$kid_score,\n  mom_iq = kidiq$mom_iq,\n  y_hat = a + b * kidiq$mom_iq,\n  e = kidiq$kid_score - (a + b * kidiq$mom_iq),\n  y_hat_plus_e = (a + b * kidiq$mom_iq) + (kidiq$kid_score - (a + b * kidiq$mom_iq))\n)\n\n# Visualizzazione dei primi 6 valori\nhead(df)\n#>   kid_score mom_iq y_hat      e y_hat_plus_e\n#> 1        65  121.1  99.7 -34.68           65\n#> 2        98   89.4  80.3  17.69           98\n#> 3        85  115.4  96.2 -11.22           85\n#> 4        83   99.4  86.5  -3.46           83\n#> 5       115   92.7  82.4  32.63          115\n#> 6        98  107.9  91.6   6.38           98\n```\n:::\n\n\n\n\n## Trasformazione dei dati\n\nIn generale, per variabili a livello di scala ad intervalli, l'intercetta del modello di regressione lineare non ha un'interpretazione utile. Questo perché l'intercetta indica il valore atteso di $y$ quando $x = 0$, ma in caso di variabili a scala di intervalli, il valore \"0\" di $x$ è arbitrario e non corrisponde ad un \"assenza\" della variabile $x$. Ad esempio, un QI della madre pari a 0 non indica un'assenza di intelligenza, ma solo un valore arbitrario del test usato per misurare il QI. Quindi, sapere il valore medio del QI dei bambini quando il QI della madre è 0 non è di alcun interesse.\n\nPer fornire all'intercetta del modello di regressione un'interpretazione più utile, dobbiamo trasformare le osservazioni di $x$. Per esempio, esprimiamo $x$ come differenza dalla media. Chiamiamo questa nuova variabile $xd$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkidiq$xd <- kidiq$mom_iq - mean(kidiq$mom_iq)\n\nkidiq |> \n  head()\n#>   kid_score mom_hs mom_iq mom_work mom_age     xd\n#> 1        65      1  121.1        4      27  21.12\n#> 2        98      1   89.4        4      25 -10.64\n#> 3        85      1  115.4        4      27  15.44\n#> 4        83      1   99.4        3      25  -0.55\n#> 5       115      1   92.7        4      27  -7.25\n#> 6        98      0  107.9        1      18   7.90\n```\n:::\n\n\n\n\nSe ora usiamo le coppie di osservazioni $(xd_i, y_i)$, il diagramma a dispersione assume la forma seguente.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Aggiungiamo una nuova variabile centrata (scarti dalla media)\nkidiq <- kidiq %>%\n  mutate(xd = mom_iq - mean(mom_iq))\n\n# Calcolo della retta di regressione\nb <- cov(kidiq$xd, kidiq$kid_score) / var(kidiq$xd)\na <- mean(kidiq$kid_score) - b * mean(kidiq$xd)\n\n# Grafico con ggplot2\nggplot(kidiq, aes(x = xd, y = kid_score)) +\n  geom_point(alpha = 0.4) +  # Punti del grafico\n  geom_abline(intercept = a, slope = b, color = \"blue\") +  # Retta di regressione\n  labs(\n    x = \"QI della madre (scarti dalla media)\", \n    y = \"QI del bambino\"\n  ) +\n  ggtitle(\"Retta di regressione sui dati centrati\")\n\n```\n\n::: {.cell-output-display}\n![](01_lin_mod_frequentist_files/figure-html/unnamed-chunk-20-1.png){width=576}\n:::\n:::\n\n\n\n\nIn pratica, abbiamo spostato tutti i punti del grafico lungo l'asse delle $x$, in modo tale che la media dei valori di $x$ sia uguale a 0. Questo non ha cambiato la forma dei punti nel grafico, ma ha solo spostato l'origine dell'asse $x$. La pendenza della linea di regressione tra $x$ e $y$ rimane la stessa, sia per i dati originali che per quelli trasformati. L'unica cosa che cambia è il valore dell'intercetta della linea di regressione, che ora ha un'interpretazione più significativa.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfm1 <- lm(kid_score ~ xd, data = kidiq)\ncoef(fm1)\n#> (Intercept)          xd \n#>       86.80        0.61\n```\n:::\n\n\n\n\nL'intercetta rappresenta il punto in cui la retta di regressione incontra l'asse $y$ nel diagramma a dispersione. Nel caso dei dati trasformati, abbiamo spostato la nube di punti lungo l'asse $x$ di una quantità pari a $x - \\bar{x}$, ma le relazioni spaziali tra i punti rimangono invariate. Pertanto, la pendenza della retta di regressione non cambia rispetto ai dati non trasformati. Tuttavia, il valore dell'intercetta viene influenzato dalla trasformazione. In particolare, poiché $xd = 0$ corrisponde a $x = \\bar{x}$ nei dati grezzi, l'intercetta del modello di regressione lineare calcolata sui dati trasformati corrisponde al valore atteso di $y$ quando $x$ assume il valore medio sulla scala dei dati grezzi. In altre parole, l'intercetta del modello di regressione lineare sui dati trasformati rappresenta il valore atteso del QI dei bambini corrispondente al QI medio delle madri.\n\n## Il metodo dei minimi quadrati\n\nPer stimare i coefficienti $a$ e $b$, possiamo minimizzare la somma dei quadrati dei residui tra i valori osservati $y_i$ e quelli previsti $a + b x_i$.\n\nIniziamo con il creare una griglia per i valori di $b$. Supponiamo che il valore di $a$ sia noto ($a = 25.79978$). Usiamo R per creare una griglia di valori possibili per $b$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Griglia di valori per b\nb_grid <- seq(0, 1, length.out = 1001)\na <- 25.79978  # Intercetta nota\n```\n:::\n\n\n\n\nDefiniamo ora una funzione che calcola la somma dei quadrati dei residui ($SSE$) per ciascun valore di $b$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Funzione per la somma dei quadrati dei residui\nsse <- function(a, b, x, y) {\n  sum((y - (a + b * x))^2)\n}\n```\n:::\n\n\n\n\nApplichiamo la funzione `sse` alla griglia di valori $b$ per calcolare la somma dei quadrati dei residui per ogni valore di $b$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calcolo di SSE per ciascun valore di b\nsse_vals <- sapply(b_grid, function(b) sse(a, b, kidiq$mom_iq, kidiq$kid_score))\n```\n:::\n\n\n\n\nTracciamo un grafico che mostra la somma dei quadrati dei residui ($SSE$) in funzione dei valori di $b$, evidenziando il minimo.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identificazione del valore di b che minimizza SSE\nb_min <- b_grid[which.min(sse_vals)]\n\n# Grafico\nplot(\n  b_grid, sse_vals, type = \"l\", col = \"blue\", lwd = 2,\n  xlab = expression(paste(\"Possibili valori di \", hat(beta))),\n  ylab = \"Somma dei quadrati dei residui (SSE)\",\n  main = \"Minimizzazione dei residui quadratici\"\n)\npoints(b_min, min(sse_vals), pch = 19, col = \"red\")\nlegend(\"topright\", legend = expression(hat(beta)), col = \"red\", pch = 19)\n```\n\n::: {.cell-output-display}\n![](01_lin_mod_frequentist_files/figure-html/unnamed-chunk-25-1.png){width=576}\n:::\n:::\n\n\n\n\nInfine, identifichiamo il valore di $b$ che minimizza la somma dei quadrati dei residui.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb_min\n#> [1] 0.61\n```\n:::\n\n\n\n\nCon questa simulazione, abbiamo stimato il coefficiente $b$ minimizzando la somma dei quadrati dei residui. Questo approccio può essere esteso per stimare simultaneamente entrambi i coefficienti ($a$ e $b$) utilizzando metodi di ottimizzazione più avanzati, come `optim` in R.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptim_result <- optim(\n  par = c(a = 25, b = 0.5),  # Valori iniziali\n  fn = function(params) {\n    a <- params[1]\n    b <- params[2]\n    sse(a, b, kidiq$mom_iq, kidiq$kid_score)\n  }\n)\n\n# Coefficienti stimati\noptim_result$par\n#>     a     b \n#> 25.79  0.61\n```\n:::\n\n\n\n\nQuesta simulazione illustra come, tramite il metodo dei minimi quadrati, sia possibile stimare i parametri di un modello bivariato di regressione.\n\n## L'errore standard della regressione\n\nIl secondo obiettivo del modello di regressione lineare è quello di misurare quanto della variabilità di $y$ possa essere spiegata dalla variabilità di $x$ per ogni osservazione. L'indice di bontà di adattamento del modello viene fornito dalla deviazione standard dei residui, chiamata anche \"errore standard della stima\" (o errore standard della regressione), $s_e$. Per calcolare $s_e$, si sommano i quadrati dei residui $e_i$ per ogni osservazione e si divide per $n-2$, dove $n$ rappresenta la numerosità del campione e $2$ il numero di coefficienti stimati nel modello di regressione. Si prende poi la radice quadrata del risultato. L'indice $s_e$ possiede la stessa unità di misura di $y$ ed è una stima della deviazione standard dei residui nella popolazione.\n\nIllustriamo il calcolo di $s_e$ con i dati a disposizione. I residui $e$ possono essere calcolati sottraendo ai valori osservati $y_i$ i valori predetti dal modello $a + b x_i$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calcolo dei residui\ne <- kidiq$kid_score - (a + b * kidiq$mom_iq)\n\n# Mostriamo i primi 10 residui\nhead(e, 10)\n#>  [1] -34.68  17.69 -11.22  -3.46  32.63   6.38 -41.52   3.86  26.41  11.21\n```\n:::\n\n\n\n\nCalcoliamo il valore medio assoluto dei residui per avere un'indicazione della deviazione media rispetto alla retta di regressione.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Media assoluta dei residui\nmean(abs(e))\n#> [1] 14.5\n```\n:::\n\n\n\n\nL'errore standard della stima $s_e$ si calcola come la radice quadrata della somma dei quadrati dei residui divisa per $n-2$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calcolo di s_e\nse <- sqrt(sum(e^2) / (length(e) - 2))\nse\n#> [1] 18.3\n```\n:::\n\n\n\n\nNotiamo che il valore medio assoluto dei residui e l'errore standard $s_e$ non sono identici, ma hanno lo stesso ordine di grandezza. $s_e$ è una misura più rigorosa della deviazione standard dei residui. \n\nQuesta analisi dimostra come $s_e$ consenta di valutare quanto le previsioni del modello si discostino (in media) dai dati osservati.\n\n### Sottostima dell'Errore nel Modello di Regressione\n\nCome discusso da @gelman2021regression, la **radice quadrata media dei residui**, calcolata come \n\n$$\n\\sqrt{\\frac{1}{n} \\sum_{i=1}^n \\big(y_i - (\\hat{a} + \\hat{b}x_i)\\big)^2},\n$$\n\ntende a sottostimare la deviazione standard $\\sigma$ dell'errore nel modello di regressione. Questa sottostima è dovuta al fenomeno del **sovradimensionamento**, dato che i parametri $a$ e $b$ sono stimati utilizzando gli stessi $n$ punti dati su cui vengono calcolati i residui. In altre parole, i residui non sono del tutto indipendenti dal modello.\n\nUn approccio alternativo per valutare l'errore predittivo e mitigare il problema del sovradimensionamento è la **validazione incrociata**. In particolare, l'approccio leave-one-out (LOOCV) offre una soluzione semplice ed efficace. Questo metodo consiste nell'adattare il modello $n$ volte, escludendo ogni volta un punto dati, adattando il modello ai rimanenti $n-1$ punti, e utilizzando tale modello per predire l'osservazione esclusa.\n\n#### Procedura Leave-One-Out:\n\n- Per $i = 1, \\ldots, n$:\n  1. Adatta il modello $y = a + bx + \\text{errore}$ ai $n-1$ punti dati $(x, y)_j, j \\neq i$. Denomina i coefficienti stimati come $\\hat{a}_{-i}$ e $\\hat{b}_{-i}$.\n  2. Calcola il residuo validato incrociato:\n  \n     $$\n     r_{\\text{CV}} = y_i - (\\hat{a}_{-i} + \\hat{b}_{-i} x_i).\n     $$\n     \n  3. Salva il residuo al quadrato per il calcolo successivo.\n\n- Calcola infine la stima di $\\sigma_{\\text{CV}}$ come:\n\n  $$\n  \\sigma_{\\text{CV}} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n r_{\\text{CV}}^2}.\n  $$\n\n#### Applicazione Pratica:\n\nEcco un esempio applicato al modello che predice l'intelligenza del bambino ($\\texttt{kid\\_score}$) in funzione dell'intelligenza della madre ($\\texttt{mom\\_iq}$) utilizzando il dataset `kidiq`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(round = 5)\n\n# Array per salvare i residui validati incrociati\nresiduals_cv <- numeric(nrow(kidiq))\n\n# Loop per la validazione incrociata leave-one-out\nfor (i in 1:nrow(kidiq)) {\n  # Dati di training escludendo l'i-esimo punto\n  train_data <- kidiq[-i, ]\n  test_data <- kidiq[i, ]\n  \n  # Addestramento del modello\n  model <- lm(kid_score ~ mom_iq, data = train_data)\n  \n  # Predizione sull'i-esimo punto\n  y_pred <- predict(model, newdata = test_data)\n  \n  # Calcolo del residuo validato incrociato\n  residual_cv <- test_data$kid_score - y_pred\n  residuals_cv[i] <- residual_cv^2\n}\n\n# Calcolo di sigma_cv\nsigma_cv <- sqrt(mean(residuals_cv))\n\ncat(\"Stima di σ_CV:\", sigma_cv, \"\\n\")\n#> Stima di σ_CV: 18.3\n```\n:::\n\n\n\n\n#### Confronto con la Stima Tradizionale\n\nCalcoliamo la stima tradizionale di $\\sigma$ utilizzando il modello completo e confrontiamola con $\\sigma_{\\text{CV}}$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modello completo\nfm2 <- lm(kid_score ~ mom_iq, data = kidiq)\n\n# Stima tradizionale dell'errore standard della regressione\nres <- summary(fm2)\ncat(\"Errore standard della regressione (tradizionale):\", res$sigma, \"\\n\")\n#> Errore standard della regressione (tradizionale): 18.3\n```\n:::\n\n\n\n\nLa stima tradizionale di $\\sigma$ si basa sulla seguente formula:\n\n$$\n\\hat{\\sigma}_e = \\sqrt{\\frac{\\sum_{i=1}^n (e_i - \\bar{e})^2}{n - p}},\n$$\n\ndove $e_i$ sono i residui del modello e $p = 2$ è il numero di parametri stimati ($a$ e $b$).\n\n#### Interpretazione\n\nNel caso analizzato, i valori stimati di $\\sigma_{\\text{CV}}$ e $\\hat{\\sigma}_e$ tradizionale possono risultare molto simili. Tuttavia, in generale, la stima di $\\sigma_{\\text{CV}}$ tende a essere leggermente superiore, in quanto riflette meglio l'errore predittivo su dati non utilizzati per adattare il modello. Questo rende $\\sigma_{\\text{CV}}$ una misura più robusta e conservativa dell'incertezza del modello.\n\nLa validazione incrociata, e in particolare l'approccio LOOCV, rappresenta uno strumento importante per valutare le performance predittive di un modello di regressione e per ottenere stime più affidabili della deviazione standard dell'errore.\n\n## Indice di determinazione\n\nUn importante risultato dell'analisi di regressione riguarda la scomposizione della varianza della variabile dipendente $y$ in due componenti: la varianza spiegata dal modello e la varianza residua. Questa scomposizione è descritta mediante l'indice di determinazione $R^2$, che fornisce una misura della bontà di adattamento del modello ai dati del campione. \n\nPer una generica osservazione $x_i, y_i$, la deviazione di $y_i$ rispetto alla media $\\bar{y}$ può essere espressa come la somma di due componenti: il residuo $e_i=y_i- \\hat{y}_i$ e lo scarto di $\\hat{y}_i$ rispetto alla media $\\bar{y}$:\n\n$$\ny_i - \\bar{y} = (y_i- \\hat{y}_i) + (\\hat{y}_i - \\bar{y}) = e_i + (\\hat{y}_i - \\bar{y}).\n$$\n\nLa varianza totale di $y$ può quindi essere scritta come:\n\n$$\n\\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}(e_i + (\\hat{y}_i - \\bar{y}))^2.\n$$\n\nSviluppando il quadrato e sommando, si ottiene:\n\n$$\n\\sum_{i=1}^{n}(y_i - \\bar{y})^2 = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 + \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2.\n$$\n\nIl primo termine rappresenta la varianza residua, mentre il secondo termine rappresenta la varianza spiegata dal modello. L'indice di determinazione $R^2$ è definito come il rapporto tra la varianza spiegata e la varianza totale:\n\n$$\nR^2 = \\frac{\\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}.\n$$\n\nQuesto indice varia tra 0 e 1 e indica la frazione di varianza totale di $y$ spiegata dal modello di regressione lineare. Un valore alto di $R^2$ indica che il modello di regressione lineare si adatta bene ai dati, in quanto una grande parte della varianza di $y$ è spiegata dalla variabile indipendente $x$.\n\nPer l'esempio in discussione, possiamo calcolare la devianza totale, la devianza spiegata e l'indice di determinazione $R^2$ come segue:\n\nLa devianza totale misura la variabilità complessiva dei punteggi osservati $y$ rispetto alla loro media:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Devianza totale\ndev_t <- sum((kidiq$kid_score - mean(kidiq$kid_score))^2)\ndev_t\n#> [1] 180386\n```\n:::\n\n\n\n\nLa devianza spiegata misura la variabilità che il modello è in grado di spiegare, considerando i valori predetti $a + b x$:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Devianza spiegata\ndev_r <- sum(((a + b * kidiq$mom_iq) - mean(kidiq$kid_score))^2)\ndev_r\n#> [1] 36249\n```\n:::\n\n\n\n\nL'indice $R^2$ è il rapporto tra la devianza spiegata e la devianza totale, e indica la frazione della variabilità totale che è spiegata dal modello di regressione:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Indice di determinazione\nR2 <- dev_r / dev_t\nround(R2, 3)\n#> [1] 0.201\n```\n:::\n\n\n\n\nPer verificare i calcoli, utilizziamo il modello di regressione lineare in R e leggiamo $R^2$ direttamente dal sommario del modello:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Modello di regressione lineare\nmod <- lm(kid_score ~ mom_iq, data = kidiq)\n\n# Sommario del modello per leggere R^2\nsummary(mod)$r.squared\n#> [1] 0.201\n```\n:::\n\n\n\n\nIl risultato mostra che circa il 20% della variabilità nei punteggi del QI dei bambini è spiegabile conoscendo il QI delle madri. Questo significa che il modello cattura una porzione rilevante della relazione, ma lascia anche spazio a fattori non inclusi nel modello che influenzano il QI dei bambini.\n\n## Simulazione di Livelli di Copertura\n\nVerifichiamo la copertura degli intervalli di credibilità al 95% attraverso simulazioni ripetute.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n# Parametri veri\na_true <- 0.2\nb_true <- 0.3\nsigma_true <- 0.5\n# Numero di simulazioni\nnum_simulations <- 1000\n# Conteggio delle coperture\ncoverage_a <- 0\ncoverage_b <- 0\nfor (i in 1:num_simulations) {\n  # Generazione dei dati\n  x <- 1:20\n  y <- a_true + b_true * x + sigma_true * rnorm(length(x))\n  # Adattamento del modello\n  fit <- lm(y ~ x)\n  ci <- confint(fit) # Intervalli di confidenza\n  # Verifica delle coperture\n  if (ci[1,1] <= a_true & ci[1, 2] >= a_true) {\n    coverage_a <- coverage_a + 1\n  }\n  if (ci[2,1] <= b_true & ci[2, 2] >= b_true) {\n    coverage_b <- coverage_b + 1\n  }\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Risultati\ncat(\"Coverage for a:\", coverage_a / num_simulations, \"\\n\")\n#> Coverage for a: 0.952\ncat(\"Coverage for b:\", coverage_b / num_simulations, \"\\n\")\n#> Coverage for b: 0.955\n```\n:::\n\n\n\n\nI risultati indicano che i livelli di copertura empirici ottenuti con l'approccio frequentista corrispondono strettamente ai livelli teorici attesi.\n\n### Confronti, non Effetti\n\n@gelman2021regression sottolineano che i coefficienti di regressione sono spesso denominati \"effetti\", ma questa terminologia può trarre in inganno. Gli \"effetti\", infatti, implicano una relazione causale. Tuttavia, ciò che un modello di regressione stima non è necessariamente un effetto causale, ma piuttosto un pattern osservazionale. In particolare, ciò che osserviamo è che la media della variabile dipendente nella sottopopolazione con $X = x + 1$ è spesso maggiore o minore (a seconda del segno di $\\beta$) rispetto alla media della sottopopolazione con $X = x$.\n\nLa regressione è uno strumento matematico utilizzato principalmente per fare previsioni. I coefficienti di regressione devono quindi essere interpretati come confronti medi. Solo in circostanze specifiche, quando la regressione descrive un processo causale ben definito, è possibile interpretarli come effetti. Tuttavia, questa interpretazione causale deve essere giustificata dal disegno dello studio e non può essere dedotta unicamente dall'uso del modello statistico.\n\n## Inferenza\n\nUna delle preoccupazioni principali nell'analisi dei dati è la possibilità di giungere a conclusioni forti che non si replicano o che non riflettono le proprietà reali nella popolazione sottostante. Le teorie statistiche sono state sviluppate per quantificare tali possibilità nel contesto dell'inferenza e della presa di decisioni. Il terzo obiettivo del modello di regressione è dunque quello dell'inferenza [@caudek2001statistica]. \n\n### Significatività statistica\n\nUna regola decisionale ampiamente utilizzata, ma che gli statistici e le buone pratiche della metodologia corrente raccomandano di **non** usare, consiste nel considerare un risultato come “reale” o “stabile” se è \"statisticamente significativo\" e, viceversa, trattare risultati \"non significativi\" come rumorosi o inaffidabili.\n\nLa significatività statistica è convenzionalmente definita come un p-value inferiore a 0.05 rispetto a una certa ipotesi nulla o a un valore prespecificato che indichi l'assenza di un effetto, come discusso nel contesto del test delle ipotesi. Nel caso della regressione, questo corrisponde approssimativamente al fatto che i coefficienti siano considerati \"statisticamente significativi\" se sono lontani da zero di una quantità pari ad almeno due volte il loro errore standard; in caso contrario, sono considerati \"non significativi\". In termini più generali, una stima è definita come \"non statisticamente significativa\" se il valore osservato potrebbe essere ragionevolmente spiegato da una semplice variazione casuale.\n\n### Test delle ipotesi e pratica statistica\n\nÈ buona pratica **non utilizzare** i test di significatività per l’ipotesi nulla. In psicologia, così come le scienze sociali, non è ragionevole pensare che le ipotesi nulle possano essere realisticamente vere: quasi ogni trattamento o intervento che si può considerare avrà qualche effetto, e nessun coefficiente di regressione o confronto di interesse sarà esattamente pari a zero. Non è dunque particolarmente utile formulare e testare ipotesi nulle che sappiamo già in partenza essere false. I test delle ipotesi nulle diventano semplicemente una questione di raccolta dati: con un campione sufficientemente grande, qualsiasi ipotesi può essere rifiutata. Tuttavia, raccogliere enormi quantità di dati solo per respingere un’ipotesi che già non ritenevamo plausibile fin dall’inizio non ha molto senso.\n\nDetto ciò, non tutti gli effetti o i confronti sono rilevabili in un singolo studio. Quindi, pur non avendo mai come obiettivo di ricerca il rifiuto di un’ipotesi nulla, riconosciamo l’utilità di verificare la coerenza di un dataset con un modello nullo specifico. L’idea è che il non-rigetto dell’ipotesi nulla indichi che i dati non contengono abbastanza informazioni per andare oltre il modello nullo. Il punto del rigetto dell’ipotesi nulla non è dimostrare che quest’ultima sia falsa—in generale, non crediamo all’ipotesi nulla già prima di iniziare uno studio—ma piuttosto indicare che i dati contengono informazioni sufficienti per adattare un modello più complesso.\n\nUn uso dei test delle ipotesi che troviamo problematico è quando un ricercatore inizia con un’ipotesi scientifica (ad esempio, che un certo trattamento abbia un effetto positivo generale) e, per confermarla, formula un’ipotesi statistica nulla (ad esempio, che non ci sia alcuna correlazione tra il trattamento e l’esito). Si raccolgono dati che portano al rifiuto dell’ipotesi nulla, e questo viene interpretato come prova a favore dell’ipotesi scientifica iniziale. Il problema è che un’ipotesi statistica (come $\\beta = 0$ o $\\beta_1 = \\beta_2$) è molto più specifica di un’ipotesi scientifica. Dunque, il rigetto di un’ipotesi statistica non fornisce necessariamente informazioni utili sull’ipotesi scientifica. Inoltre, le violazioni delle assunzioni tecniche del modello statistico possono portare a un’elevata probabilità di rigettare l’ipotesi nulla, anche in assenza di un effetto reale. Quello che il rigetto può fare, invece, è motivare il passo successivo: costruire un modello per confrontare gli effetti di interesse.\n\n### Una Regola Euristica per l’Inferenza\n\nUn principio utile è che, se il rapporto tra la stima di un coefficiente e il suo errore standard è inferiore a 2, i dati probabilmente non contengono informazioni sufficienti per giustificare l'inclusione di quel coefficiente nel modello [@gelman2014beyond; @gelman2020regression]. Questa regola evita la dicotomia \"significativo/non significativo\" e incoraggia a valutare la forza del segnale rispetto al rumore nei dati.\n\n\n## Riflessioni Conclusive\n\nIn questo capitolo, abbiamo adottato una prospettiva frequentista per stimare i parametri di un modello di regressione bivariato. Come sottolineato da @alexander2023telling, i modelli statistici non sono strumenti per scoprire verità assolute, ma piuttosto mezzi per interpretare i dati a nostra disposizione e attribuire loro un senso. In altre parole, non dobbiamo intenderli come riproduzioni fedeli della realtà, ma come \"lenti\" attraverso cui focalizzare e comprendere alcuni aspetti del mondo che ci circonda.\n\nCome affermava Carlo Tullio Altan, una teoria (e dunque un modello) “mostra nella misura in cui nasconde”. Traslando questa idea al modello di regressione, possiamo dire che, quando descriviamo la variabile dipendente tramite un determinato insieme di predittori, mettiamo in risalto certi aspetti della realtà, lasciandone in ombra altri, i quali potrebbero emergere solo scegliendo predittori diversi. Nelle prossime sezioni approfondiremo il significato statistico di questo concetto, soffermandoci sul tema dell’errore di specificazione.\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered} \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.2\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] broom_1.0.7       viridis_0.6.5     viridisLite_0.4.2 gridExtra_2.3    \n#>  [5] patchwork_1.3.0   bayesplot_1.11.1  psych_2.4.6.26    scales_1.3.0     \n#>  [9] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#> [13] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n#> [17] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#> [21] rio_1.2.3         here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] utf8_1.2.4        generics_0.1.3    stringi_1.8.4     lattice_0.22-6   \n#>  [5] hms_1.1.3         digest_0.6.37     magrittr_2.0.3    evaluate_1.0.1   \n#>  [9] grid_4.4.2        timechange_0.3.0  fastmap_1.2.0     Matrix_1.7-1     \n#> [13] R.oo_1.27.0       rprojroot_2.0.4   jsonlite_1.8.9    R.utils_2.12.3   \n#> [17] backports_1.5.0   mgcv_1.9-1        fansi_1.0.6       mnormt_2.1.1     \n#> [21] cli_3.6.3         rlang_1.1.4       R.methodsS3_1.8.2 splines_4.4.2    \n#> [25] munsell_0.5.1     withr_3.0.2       tools_4.4.2       parallel_4.4.2   \n#> [29] tzdb_0.4.0        colorspace_2.1-1  pacman_0.5.1      vctrs_0.6.5      \n#> [33] R6_2.5.1          lifecycle_1.0.4   htmlwidgets_1.6.4 pkgconfig_2.0.3  \n#> [37] pillar_1.9.0      gtable_0.3.6      glue_1.8.0        haven_2.5.4      \n#> [41] xfun_0.49         tidyselect_1.2.1  farver_2.1.2      htmltools_0.5.8.1\n#> [45] nlme_3.1-166      labeling_0.4.3    rmarkdown_2.29    compiler_4.4.2\n```\n:::\n\n\n\n\n## Bibliografia {.unnumbered}\n\n",
    "supporting": [
      "01_lin_mod_frequentist_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}