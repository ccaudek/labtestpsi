{
  "hash": "f7919d494eedfab2569260bbd8d69479",
  "result": {
    "engine": "knitr",
    "markdown": "# Fondamenti della probabilità {#sec-prob-sigma-algebra}\n\n::: callout-important\n## In questo capitolo imparerai a:\n\n- capire la $\\sigma$-algebra e gli assiomi di Kolmogorov;\n- applicare le regole fondamentali della probabilità;\n- utilizzare elementi del calcolo combinatorio.\n:::\n\n::: callout-tip\n## Prerequisiti\n\n- Leggere il capitolo *Probability and counting* di **Introduction to Probability** [@blitzstein2019introduction]. \n- Leggere l'appendice sec-apx-combinatorics.\n:::\n\n::: callout-caution\n## Preparazione del Notebook\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(readr, lubridate, reshape2)\n```\n:::\n\n\n\n\n:::\n\n## Introduzione \n\nNel @sec-prob-spaces abbiamo introdotto la teoria della misura e della probabilità su insiemi con un numero finito di elementi. Tuttavia, molti degli spazi matematici che incontriamo nelle applicazioni pratiche, come gli interi e la retta reale, non hanno un numero finito di elementi, ma piuttosto un numero numerabile infinito o addirittura non numerabile infinito di elementi. Sfortunatamente, estendere la teoria della misura e della probabilità a spazi più generali come questi non è sempre semplice.\n\nSenza entrare nei dettagli, è stato dimostrato che la forma più generale della teoria della misura e della probabilità applicabile a qualsiasi spazio matematico è chiamata $\\sigma$-algebra. In questo capitolo, forniremo un'introduzione intuitiva ai vincoli delle $\\sigma$-algebre ed esamineremo alcune notevoli applicazioni. In particolare, introdurremo i concetti di variabile casuale, funzioni di massa di probabilità e funzioni di ripartizione.\n\nQuesti concetti sono fondamentali per comprendere come la probabilità e la misura possono essere utilizzate in contesti più complessi, permettendo di estendere le nostre analisi a insiemi infiniti e spazi continui, che sono comuni nelle applicazioni psicologiche.\n\n## $\\sigma$-Algebra\n\nUna **$\\sigma$-algebra** è una struttura matematica che permette di definire in modo coerente quali sottoinsiemi di un insieme sono \"misurabili\". \n\n## Definizione di $\\sigma$-Algebra\n\nUna **$\\sigma$-algebra** è una collezione di sottoinsiemi di uno spazio $X$ che soddisfa le seguenti proprietà:\n\n- **Chiusura rispetto al complemento**: Se un sottoinsieme $A$ appartiene alla $\\sigma$-algebra $\\mathcal{F}$, allora anche il suo complemento $A^c$ appartiene a $\\mathcal{F}$. Questo significa che se $\\mathcal{F}$ contiene un certo sottoinsieme, deve contenere anche tutti gli elementi che non sono in quel sottoinsieme.\n\n- **Chiusura rispetto alle unioni numerabili**: Se una sequenza numerabile di sottoinsiemi $A_1, A_2, A_3, \\ldots$ appartiene alla $\\sigma$-algebra $\\mathcal{F}$, allora anche l'unione di tutti questi sottoinsiemi appartiene a $\\mathcal{F}$. Questo implica che se $\\mathcal{F}$ contiene una serie di sottoinsiemi, deve contenere anche il loro insieme unito.\n\n- **Inclusione dello spazio campionario**: Lo spazio campionario $X$ stesso deve appartenere alla $\\sigma$-algebra $\\mathcal{F}$. In altre parole, l'intero insieme $X$ è considerato un sottoinsieme misurabile.\n\nLa *chiusura* in questo contesto significa che la collezione $\\mathcal{F}$ è stabile rispetto a determinate operazioni insiemistiche. In particolare, se si applicano le operazioni di complemento o di unione numerabile a elementi della $\\sigma$-algebra, i risultati di queste operazioni rimarranno all'interno della stessa $\\sigma$-algebra. Questo garantisce che la $\\sigma$-algebra non \"perda\" elementi a causa di queste operazioni, mantenendo così la coerenza e la completezza della collezione di sottoinsiemi.\n\n### Spazio Misurabile\n\nUn insieme dotato di una $\\sigma$-algebra, $(X, \\mathcal{X})$, è detto **spazio misurabile**. Gli elementi di una $\\sigma$-algebra sono noti come **sottoinsiemi misurabili**, mentre i sottoinsiemi non appartenenti alla $\\sigma$-algebra sono detti **non misurabili**. La distinzione tra sottoinsiemi misurabili e non misurabili è cruciale per evitare comportamenti anomali e controintuitivi nella teoria della misura e della probabilità.\n\n## Gli Assiomi di Kolmogorov\n\nUna volta definita una $\\sigma$-algebra, è possibile introdurre la probabilità come una misura definita su questa collezione di sottoinsiemi. Gli assiomi di Kolmogorov stabiliscono le proprietà fondamentali che ogni funzione di probabilità deve soddisfare:\n\n1. **Non negatività**: Per qualsiasi evento $A$ nello spazio campionario $\\Omega$, la probabilità di $A$ è non negativa.\n\n   $$\n   P(A) \\geq 0.\n   $$\n\n2. **Normalizzazione**: La probabilità dell'intero spazio campionario $\\Omega$ è 1.\n\n   $$\n   P(\\Omega) = 1.\n   $$\n\n3. **Additività numerabile**: Per qualsiasi sequenza di eventi mutuamente esclusivi ${A_i}_{i=1}^\\infty \\subset \\mathcal{F}$ (cioè $A_i \\cap A_j = \\varnothing$ per $i \\neq j$), la probabilità della loro unione è la somma delle loro probabilità.\n\n   $$\n   P\\left(\\bigcup_{i=1}^{\\infty} A_i\\right) = \\sum_{i=1}^{\\infty} P(A_i).\n   $$\n\n### Connessione tra $\\sigma$-Algebra e Probabilità\n\nGli assiomi di Kolmogorov richiedono una $\\sigma$-algebra $\\mathcal{F}$ come base per definire una misura di probabilità $P$. La $\\sigma$-algebra delimita l'insieme di sottoinsiemi dello spazio $X$ per i quali la probabilità è ben definita.\n\n1. **Non negatività** La misura di probabilità assegna un valore non negativo a ogni evento in $\\mathcal{F}$.\n2. **Normalizzazione** garantisce che $P(\\Omega) = 1$, garantendo coerenza nella distribuzione della probabilità.\n3. **Additività numerabile**: La chiusura della $\\sigma$-algebra rispetto alle unioni numerabili consente di applicare l'additività anche a collezioni infinite di eventi.\n\nIn sintesi, gli assiomi di Kolmogorov richiedono una $\\sigma$-algebra come struttura all'interno della quale queste proprietà valgono. La $\\sigma$-algebra è quindi la collezione di eventi per i quali la misura di probabilità è ben definita e coerente con gli assiomi di Kolmogorov.\n\n#### Esempio di $\\sigma$-Algebra \n\nConsideriamo lo spazio campionario $\\Omega = {1, 2, 3}$. Una possibile $\\sigma$-algebra su $\\Omega$ è:\n\n$\\mathcal{F}$ = {$\\varnothing$,{1},{2,3}, {1, 2, 3}}.\n\nQuesta $\\sigma$-algebra soddisfa tutte le proprietà richieste:\n\n- Lo spazio campionario $\\Omega$ e l'insieme vuoto $\\varnothing$ appartengono a $\\mathcal{F}$.\n- Il complemento di ogni sottoinsieme in $\\mathcal{F}$ appartiene ancora a $\\mathcal{F}$.\n- L'unione di qualsiasi collezione di sottoinsiemi in $\\mathcal{F}$ appartiene a $\\mathcal{F}$.\n\nIn conclusione, la $\\sigma$-algebra è un concetto essenziale nella teoria della probabilità, poiché delimita quali eventi possono essere misurati e assegnati una probabilità. Attraverso gli assiomi di Kolmogorov, questa struttura consente di costruire un sistema probabilistico coerente, garantendo che la probabilità sia definita in modo rigoroso e che operazioni come complemento, unione e intersezione numerabili siano sempre ben poste.\n\n## Probabilità\n\nUna volta definiti gli assiomi di Kolmogorov, è possibile introdurre formalmente il concetto di **probabilità**.\n\nLa **probabilità** di un evento è una misura numerica che quantifica il grado di fiducia nel verificarsi di tale evento, in accordo con gli assiomi di Kolmogorov. Più precisamente:\n\n- Se $P(A) = 0$, l'evento $A$ è impossibile.\n- Se $P(A) = 1$, l'evento $A$ è certo.\n\nPer indicare la probabilità che un evento $A$ **non** si verifichi, si usa la notazione $P(A^c)$, dove:\n\n$$\nP(A^c) = 1 - P(A).\n$$\n\n### Proprietà Derivate dagli Assiomi di Kolmogorov\n\nGli assiomi di Kolmogorov implicano alcune proprietà fondamentali, tra cui:\n\n- $P(\\varnothing) = 0$ (la probabilità dell'evento impossibile è nulla),\n- $0 \\leq P(A) \\leq 1$ (la probabilità è sempre compresa tra 0 e 1),\n- $P(A^c) = 1 - P(A)$ (probabilità del complemento),\n- Se $A \\subset B$, allora $P(A) \\leq P(B)$ (monotonia),\n- Se $A \\cap B = \\varnothing$, allora $P(A \\cup B) = P(A) + P(B)$ (additività per eventi incompatibili).\n\n### Regola della Somma\n\nLa **regola della somma** permette di calcolare la probabilità della disgiunzione logica (\"$A \\text{ oppure } B$\") tra due eventi $A$ e $B$, considerando se questi siano incompatibili (mutuamente esclusivi) o meno.\n\n#### Caso 1: Eventi incompatibili\n\nSe due eventi $A$ e $B$ **non possono verificarsi contemporaneamente**, essi sono detti **incompatibili** o **mutuamente esclusivi**. In questo caso, la probabilità della loro disgiunzione è semplicemente la somma delle probabilità individuali:\n\n$$\nP(A \\text{ oppure } B) = P(A) + P(B).\n$$\n\nQuesta formula riflette il fatto che la probabilità della loro intersezione è nulla:\n\n$$\nP(A \\cap B) = 0.\n$$\n\nGraficamente, gli eventi incompatibili possono essere rappresentati con diagrammi in cui le aree associate agli eventi non si sovrappongono. La probabilità dell'unione è quindi la somma delle aree.\n\n**Esempio: Negazione di un evento**  \nUn esempio comune di eventi incompatibili si verifica quando $B$ è il complemento di $A$, cioè $B = A^c$. In questo caso, poiché $A \\text{ oppure } A^c$ è sempre vero, la somma delle loro probabilità deve essere 1:\n\n$$\nP(A) + P(A^c) = 1.\n$$\n\nDa cui segue che:\n\n$$\nP(A^c) = 1 - P(A).\n$$\n\n#### Caso 2: Eventi non incompatibili\n\nQuando $A$ e $B$ **non sono incompatibili** (cioè possono verificarsi contemporaneamente), la regola della somma deve tenere conto della loro intersezione. La probabilità della disgiunzione si calcola come:\n\n$$\nP(A \\text{ oppure } B) = P(A) + P(B) - P(A \\cap B).\n$$\n\nIl termine $P(A \\cap B)$ rappresenta la probabilità che entrambi gli eventi si verifichino, e viene sottratto per evitare di conteggiarlo due volte.\n\n---\n\n### Legge della Probabilità Totale\n\nLa **legge della probabilità totale** permette di esprimere la probabilità di un evento $A$ come somma delle probabilità condizionate rispetto a un'eventuale partizione dello spazio campione.\n\nSe $B$ e $B^c$ formano una partizione dello spazio campione (ossia sono incompatibili e coprono l'intero spazio), allora:\n\n$$\nP(A) = P(A \\cap B) + P(A \\cap B^c).\n$$\n\nUtilizzando la definizione di probabilità condizionata, possiamo scrivere:\n\n$$\nP(A) = P(A \\mid B) P(B) + P(A \\mid B^c) P(B^c).\n$$\n\nQuesto principio è particolarmente utile quando si conosce la probabilità condizionata di $A$ dato uno o più eventi $B$.\n\nAnalogamente, per l'evento $B$:\n\n$$\nP(B) = P(B \\cap A) + P(B \\cap A^c).\n$$\n\nIn conclusione, gli assiomi di Kolmogorov forniscono una base rigorosa per il calcolo delle probabilità, garantendo coerenza e logica nella loro applicazione. La regola della somma e la legge della probabilità totale sono strumenti fondamentali per calcolare le probabilità in scenari più complessi, in cui gli eventi possono essere interdipendenti o condizionati. Questi principi permettono di affrontare problemi reali in modo strutturato e sistematico.\n\n## Probabilità e Calcolo Combinatorio\n\nI problemi scolastici più comuni sulle probabilità richiedono l'uso del calcolo combinatorio. La struttura generale di questi problemi è sempre la stessa: dobbiamo contare il numero di modi in cui un evento compatibile con l'evento di \"successo\" definito dal problema si realizza e poi trovare la proporzione di tali eventi rispetto a tutti gli eventi possibili (inclusi quelli di \"insuccesso\") che possono verificarsi nello spazio campionario. Questi problemi presentano due difficoltà principali:\n\n1. Trasformare la descrizione verbale del problema in una formulazione matematica chiara, suddividendo gli eventi possibili nello spazio campionario in base alle condizioni di successo e insuccesso definite dal problema.\n2. Contare il numero di successi e il numero totale di eventi.\n\nPer risolvere questi problemi, dobbiamo utilizzare tecniche del calcolo combinatorio, come le permutazioni e le combinazioni, che ci permettono di contare in modo preciso il numero di possibilità.\n\nConsideriamo un esempio semplice e intuitivo per chiarire il concetto. Supponiamo di avere una scatola con 10 palline numerate da 1 a 10. Vogliamo calcolare la probabilità di estrarre una pallina con un numero pari.\n\n1. **Definizione degli eventi**: In questo caso, l'evento di \"successo\" è l'estrazione di una pallina con un numero pari.\n   - Eventi di successo: {2, 4, 6, 8, 10}\n   - Eventi totali: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n\n2. **Conteggio delle possibilità**:\n   - Numero di eventi di successo: 5\n   - Numero totale di eventi: 10\n\n3. **Calcolo della probabilità**:\n   $$\n   P(\\text{numero pari}) = \\frac{\\text{numero di eventi di successo}}{\\text{numero totale di eventi}} = \\frac{5}{10} = 0.5.\n   $$\n\nPer problemi più complessi, come il calcolo della probabilità di ottenere una determinata combinazione di carte da un mazzo o di formare un particolare gruppo di persone da una popolazione più grande, utilizziamo strumenti del calcolo combinatorio. \n\n## Il Problema dei Fratelli Bernoulli\n\nLa soluzione dei problemi di probabilità non è sempre semplice e nella storia della matematica ci sono molti esempi di celebri matematici che hanno commesso errori. Uno di questi aneddoti riguarda Jakob Bernoulli, uno dei pionieri della teoria della probabilità.\n\nJakob Bernoulli si interessò al calcolo delle probabilità mentre cercava di formalizzare le leggi del caso nel suo libro \"Ars Conjectandi\", pubblicato postumo nel 1713. Uno dei problemi che affrontò riguardava il calcolo della probabilità di ottenere almeno una testa in 8 lanci di una moneta equa. Nonostante il suo approccio iniziale fosse corretto, Bernoulli commise un errore nel calcolo combinatorio durante il processo.\n\nPer risolvere il problema di calcolare la probabilità di ottenere almeno una testa in 8 lanci, bisogna considerare la probabilità complementare, ovvero la probabilità di non ottenere alcuna testa (ottenere solo croci) in 8 lanci, e poi sottrarla da 1:\n\n1. **Calcolo della probabilità complementare**: La probabilità di ottenere solo croci in un singolo lancio è $\\frac{1}{2}$. La probabilità di ottenere solo croci in 8 lanci consecutivi è:\n   $$\n   \\left(\\frac{1}{2}\\right)^8 = \\frac{1}{256}.\n   $$\n\n2. **Calcolo della probabilità di ottenere almeno una testa**:\n   $$\n   P(\\text{almeno una testa}) = 1 - P(\\text{nessuna testa}) = 1 - \\frac{1}{256} = \\frac{255}{256}.\n   $$\n\nJakob Bernoulli commise un errore nel calcolo combinatorio che lo portò a una soluzione errata. Egli sottostimò la probabilità di ottenere almeno una testa, probabilmente a causa di un errore nel conteggio delle possibili combinazioni di successi e insuccessi.\n\nQuesto errore fu successivamente corretto da altri matematici, tra cui suo nipote Daniel Bernoulli, che dimostrarono il metodo corretto per risolvere tali problemi utilizzando il calcolo combinatorio in modo appropriato.\n\nLa storia del calcolo combinatorio e della probabilità è ricca di aneddoti, come quello di Jakob Bernoulli, che mettono in luce quanto i problemi di probabilità possano essere estremamente controintuitivi, persino per i grandi matematici. Oggi, grazie al lavoro e alle correzioni apportate dai matematici del passato, siamo in grado di risolvere molti di questi problemi con maggiore facilità. La teoria della probabilità, come molte altre discipline scientifiche, è il risultato di un lungo processo di sviluppo e comprensione, che ha richiesto tempo e sforzi considerevoli.\n\nUna delle sfide della probabilità è che spesso i problemi non si prestano a soluzioni immediate o intuitive. Tuttavia, esistono due approcci fondamentali per affrontarli. Il primo consiste nell'applicare i teoremi della teoria della probabilità, un metodo che, come abbiamo visto, può risultare controintuitivo. Il secondo approccio è quello della simulazione Monte Carlo, che consente di ottenere una soluzione approssimata, ma molto vicina al valore reale, seguendo una procedura più intuitiva. Il nome di questo metodo deriva dal famoso Casinò di Monte Carlo a Monaco, ma possiamo semplicemente riferirci ad esso come metodo di simulazione.\n\nLa simulazione Monte Carlo è una classe generale di metodi stocastici, in contrasto con i metodi deterministici, utilizzati per risolvere approssimativamente problemi analitici attraverso la generazione casuale delle quantità di interesse. Tra i metodi comunemente utilizzati troviamo il campionamento con reinserimento, in cui la stessa unità può essere selezionata più volte, e il campionamento senza reinserimento, in cui ogni unità può essere selezionata una sola volta. Questi strumenti offrono un potente mezzo per affrontare problemi complessi in modo pratico e accessibile.\n\n::: {#exm-}\n\nConsideriamo il seguente esercizio che presenta un \"classico\" problema di calcolo delle probabilità.\n\n\"Un'urna contiene 10 palline rosse, 10 palline blu e 20 palline verdi. Se si estraggono 5 palline a caso senza reinserimento, qual è la probabilità che venga selezionata almeno una pallina di ciascun colore?\"\n\nLa soluzione al problema consiste nel contare il numero di modi in cui possono verificarsi gli eventi incompatibili con la condizione richiesta, dividere per il numero totale di modi in cui 5 palline possono essere estratte da un'urna con 40 palline, e sottrarre tale risultato da 1.\n\nIniziamo dal denominatore (\"in quanti modi possono essere estratte 5 palline da un'urna che ne contiene 40\"). La soluzione è data dal coefficiente binomiale: $\\binom{40}{5}$.\n\nDobbiamo poi enumerare tutti i casi incompatibili con la condizione espressa dal problema; al numeratore avremo quindi: (modi di ottenere nessuna pallina rossa) + (modi di ottenere nessuna pallina blu) + (modi di ottenere nessuna pallina verde) - (modi di ottenere nessuna pallina rossa o blu) - (modi di ottenere nessuna pallina rossa o verde) - (modi di ottenere nessuna pallina blu o verde).\n\nLa soluzione è dunque:\n\n$$\nP(\\text{almeno una rossa, blu e verde}) = \\frac{\n\\binom{30}{5} + \\binom{30}{5} + \\binom{20}{5} - \\binom{20}{5} - \\binom{10}{5} - \\binom{10}{5}\n}{\\binom{40}{5}} = 0.568.\n$$\n\nLo stesso risultato si ottiene con una simulazione.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\n# Creare un'urna con le palline\nurn <- c(rep(\"red\", 10), rep(\"blue\", 10), rep(\"green\", 20))\n\n# Numero di simulazioni\nsimulations <- 100000\n\ncount <- 0\nfor (i in 1:simulations) {\n  # Estrarre 5 palline dall'urna\n  draw <- sample(urn, 5, replace = FALSE)\n  \n  # Verificare se c'è almeno una pallina di ogni colore (red, blue, green)\n  if (\"red\" %in% draw && \"blue\" %in% draw && \"green\" %in% draw) {\n    count <- count + 1\n  }\n}\n\n# Calcolare la probabilità simulata\nprob_simulated <- count / simulations\nprob_simulated\n#> [1] 0.568\n```\n:::\n\n\n\n\n\n:::\n\nIl metodo di simulazione consente di risolvere problemi che implicano il calcolo delle probabilità relative a vari eventi generati dal lancio dei dadi.\n\n::: {#exm-}\n\nIl problema dei compleanni, generalmente attribuito a Richard von Mises, è un noto esempio controintuitivo di calcolo delle probabilità che utilizza il calcolo combinatorio, in particolare le permutazioni. Il problema chiede quanti individui sono necessari affinché la probabilità che almeno due persone abbiano lo stesso compleanno superi il 50%, assumendo che ogni giorno dell'anno sia ugualmente probabile come compleanno. Sorprendentemente, la risposta è solo 23 persone, molto meno di quanto la maggior parte delle persone immagina.\n\nPer risolvere il problema dei compleanni utilizzando le permutazioni, consideriamo la seguente relazione:\n\n$$\n\\begin{align*}\nP(\\text{almeno due persone hanno lo stesso compleanno}) &= \\\\\n1 - P(\\text{nessuno ha lo stesso compleanno}).\n\\end{align*}\n$$\n\nQuesta uguaglianza è valida perché l'evento \"nessuno ha lo stesso compleanno\" è il complemento dell'evento \"almeno due persone hanno lo stesso compleanno\". Pertanto, dobbiamo calcolare la probabilità che nessuno abbia lo stesso compleanno.\n\nSia $k$ il numero di persone. Per calcolare la probabilità che nessuno abbia lo stesso compleanno, dobbiamo contare il numero di modi in cui $k$ persone possono avere compleanni diversi. Poiché ogni compleanno è ugualmente probabile, possiamo usare le permutazioni per contare il numero di modi in cui $k$ compleanni unici possono essere disposti su 365 giorni:\n\n$$\n365P_k = \\frac{365!}{(365 - k)!}.\n$$\n\nDividiamo questo numero per il numero totale di elementi nello spazio campionario, che è il numero totale di modi in cui $k$ compleanni possono essere disposti su 365 giorni:\n\n$$\n365^k.\n$$\n\nQuindi, la probabilità che nessuno abbia lo stesso compleanno è:\n\n$$\nP(\\text{nessuno ha lo stesso compleanno}) = \\frac{365P_k}{365^k} = \\frac{365!}{365^k (365 - k)!}.\n$$\n\nUsando questa formula, la probabilità che almeno due persone abbiano lo stesso compleanno è:\n\n$$\nP(\\text{almeno due persone hanno lo stesso compleanno}) = 1 - \\frac{365!}{365^k (365 - k)!}.\n$$\n\nIn sintesi, calcolando questa probabilità, si scopre che bastano solo 23 persone affinché la probabilità che almeno due di loro abbiano lo stesso compleanno superi il 50%, un risultato sorprendente rispetto all'intuizione comune.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Funzione per calcolare la probabilità\nbirthday <- function(k) {\n  logdenom <- k * log(365) + lgamma(365 - k + 1) # log denominatore\n  lognumer <- lgamma(366) # log numeratore\n  pr <- 1 - exp(lognumer - logdenom) # trasformazione inversa\n  return(pr)\n}\n\n# Calcola la probabilità per k persone\nk <- 1:50\nbday <- sapply(k, birthday)\n\n# Plot dei risultati\nggplot(data.frame(k = k, bday = bday), aes(x = k, y = bday)) +\n  geom_line(marker = \"o\", alpha = 0.5) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dashed\", color = \"gray\") +\n  labs(\n    x = \"Numero di persone\",\n    y = \"Probabilità che almeno due persone\\nabbiano lo stesso compleanno\",\n    title = \"Probabilità del Problema dei Compleanni\"\n  ) +\n  xlim(0, 50) +\n  ylim(0, 1)\n#> Warning in geom_line(marker = \"o\", alpha = 0.5): Ignoring unknown\n#> parameters: `marker`\n\n# Probabilità per 20-25 persone\nbday[20:25]\n#> [1] 0.411 0.444 0.476 0.507 0.538 0.569\n```\n\n::: {.cell-output-display}\n![](03_prob_on_general_spaces_files/figure-html/unnamed-chunk-3-1.png){width=576}\n:::\n:::\n\n\n\n\n\nOsserviamo che quando il numero di persone è 23, la probabilità che almeno due persone abbiano lo stesso compleanno supera 0.5. Quando il numero di persone è più di 50, questa probabilità è quasi 1.\n\n:::\n\n::: {#exm-}\n\nIn precedenza, abbiamo derivato la soluzione analitica esatta per il problema dei compleanni, ma possiamo ottenere una soluzione approssimata in modo più intuitivo utilizzando il metodo della simulazione Monte Carlo.\n\nLa simulazione Monte Carlo si basa su un'idea semplice: simuliamo il processo descritto dal problema e analizziamo i risultati di molte prove per stimare la probabilità cercata.\n\nPassi della simulazione sono i seguenti.\n\n1. **Campionamento casuale:** Simuliamo i compleanni di $k$ persone scegliendo $k$ numeri interi casuali tra 1 e 365, dove ogni numero rappresenta un giorno dell'anno. Questo campionamento avviene con reinserimento: un giorno può essere estratto più volte, perché non esiste alcuna restrizione sul fatto che due o più persone possano nascere lo stesso giorno.\n\n2. **Verifica della condizione:** Controlliamo se almeno due numeri campionati sono uguali, ovvero se almeno due persone condividono lo stesso compleanno.\n\n3. **Ripetizione della simulazione:** Ripetiamo il processo di campionamento molte volte (ad esempio 1000, o anche 1 milione di volte per una stima più precisa).\n\n4. **Stima della probabilità:** Calcoliamo la frazione di simulazioni in cui si è verificato l'evento \"almeno due persone condividono un compleanno\". Questa frazione è la nostra stima Monte Carlo della probabilità cercata.\n\nIl seguente codice R implementa questo approccio:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345) # Imposta il seme per la riproducibilità\n\nk <- 23  # Numero di persone\nsims <- 1000  # Numero di simulazioni\nevent <- 0  # Contatore eventi\n\n# Simulazioni per stimare la probabilità\nfor (i in 1:sims) {\n  days <- sample(1:365, k, replace = TRUE)\n  unique_days <- unique(days)\n  if (length(unique_days) < k) {\n    event <- event + 1\n  }\n}\n\n# Frazione di prove in cui almeno due compleanni sono uguali\nanswer <- event / sims\ncat(sprintf(\"Stima della probabilità: %.6f\\n\", answer))\n#> Stima della probabilità: 0.526000\n\n# Aumentare il numero di simulazioni a un milione per maggiore accuratezza\nsims_large <- 1000000\nevent_large <- 0\n\nfor (i in 1:sims_large) {\n  days <- sample(1:365, k, replace = TRUE)\n  unique_days <- unique(days)\n  if (length(unique_days) < k) {\n    event_large <- event_large + 1\n  }\n}\n\nanswer_large <- event_large / sims_large\ncat(sprintf(\"Stima con un milione di simulazioni: %.6f\\n\", answer_large))\n#> Stima con un milione di simulazioni: 0.506565\n```\n:::\n\n\n\n\n\nNel codice sopra, abbiamo impostato il numero di simulazioni a un milione. Osserviamo che quando il numero di persone è 23, la probabilità che almeno due persone abbiano lo stesso compleanno è superiore a 0.5. Quando il numero di persone supera 50, questa probabilità è vicina a 1.\n\n:::\n\n## Le Assunzioni nella Soluzione dei Problemi \n\nNella realtà, i compleanni non seguono una distribuzione uniforme. Una soluzione migliore al problema dei compleanni sarebbe quella di estrarre i compleanni dalla distribuzione effettiva piuttosto che da una distribuzione uniforme in cui ogni giorno ha la stessa probabilità. Non esiste un metodo matematico standard per calcolare questa probabilità; l'unico modo per farlo è attraverso la simulazione.\n\nNegli Stati Uniti, il CDC e la Social Security Administration monitorano il numero di nascite giornaliere. Nel 2016, [FiveThirtyEight](https://projects.fivethirtyeight.com/polls/) ha pubblicato un articolo sulle frequenze giornaliere di nascita e ha reso disponibili i dati in un file CSV su GitHub. Utilizzando il codice fornito da [Andrew Heiss](https://www.andrewheiss.com/blog/2024/05/03/birthday-spans-simulation-sans-math/), possiamo caricare quei dati e calcolare le probabilità giornaliere dei compleanni negli Stati Uniti.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](03_prob_on_general_spaces_files/figure-html/unnamed-chunk-5-1.png){width=576}\n:::\n:::\n\n\n\n\n\nI dati mostrano alcuni pattern interessanti e chiaramente distinguibili:\n\n- **Festività principali**: Durante il periodo natalizio e di Capodanno, le nascite sembrano essere meno frequenti. In particolare, il giorno di Natale, la vigilia di Natale e il giorno di Capodanno registrano il numero medio di nascite più basso.\n\n- **Festività minori e date particolari**: Anche altre festività e occasioni speciali, come la vigilia di Capodanno, Halloween, il 4 luglio, il 1° aprile e l'intera settimana del Ringraziamento, mostrano una diminuzione significativa nelle medie delle nascite.\n\n- **Il giorno 13**: Una leggera riduzione del numero medio di nascite si osserva il 13 di ogni mese. Questo pattern si evidenzia chiaramente nella colonna corrispondente ai giorni 13.\n\n- **Picco di nascite a settembre**: I giorni con il numero medio di nascite più elevato si concentrano a metà settembre, dal 9 al 20, con l’eccezione dell’11 settembre, che mostra un valore inferiore.\n\nÈ probabile che in Italia i pattern relativi alla distribuzione delle nascite siano almeno in parte diversi, a causa di variazioni culturali, sociali e legate alla gestione sanitaria.\n\n---\n\nNon è l’obiettivo qui riformulare la soluzione del problema dei compleanni utilizzando la distribuzione effettiva delle nascite invece di quella uniforme, ma piuttosto sottolineare un aspetto cruciale: ogni procedura di risoluzione dei problemi probabilistici si basa su **assunzioni**. Nel caso del problema dei compleanni, assumiamo che i compleanni siano distribuiti uniformemente durante l’anno. Tuttavia, questa ipotesi è solo un'approssimazione, poiché la distribuzione reale dei compleanni non è uniforme. \n\nLe soluzioni che otteniamo nei problemi probabilistici descrivono con maggiore precisione le regolarità del mondo reale quando le assunzioni su cui si basano sono ragionevoli e ben fondate. \n\n---\n\nQuesto principio si applica non solo al problema dei compleanni, ma a tutti i modelli probabilistici e, più in generale, a tutti i modelli scientifici. Ogni modello si basa su una serie di ipotesi che ne delimitano validità e applicabilità. Di conseguenza, è essenziale valutare attentamente la plausibilità di tali ipotesi per garantire che il modello offra una rappresentazione coerente e utile del fenomeno che si intende descrivere.\n\n## Commenti e considerazioni finali\n\nLa teoria delle probabilità è un pilastro fondamentale della statistica, con applicazioni pratiche in numerosi campi, tra cui la psicologia. Comprendere le probabilità ci consente di prendere decisioni informate in situazioni di incertezza e di formulare previsioni affidabili. Una solida conoscenza delle basi della probabilità ci permette di affrontare una vasta gamma di problemi e di fare scelte ponderate basate sulla probabilità dei vari esiti possibili. Tuttavia, è importante ricordare che i modelli probabilistici sono solo approssimazioni della realtà e possono essere influenzati da semplificazioni o dalle limitazioni dei dati disponibili. Pertanto, è fondamentale interpretare i risultati con cautela e avere piena consapevolezza delle assunzioni che sottendono le analisi.\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered} \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.2\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] reshape2_1.4.4    viridis_0.6.5     viridisLite_0.4.2 gridExtra_2.3    \n#>  [5] patchwork_1.3.0   bayesplot_1.11.1  psych_2.4.6.26    scales_1.3.0     \n#>  [9] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#> [13] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n#> [17] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#> [21] rio_1.2.3         here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] utf8_1.2.4         generics_0.1.3     stringi_1.8.4     \n#>  [4] lattice_0.22-6     hms_1.1.3          digest_0.6.37     \n#>  [7] magrittr_2.0.3     RColorBrewer_1.1-3 evaluate_1.0.1    \n#> [10] grid_4.4.2         timechange_0.3.0   fastmap_1.2.0     \n#> [13] plyr_1.8.9         rprojroot_2.0.4    jsonlite_1.8.9    \n#> [16] fansi_1.0.6        mnormt_2.1.1       cli_3.6.3         \n#> [19] crayon_1.5.3       rlang_1.1.4        bit64_4.5.2       \n#> [22] munsell_0.5.1      withr_3.0.2        yaml_2.3.10       \n#> [25] tools_4.4.2        parallel_4.4.2     tzdb_0.4.0        \n#> [28] colorspace_2.1-1   pacman_0.5.1       curl_6.0.1        \n#> [31] vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4   \n#> [34] bit_4.5.0.1        htmlwidgets_1.6.4  vroom_1.6.5       \n#> [37] pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.6      \n#> [40] Rcpp_1.0.13-1      glue_1.8.0         xfun_0.49         \n#> [43] tidyselect_1.2.1   farver_2.1.2       htmltools_0.5.8.1 \n#> [46] nlme_3.1-166       labeling_0.4.3     rmarkdown_2.29    \n#> [49] compiler_4.4.2\n```\n:::\n",
    "supporting": [
      "03_prob_on_general_spaces_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}