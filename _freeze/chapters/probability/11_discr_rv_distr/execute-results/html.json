{
  "hash": "a9338e92b2fb904b5c8e97d2675cd1d9",
  "result": {
    "engine": "knitr",
    "markdown": "# Distribuzioni di v.c. discrete {#sec-prob-discrete-prob-distr}\n\n**Prerequisiti**\n\n**Concetti e Competenze Chiave**\n\n**Preparazione del Notebook**\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhere::here(\"code\", \"_common.R\") |> \n  source()\n\n# Load packages\nif (!requireNamespace(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(mice)\n```\n:::\n\n\n\n\n\n## Introduzione \n\nLa previsione è un processo che ci permette di formulare ipotesi su eventi incerti, sfruttando le regolarità osservate nei processi naturali, sociali e psicologici. Uno degli obiettivi principali della data science è proprio quello di prevedere fenomeni di cui non abbiamo ancora certezza, inclusi, ma non limitati a, eventi futuri.\n\nLa capacità di fare previsioni senza considerare ogni possibile risultato dipende dalla conoscenza della popolazione di riferimento. Gli esseri umani organizzano e rappresentano questa conoscenza in vari modi. In questo capitolo, esploreremo le implicazioni di un approccio specifico alla rappresentazione delle popolazioni: le distribuzioni di probabilità.\n\nSupponiamo di avere una distribuzione di probabilità $p(x)$ associata a una variabile casuale $X$. Consideriamo che questa distribuzione rappresenti la variabilità osservata all'interno di una popolazione. Se selezionassimo un'istanza in modo uniforme e casuale dalla popolazione, quale valore della variabile $X$ dovremmo aspettarci? Ci aspettiamo che un campione estratto casualmente dalla popolazione segua la distribuzione $p(x)$. In altre parole, questa distribuzione è ciò che definiamo un modello statistico, o più semplicemente, un modello della popolazione. Il termine \"modello\" sottolinea che la distribuzione non è la popolazione stessa, ma una rappresentazione astratta che utilizziamo per fare previsioni.\n\nIn particolare, ci concentreremo sulle distribuzioni di probabilità discrete, essenziali per comprendere i fenomeni aleatori che presentano un numero finito o numerabile di esiti. Queste distribuzioni sono cruciali nella modellazione e nell'analisi di eventi che si verificano in contesti discreti, fornendo le basi per una comprensione più profonda delle dinamiche probabilistiche che governano tali fenomeni.\n\nOgni distribuzione di probabilità è caratterizzata da uno o più parametri, che consentono di controllare specifici aspetti della distribuzione stessa. Esploreremo diverse distribuzioni discrete, ciascuna con le sue caratteristiche e applicazioni:\n\n1. Distribuzione di Bernoulli\n   - Rappresenta esperimenti con due possibili esiti: \"successo\" o \"insuccesso\"\n   - Costituisce il nucleo dei processi Bernoulliani\n   - Parametro chiave: probabilità di successo in ciascuna prova\n\n2. Distribuzione Binomiale\n   - Descrive il numero totale di successi in un numero fisso di prove Bernoulliane\n   - Nasce dalla somma di prove Bernoulliane indipendenti\n   - Parametri: probabilità di successo in ciascuna prova e numero totale di prove\n\n3. Distribuzione di Poisson\n   - Modella eventi rari o che si verificano su intervalli di tempo o spazio variabili\n   - Adatta quando il numero di prove è una variabile casuale\n   - Parametro: tasso medio di successo per unità di tempo o spazio\n\n4. Distribuzione Beta-Binomiale\n   - Utilizzata quando la probabilità di successo in una serie di prove Bernoulliane non è costante\n   - Offre una rappresentazione più flessibile rispetto alla distribuzione binomiale\n   - Parametri: derivati dalla distribuzione Beta sottostante\n\n5. Distribuzione Uniforme Discreta\n   - Ogni evento all'interno di un determinato intervallo finito ha la stessa probabilità\n   - Utile quando non ci sono motivi per privilegiare un risultato rispetto a un altro\n   - Non dipende da parametri una volta stabilito il supporto della distribuzione\n\nIn sintesi, attraverso lo studio di queste distribuzioni, acquisiremo gli strumenti necessari per analizzare e prevedere una vasta gamma di situazioni reali. \n\n## Distribuzione di Bernoulli\n\nIn statistica, un esperimento che ammette solo due esiti possibili è modellato attraverso quella che viene chiamata \"prova Bernoulliana\". Un esempio tipico è il lancio di una moneta, che può dare come risultato testa o croce.\n\n::: {#def-}\nUna variabile casuale $X$ che assume valori in $\\{0, 1\\}$ è detta variabile di Bernoulli. La sua distribuzione di probabilità è definita come:\n\n$$\nP(X \\mid \\theta) =\n  \\begin{cases}\n    p     & \\text{se $X = 1$ (successo)}, \\\\\n    1 - p & \\text{se $X = 0$ (insuccesso)},\n  \\end{cases}\n$$\n\ndove $0 \\leq p \\leq 1$. Il parametro $p$ rappresenta la probabilità del \"successo\" ($X = 1$), mentre $1 - p$ è la probabilità dell'\"insuccesso\" ($X = 0$).\n:::\n\nLa distribuzione di Bernoulli descrive quindi un contesto in cui la probabilità di osservare l'esito 1 è $p$ e quella di osservare l'esito 0 è $1 - p$. Viene utilizzata per modellare situazioni binarie, come una risposta \"sì\" o \"no\", oppure un \"successo\" o \"insuccesso\".\n\nCalcolando il valore atteso e la varianza, otteniamo:\n\n$$\n\\begin{align}\n\\mathbb{E}(X) &= 0 \\cdot P(X=0) + 1 \\cdot P(X=1) = p, \\\\\n\\mathbb{V}(X) &= (0 - p)^2 \\cdot P(X=0) + (1 - p)^2 \\cdot P(X=1) = p(1-p).\n\\end{align}\n$$ {#eq-ev-var-bern}\n\nEsplicitando ulteriormente la formula della varianza con $P(YX=0) = 1 - p$ e $P(X=1) = p$, abbiamo:\n\n$$ \\mathbb{V}(X) = (0 - p)^2 \\cdot (1 - p) + (1 - p)^2 \\cdot p $$\n\nCalcoliamo ora le singole parti dell'espressione:\n1. $(0 - p)^2 = p^2$\n2. $(1 - p)^2 = 1 - 2p + p^2$\n\nSostituendo queste espressioni nell'equazione della varianza, otteniamo:\n\n$$ \\mathbb{V}(X) = p^2 \\cdot (1 - p) + (1 - 2p + p^2) \\cdot p $$\n\n$$ \\mathbb{V}(X) = p^2 - p^3 + p - 2p^2 + p^3 $$\n\nSemplificando:\n\n$$ \\mathbb{V}(X) = p - p^2 $$\n\n$$ \\mathbb{V}(X) = p(1-p). $$\n\nIn sintesi, la varianza di una variabile aleatoria binaria $X$, distribuita secondo Bernoulli con parametro $p$, è data da $p(1-p)$. Tale risultato mostra come la varianza massima si ottenga per $p = 0.5$, condizione che corrisponde alla massima incertezza intrinseca nel processo, ossia quando la probabilità di successo eguaglia quella di insuccesso.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Valori di p tra 0 e 1\np <- seq(0, 1, length.out = 100)\n\n# Varianza della distribuzione di Bernoulli è p * (1 - p)\nvariance <- p * (1 - p)\n\n# Grafico\nplot(p, variance, type = \"l\", col = \"blue\", lwd = 2, \n     xlab = expression(p), ylab = \"Varianza\", \n     main = \"Varianza di una Variabile Bernoulliana in funzione di p\")\n```\n\n::: {.cell-output-display}\n![](11_discr_rv_distr_files/figure-html/unnamed-chunk-2-1.png){width=576}\n:::\n:::\n\n\n\n\n\nPer indicare che la variabile casuale $X$ segue una distribuzione Bernoulliana di parametro $p$ Utilizziamo la notazione $X \\sim \\mathcal{Bern}(p)$, o in maniera equivalente $\\mathcal{Bern}(X \\mid p)$.\n\nAd esempio, nel caso del lancio di una moneta equilibrata, la variabile di Bernoulli assume i valori $0$ e $1$ con uguale probabilità di $\\frac{1}{2}$. Pertanto, la funzione di massa di probabilità assegna una probabilità di $\\frac{1}{2}$ sia per $X = 0$ che per $X = 1$, mentre la funzione di distribuzione cumulativa risulta essere $\\frac{1}{2}$ per $X = 0$ e $1$ per $X = 1$.\n\nGeneriamo dei valori casuali dalla distribuzione di Bernoulli. Iniziamo con un singolo valore:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probabilità di successo\np <- 0.5\n\n# Genera un singolo valore\nbernoulli_sample <- rbinom(n = 1, size = 1, prob = p)\nprint(bernoulli_sample)\n#> [1] 1\n\n# Genera un campione di 10 valori\nbernoulli_sample <- rbinom(n = 10, size = 1, prob = p)\nprint(bernoulli_sample)\n#>  [1] 1 0 1 1 1 1 0 1 1 0\n```\n:::\n\n\n\n\n\n::: {.callout-tip}  \n## Distribuzioni in R  \n\nIn R, per ogni distribuzione, esistono quattro funzioni, i cui nomi iniziano rispettivamente con le lettere **d** (densità), **p** (probabilità cumulativa), **q** (quantile) e **r** (generazione di un campione casuale).  \n:::\n\nIl pacchetto *{stats}* di R contiene molte funzioni che restituiscono i valori teorici delle statistiche per specifiche distribuzioni. Per maggiori dettagli, consulta `?Distributions`.\n\n## Distribuzione Binomiale\n\nLa distribuzione binomiale è una distribuzione di probabilità discreta che modella il numero di successi $y$ in un numero fissato $n$ di prove di Bernoulli indipendenti e identiche, dove ciascuna prova ha solo due esiti possibili: \"successo\" (rappresentato da \"1\") con probabilità $p$ o \"insuccesso\" (rappresentato da \"0\") con probabilità $1 - p$. La notazione utilizzata è la seguente:\n\n$$\nY \\sim \\mathcal{Binom}(n, p).\n$$\n\n::: {#def-}\nLa distribuzione binomiale descrive la probabilità di osservare esattamente $y$ successi in $n$ prove di Bernoulli indipendenti:\n\n$$\nP(Y = y) = \\binom{n}{y} p^{y} (1 - p)^{n - y} = \\frac{n!}{y!(n - y)!} p^{y} (1 - p)^{n - y},\n$$ {#eq-binom-distr}\n\ndove $\\binom{n}{y}$, noto come coefficiente binomiale, rappresenta il numero di modi possibili per ottenere $y$ successi in $n$ prove, e $p$ è la probabilità di successo in ciascuna prova.\n:::\n\nLa distribuzione binomiale si presta bene a esempi classici come il lancio ripetuto di una moneta o l'estrazione di biglie da un'urna. Ad esempio, nel caso del lancio di una moneta, questa distribuzione descrive la probabilità di ottenere un determinato numero di \"teste\" in un certo numero di lanci, con ogni lancio che segue una distribuzione di Bernoulli con probabilità di successo $p$.\n\nUna caratteristica interessante della distribuzione binomiale è la sua *proprietà di riproducibilità*: se due variabili casuali indipendenti, $y_1$ e $y_2$, seguono entrambe distribuzioni binomiali con lo stesso parametro $p$, ma con un diverso numero di prove ($n_1$ e $n_2$), la loro somma, $y = y_1 + y_2$, sarà ancora distribuita binomialmente, con parametri $n_1 + n_2$ e $p$.\n\n### Calcolo delle Probabilità\n\nPer chiarire il calcolo delle probabilità nella distribuzione binomiale, consideriamo una serie di prove di Bernoulli. Supponiamo di avere $n$ prove, con $y$ successi. La configurazione di questi risultati può essere rappresentata come:\n\n$$\n\\overbrace{SS\\dots S}^\\text{$y$ successi} \\overbrace{II\\dots I}^\\text{$n - y$ insuccessi}\n$$\n\nLa probabilità di ottenere esattamente $y$ successi in una sequenza specifica di prove è pari a:\n\n$$\np^y \\cdot (1 - p)^{n - y},\n$$\n\ndove $p^y$ è la probabilità di ottenere $y$ successi, e $(1 - p)^{n - y}$ è la probabilità di ottenere $n - y$ insuccessi.\n\nTuttavia, siamo interessati alla probabilità complessiva di ottenere esattamente $y$ successi in *qualsiasi* ordine. Il numero di modi in cui ciò può avvenire è dato dal coefficiente binomiale $\\binom{n}{y}$, che rappresenta tutte le possibili disposizioni dei successi e degli insuccessi nelle $n$ prove.\n\nQuindi, moltiplicando la probabilità di una singola sequenza per il numero di sequenze possibili, otteniamo la probabilità di osservare esattamente $y$ successi:\n\n$$\nP(Y = y) = \\binom{n}{y} p^y (1 - p)^{n - y}.\n$$\n\nQuesto risultato corrisponde alla formula della distribuzione binomiale.\n\n### Applicazioni Pratiche della Distribuzione Binomiale\n\nConsideriamo un esempio pratico per illustrare l'applicazione della distribuzione binomiale. Supponiamo di osservare 2 successi in 4 prove Bernoulliane, dove la probabilità di successo in ogni prova è $p = 0.2$. La probabilità di ottenere questo risultato specifico è calcolata utilizzando l'eq. {eq}`eq-binom-distr`:\n\n$$\nP(Y=2) = \\frac{4!}{2!(4-2)!} \\cdot 0.2^{2} \\cdot (1-0.2)^{4-2} = 0.1536.\n$$\n\nQuesto calcolo può essere replicato in Python. Utilizzando il modulo `math`, possiamo calcolare direttamente:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parametri\nn <- 4\np <- 0.2\ny <- 2\n\n# Probabilità di ottenere esattamente y successi\nprob <- choose(n, y) * p^y * (1 - p)^(n - y)\nprint(prob)\n#> [1] 0.154\n```\n:::\n\n\n\n\n\nIn alternativa, possiamo sfruttare la libreria SciPy per eseguire calcoli analoghi. SciPy offre una vasta gamma di funzioni per la gestione delle distribuzioni statistiche, tra cui la distribuzione binomiale.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probabilità di ottenere esattamente y successi\nprob <- choose(n, y) * p^y * (1 - p)^(n - y)\nprint(prob)\n#> [1] 0.154\n```\n:::\n\n\n\n\n\nUtilizzando `dbinom(y, n, p)`, possiamo trovare le probabilità per ogni possibile valore $y$ in una distribuzione binomiale di parametri $n = 4$ e $\\theta = 0.2$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Usando la funzione dbinom\nprob <- dbinom(x = y, size = n, prob = p)\nprint(prob)\n#> [1] 0.154\n```\n:::\n\n\n\n\n\nVisualizziamo la distribuzione di massa di probabilità:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ny <- 0:n  # Numero di successi\nprobabilities <- dbinom(y, size = n, prob = p)  # Probabilità associate\n\n# Preparare i dati in un data frame\ndf <- data.frame(Successi = y, Probabilità = probabilities)\n\ndf |> \n  ggplot(aes(x = Successi, y = Probabilità)) +\n    geom_segment(aes(xend = Successi, yend = 0), lwd = 1.2, color = \"blue\") +\n    geom_point(size = 3, color = \"blue\") +\n    labs(\n      x = \"Numero di Successi y\",\n      y = \"Probabilità\",\n      title = paste(\"Distribuzione Binomiale: n =\", n, \", p =\", p)\n  )\n```\n\n::: {.cell-output-display}\n![](11_discr_rv_distr_files/figure-html/unnamed-chunk-7-1.png){width=576}\n:::\n:::\n\n\n\n\n\nUn campione casuale si ottiene con `rbinom()`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nsamples <- rbinom(n = 30, size = 5, prob = 0.5)\nprint(samples)\n#>  [1] 4 4 2 4 3 3 3 1 3 3 2 3 4 2 2 4 5 1 2 3 4 1 5 4 1 3 2 4 2 4\n```\n:::\n\n\n\n\n\nPer esplorare ulteriormente, consideriamo la distribuzione di probabilità di diverse distribuzioni binomiali per due valori di $n$ e $\\theta$. La seguente visualizzazione mostra come cambia la distribuzione al variare di $\\theta$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parametri\nn <- 20\np_values <- seq(0.3, 0.9, by = 0.3) # Valori di probabilità\ny <- 0:25 # Numero di successi\n\n# Creazione di un data frame per tutte le distribuzioni\ndf <- data.frame()\n\nfor (p in p_values) {\n  binom_dist <- dbinom(y, size = n, prob = p)\n  df <- rbind(df, data.frame(y = y, Prob = binom_dist, p = factor(p)))\n}\n\n# Grafico con ggplot2\ndf |> \n  ggplot(aes(x = y, y = Prob, color = p)) +\n    geom_point() +\n    geom_line() +\n    labs(\n      x = \"Numero di successi y\", \n      y = \"Probabilità\",\n      title = \"Distribuzione binomiale al variare di p\",\n      color = expression(theta)\n  )\n```\n\n::: {.cell-output-display}\n![](11_discr_rv_distr_files/figure-html/unnamed-chunk-9-1.png){width=576}\n:::\n:::\n\n\n\n\n\nConsideriamo un altro esempio. Lanciando $5$ volte una moneta onesta, qual è la probabilità che esca testa almeno due volte? Troviamo la soluzione usando `stats.binom.pmf()`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calcolo della somma delle probabilità\nresult <- dbinom(2, size = 5, prob = 0.5) +\n          dbinom(3, size = 5, prob = 0.5) +\n          dbinom(4, size = 5, prob = 0.5) +\n          dbinom(5, size = 5, prob = 0.5)\n\nprint(result)\n#> [1] 0.812\n```\n:::\n\n\n\n\n\nOppure, in modo più compatto:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Valori di interesse\nresult <- sum(dbinom(2:5, size = 5, prob = 0.5))\nprint(result)\n#> [1] 0.812\n```\n:::\n\n\n\n\n\nRappresentiamo graficamente la funzione di ripartizione per una Binomiale di ordine $n$ = 5 e $\\theta$ = 0.5.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parametri\nn <- 5\np <- 0.5\ny <- 0:n\n\n# Calcolo della funzione di ripartizione cumulativa\ncdf_values <- pbinom(y, size = n, prob = p)\n\n# Creazione del data frame per ggplot2\ndf <- data.frame(y = y, cdf = cdf_values)\n\n# Grafico\ndf |> \n  ggplot(aes(x = y, y = cdf)) +\n    geom_line() +\n    geom_point() +\n    geom_hline(yintercept = 1, color = \"black\", alpha = 0.7, linetype = \"dashed\") +\n    labs(\n      title = paste(\"Funzione di ripartizione binomiale: n =\", n, \", p =\", p),\n      x = \"y\",\n      y = \"Probabilità\"\n    )\n```\n\n::: {.cell-output-display}\n![](11_discr_rv_distr_files/figure-html/unnamed-chunk-12-1.png){width=576}\n:::\n:::\n\n\n\n\n\nUn'altra funzione utile è quella che permette di trovare il numero di successi associato a una data probabilità cumulativa nella coda sinistra di una distribuzione binomiale. Questo si ottiene utilizzando la funzione `qbinom`, che rappresenta l'inversa della funzione di distribuzione cumulativa (CDF). \n\nAd esempio, consideriamo una distribuzione binomiale con $n = 5$ prove e probabilità di successo $p = 0.5$. Supponiamo di voler calcolare il numero minimo di successi per cui la probabilità cumulativa è almeno $1 - 0.8125 = 0.1875$. Possiamo farlo nel seguente modo:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Probabilità target\ntarget_probability <- 1 - 0.8125\n\n# Numero di successi corrispondente alla probabilità target\nresult <- qbinom(target_probability, size = 5, prob = 0.5)\n\nprint(result)\n#> [1] 1\n```\n:::\n\n\n\n\n\nIn questo esempio, il valore restituito è $1$, che indica che almeno 1 successo soddisfa la condizione di una probabilità cumulativa di $0.1875$.\n\n### Altro esempio\n\nConsideriamo ora una distribuzione binomiale con $n = 10$ prove e probabilità di successo $p = 0.2$. Per calcolare la probabilità cumulativa $P(Y \\leq 4)$, ovvero la probabilità di ottenere al massimo 4 successi su 10 tentativi, possiamo utilizzare la funzione `pbinom`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calcolo della probabilità cumulativa\ntarget_probability <- pbinom(4, size = 10, prob = 0.2)\nprint(target_probability)\n#> [1] 0.967\n```\n:::\n\n\n\n\n\nIl risultato rappresenta la probabilità cumulativa associata a 4 o meno successi.\n\nSe invece vogliamo determinare il numero di successi corrispondente a questa probabilità cumulativa, possiamo utilizzare la funzione inversa `qbinom`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calcolo del numero di successi associato alla probabilità cumulativa\nresult <- qbinom(target_probability, size = 10, prob = 0.2)\nprint(result)\n#> [1] 4\n```\n:::\n\n\n\n\n\nIn questo caso, il valore restituito rappresenta il numero massimo di successi $Y$ per cui la probabilità cumulativa è uguale o inferiore a $target\\_probability$. Questo è particolarmente utile per interpretare i risultati di una distribuzione binomiale in termini di successi associati a determinate probabilità cumulative.\n\n### Valore atteso e deviazione standard\n\nLa media (numero atteso di successi in $n$ prove) e la deviazione standard di una distribuzione binomiale si calcolano nel seguente modo:\n\n$$\n\\begin{align}\n\\mu    &= np,  \\notag \\\\\n\\sigma &= \\sqrt{np(1-p)}.\n\\end{align}\n$$ {#eq-mean-var-binomial}\n\n*Dimostrazione.* Dato che $Y$ rappresenta la somma di $n$ prove di Bernoulli indipendenti $Y_i$, possiamo scrivere:\n\n$$\n\\begin{align}\n\\mathbb{E}(Y) &= \\mathbb{E}\\left( \\sum_{i=1}^n Y_i \\right) = \\sum_{i=1}^n \\mathbb{E}(Y_i) = np, \\\\\n\\mathbb{V}(Y) &= \\mathbb{V} \\left( \\sum_{i=1}^n Y_i \\right) = \\sum_{i=1}^n \\mathbb{V}(Y_i) = np(1-p).\n\\end{align}\n$$\n\nPertanto, la deviazione standard è data da $\\sigma = \\sqrt{np(1-p)}$.\n\nPer esempio, prendiamo in considerazione il caso di un esperimento in cui vengono lanciate quattro monete, ciascuna con una probabilità di ottenere testa (successo) pari a $p = 0.2$. Calcoliamo il valore atteso e la varianza per questo esperimento.\n\nIl valore atteso, $\\mu$, rappresenta il numero medio di teste che ci aspettiamo di ottenere in ciascun lancio. Per la distribuzione binomiale, questo è dato da $\\mu = n p$, dove $n$ è il numero di prove (lanci di monete). Nel nostro caso, con $n = 4$ e $p = 0.2$, abbiamo:\n\n$$\n\\mu = n p = 4 \\times 0.2 = 0.8.\n$$\n\nQuesto significa che, in media, ci aspettiamo di ottenere circa 0.8 teste per ogni serie di quattro lanci.\n\nPer quanto riguarda la varianza, che misura quanto i risultati individuali tendono a differire dalla media, nella distribuzione binomiale è calcolata come $n p (1-p)$. Pertanto, per il nostro esperimento:\n\n$$\n\\text{Varianza} = n p (1-p) = 4 \\times 0.2 \\times (1 - 0.2) = 0.64.\n$$\n\nLa varianza di 0.64 suggerisce una certa dispersione intorno al valore medio di 0.8 teste.\n\nPer confermare queste aspettative teoriche, possiamo eseguire una simulazione. Creiamo una serie di esperimenti simulati in cui lanciamo quattro monete per un gran numero di volte, registrando il numero di teste ottenute in ogni serie. Calcoliamo poi la media e la varianza dei risultati ottenuti per vedere quanto si avvicinano ai valori teorici calcolati.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\n\n# Genera un campione di 1.000.000 di valori dalla distribuzione binomiale\nx <- rbinom(n = 1000000, size = 4, prob = 0.2)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(x)\n#> [1] 0.8\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nvar(x)\n#> [1] 0.639\n```\n:::\n\n\n\n\n\n### Funzioni R associate alle distribuzioni di probabilità\n\nLa seguente tabella riassume le funzioni di R utilizzate per manipolare le distribuzioni di probabilità, illustrando i casi della distribuzione Binomiale e della Normale.\n\n| Tipo                              | Esempio: Binomiale ($y \\mid n, p$)         | Esempio: Normale ($y \\mid \\mu, \\sigma$)        |\n|:----------------------------------|:----------------------------------------------|:--------------------------------------------------|\n| Funzione di verosimiglianza       | `dbinom(y, size = n, prob = p)`               | `dnorm(y, mean = μ, sd = σ)`                      |\n| Prob $Y = y$                  | `dbinom(y, size = n, prob = p)`               | Non definita (variabili continue hanno pdf, non pmf) |\n| Prob $Y \\geq y, Y \\leq y, y_1 < Y < y_2$ | `pbinom(y, size = n, prob = p)` o `1 - pbinom(y - 1, ...)` | `pnorm(y, mean = μ, sd = σ)` o `1 - pnorm(y, ...)` |\n| Inversa della CDF                 | `qbinom(q, size = n, prob = p)`               | `qnorm(q, mean = μ, sd = σ)`                      |\n| Generazione di dati simulati      | `rbinom(n, size = trials, prob = p)`          | `rnorm(n, mean = μ, sd = σ)`                      |\n\nIn seguito, useremo altre distribuzioni come Uniforme, Beta, ecc., ognuna delle quali ha un proprio insieme di funzioni disponibili in R. La sintassi segue uno schema generale comune:\n\n- **`d*`**: Calcola la funzione di densità di probabilità (per distribuzioni continue) o di massa (per distribuzioni discrete). Esempi: `dbinom`, `dnorm`.\n- **`p*`**: Calcola la funzione di ripartizione cumulativa (CDF). Esempi: `pbinom`, `pnorm`.\n- **`q*`**: Calcola l'inversa della funzione di ripartizione cumulativa (quantile function). Esempi: `qbinom`, `qnorm`.\n- **`r*`**: Genera campioni casuali secondo una determinata distribuzione. Esempi: `rbinom`, `rnorm`.\n\n\n::: {#exr-}\n\n1. Calcolare la probabilità di esattamente $y = 3$ successi su $n = 5$ prove con $p = 0.5$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndbinom(3, size = 5, prob = 0.5)\n#> [1] 0.312\n```\n:::\n\n\n\n\n\n2. Calcolare la probabilità cumulativa $P(Y \\leq 3)$:\n   \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npbinom(3, size = 5, prob = 0.5)\n#> [1] 0.812\n```\n:::\n\n\n\n\n\n3. Calcolare il valore minimo $y$ tale che $P(Y \\leq y) \\geq 0.9$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqbinom(0.9, size = 5, prob = 0.5)\n#> [1] 4\n```\n:::\n\n\n\n\n\n4. Generare un campione di 100 numeri casuali da una distribuzione binomiale:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrbinom(100, size = 5, prob = 0.5)\n#>   [1] 2 2 4 1 3 2 3 2 2 2 3 3 3 3 1 4 1 1 0 2 2 2 4 1 2 3 3 1 5 2 3 3 0 3 2 3\n#>  [37] 4 2 3 3 3 3 1 5 3 3 2 3 1 2 1 3 3 2 2 4 1 4 2 3 1 4 2 2 3 4 1 1 4 2 3 2\n#>  [73] 3 3 3 1 4 4 3 3 4 3 3 3 2 2 3 3 2 4 4 3 2 2 0 3 1 3 1 2\n```\n:::\n\n\n\n\n:::\n\n## Distribuzione Discreta Uniforme\n\nLa distribuzione discreta uniforme è un tipo particolare di distribuzione di probabilità, dove ogni risultato in un insieme finito e discreto $S$ ha la stessa probabilità $p$ di verificarsi. Questa distribuzione è caratterizzata dalla sua semplicità e dalla sua proprietà fondamentale di equiprobabilità.\n\nConsideriamo un esempio pratico con una variabile casuale discreta $X$, che può assumere valori nell'insieme $\\{1, 2, \\dots, N\\}$. Un'istanza classica di questa distribuzione si verifica quando si sceglie casualmente un numero intero tra 1 e $N$, inclusi. Se $X$ rappresenta il numero selezionato, allora la somma delle probabilità di tutti i possibili valori di $X$ deve totalizzare 1, come indicato dalla formula di normalizzazione:\n\n$$\n\\sum_{i=1}^N P(X_i) = Np = 1.\n$$\n\nDi conseguenza, la probabilità che $X$ assuma un valore specifico $x$ è uniformemente distribuita:\n\n$$\nP(X = x) = \\frac{1}{N},\n$$\n\nindicando che ogni evento ha la stessa probabilità di verificarsi.\n\nIl valore atteso, o la media, di $X$ ci dà un'idea del risultato medio atteso e si calcola come:\n\n$$\n\\mathbb{E}(X) = \\sum_{x=1}^N x \\cdot \\frac{1}{N} = \\frac{1}{N} \\cdot \\sum_{x=1}^N x.\n$$\n\nA questo punto, dobbiamo calcolare la somma $\\sum_{x=1}^{N} x$, che è la somma dei primi $N$ numeri naturali. Questa somma è data dalla formula:\n\n$$\n\\sum_{x=1}^{N} x = \\frac{N(N + 1)}{2}.\n$$\n\nSostituendo questa formula nel nostro calcolo del valore atteso, otteniamo:\n\n$$\n\\mathbb{E}(X) = \\frac{1}{N} \\cdot \\frac{N(N + 1)}{2} = \\frac{N + 1}{2}.\n$$\n\nQuindi, abbiamo dimostrato che il valore atteso $ \\mathbb{E}(X) $ per una variabile casuale $X$ che assume valori interi uniformemente distribuiti da 1 a $N$ è $\\frac{N + 1}{2}$. \n\nPer determinare quanto i valori di $X$ si disperdono attorno al valore medio, calcoliamo la varianza. Il primo passo è calcolare $\\mathbb{E}(X^2)$, il valore atteso del quadrato di $X$. Per una variabile casuale discreta uniforme, questo si ottiene moltiplicando ogni valore al quadrato per la sua probabilità (che è $1/N$ per tutti i valori) e sommando i risultati:\n\n$$\n\\mathbb{E}(X^2) = \\frac{1}{N} \\cdot \\sum_{x=1}^N x^2\n$$\n\nUsando l'identità per la somma dei quadrati dei primi $N$ numeri naturali:\n\n$$\n1^2 + 2^2 + \\dots + N^2 = \\frac{N(N + 1)(2N + 1)}{6}\n$$\n\npossiamo sostituirla per trovare $\\mathbb{E}(X^2)$:\n\n$$\n\\mathbb{E}(X^2) = \\frac{1}{N} \\cdot \\frac{N(N + 1)(2N + 1)}{6} = \\frac{(N + 1)(2N + 1)}{6}\n$$\n\nLa varianza di $X$, denotata con $\\mathbb{V}(X)$, si calcola usando la formula:\n\n$$\n\\mathbb{V}(X) = \\mathbb{E}(X^2) - [\\mathbb{E}(X)]^2\n$$\n\nAbbiamo già stabilito che $\\mathbb{E}(X) = \\frac{N + 1}{2}$ e $\\mathbb{E}(X^2) = \\frac{(N + 1)(2N + 1)}{6}$. Sostituendo questi valori nella formula della varianza, otteniamo:\n\n$$\n\\mathbb{V}(X) = \\frac{(N + 1)(2N + 1)}{6} - \\left(\\frac{N + 1}{2}\\right)^2\n$$\n\nPer semplicare l'espressione della varianza, dobbiamo sottrarre il quadrato di $\\mathbb{E}(X)$ da $\\mathbb{E}(X^2)$:\n\n$$\n\\begin{align*}\n\\mathbb{V}(X) &= \\frac{(N + 1)(2N + 1)}{6} - \\left(\\frac{N + 1}{2}\\right)^2 \\\\\n&= \\frac{(N + 1)(2N + 1)}{6} - \\frac{(N + 1)^2}{4} \\\\\n&= \\frac{2(N + 1)(2N + 1)}{12} - \\frac{3(N + 1)^2}{12} \\\\\n&= \\frac{(N + 1)(2(2N + 1) - 3(N + 1))}{12} \\\\\n&= \\frac{(N + 1)(4N + 2 - 3N - 3)}{12} \\\\\n&= \\frac{(N + 1)(N - 1)}{12}\n\\end{align*}\n$$\n\nQuindi, la varianza $\\mathbb{V}(X)$ di una variabile casuale uniforme discreta $X$ che assume valori da 1 a $N$ è $\\frac{(N + 1)(N - 1)}{12}$, il che mostra come la dispersione dei valori attorno al loro valore medio dipenda dalla grandezza di $N$. Questa formula fornisce la varianza di una variabile casuale in una distribuzione discreta uniforme, offrendo una misura quantitativa della dispersione dei valori attorno al loro valore medio. \n\n\n### Distribuzione di Poisson \n\nLa **distribuzione di Poisson** è utilizzata per modellare il numero di eventi che si verificano in un determinato intervallo di tempo o spazio, con eventi indipendenti e un tasso costante di occorrenza.\n\nLa funzione di massa di probabilità (PMF) è data da:\n\n$$\nP(Y = y \\mid \\lambda) = \\frac{\\lambda^y \\cdot e^{-\\lambda}}{y!}, \\quad y = 0, 1, 2, \\ldots\n$$\n\ndove $\\lambda$ rappresenta il tasso medio di eventi e $y$ è il numero di eventi.\n\n#### Proprietà principali\n\n- **Media**: $\\mathbb{E}[Y] = \\lambda$\n- **Varianza**: $\\text{Var}(Y) = \\lambda$\n\nDi seguito, presentiamo esempi di calcolo e simulazione con R.\n\n\n#### Grafico della distribuzione di Poisson con $\\lambda = 2$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parametro lambda\nlambda <- 2\n\n# Valori di y (numero di eventi)\ny <- 0:10\n\n# Calcolo delle probabilità\nprobabilities <- dpois(y, lambda = lambda)\n\n# Grafico della funzione di massa di probabilità\nbarplot(probabilities, names.arg = y, col = \"blue\", \n        xlab = \"Numero di eventi (k)\", ylab = \"Probabilità\", \n        main = \"Distribuzione di Massa di Probabilità di Poisson\")\n```\n\n::: {.cell-output-display}\n![](11_discr_rv_distr_files/figure-html/unnamed-chunk-23-1.png){width=576}\n:::\n:::\n\n\n\n\n\n#### Calcolo della probabilità per un numero specifico di eventi\n\nPer calcolare la probabilità di osservare esattamente 3 eventi con $\\lambda = 2$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprob <- dpois(3, lambda = 2)\nprint(prob)\n#> [1] 0.18\n```\n:::\n\n\n\n\n\n#### Calcolo della probabilità cumulativa $P(Y \\leq 3)$\n\nPer calcolare $P(Y \\leq 3)$, la probabilità cumulativa:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncum_prob <- ppois(3, lambda = 2)\nprint(cum_prob)\n#> [1] 0.857\n```\n:::\n\n\n\n\n\n#### Trovare il quantile corrispondente a una probabilità data\n\nPer trovare il numero massimo di eventi per cui la probabilità cumulativa è al massimo $0.8125$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile <- qpois(0.8125, lambda = 2)\nprint(quantile)\n#> [1] 3\n```\n:::\n\n\n\n\n\n#### Generazione di numeri casuali\n\nPer generare un campione di 1.000.000 di osservazioni da una distribuzione di Poisson con $\\lambda = 2$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nsample <- rpois(1000000, lambda = 2)\n\n# Calcolo di media e varianza del campione\nmean_sample <- mean(sample)\nvar_sample <- var(sample)\n\nprint(mean_sample)\n#> [1] 2\nprint(var_sample)\n#> [1] 2\n```\n:::\n\n\n\n\n\n::: {#exr-}\n\nConsideriamo un ospedale con una media storica di 4,5 nascite al giorno. Qual è la probabilità che nascano esattamente 6 bambini in un giorno?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calcolo della probabilità\nlambda <- 4.5\nprob <- dpois(6, lambda = lambda)\nprint(prob)\n#> [1] 0.128\n```\n:::\n\n\n\n\n\nSimuliamo 365 giorni di nascite e confrontiamo la proporzione di giorni con esattamente 6 nascite:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nn_days <- 365\nsimulated_births <- rpois(n_days, lambda = lambda)\n\n# Proporzione di giorni con esattamente 6 nascite\nproportion_six_births <- mean(simulated_births == 6)\nprint(proportion_six_births)\n#> [1] 0.14\n```\n:::\n\n\n\n\n\nIstogramma delle nascite simulate:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(simulated_births, breaks = seq(-0.5, max(simulated_births) + 0.5, by = 1), \n     col = \"blue\", xlab = \"Numero di nascite per giorno\", \n     ylab = \"Frequenza\", main = \"365 nascite simulate (Poisson)\")\n```\n\n::: {.cell-output-display}\n![](11_discr_rv_distr_files/figure-html/unnamed-chunk-30-1.png){width=576}\n:::\n:::\n\n\n\n\n\nProbabilità di più di 6 nascite in un giorno. Per calcolare la probabilità teorica $P(Y > 6)$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprob_more_than_six <- 1 - ppois(6, lambda = lambda)\nprint(prob_more_than_six)\n#> [1] 0.169\n```\n:::\n\n\n\n\n\nProporzione simulata di più di 6 nascite:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nproportion_more_than_six <- mean(simulated_births > 6)\nprint(proportion_more_than_six)\n#> [1] 0.17\n```\n:::\n\n\n\n\n\n:::\n\n\n## La Distribuzione Categorica\n\nLa **distribuzione categorica** è una distribuzione di probabilità discreta utilizzata per modellare eventi con più esiti distinti e non ordinati. È una generalizzazione della distribuzione Bernoulliana, che si limita a due esiti (successo e fallimento), ed è utile in situazioni in cui un evento può produrre uno tra molti esiti, ciascuno con una probabilità associata.\n\n### Definizione e Funzione di Massa di Probabilità\n\nLa distribuzione categorica può essere caratterizzata dalla sua funzione di massa di probabilità (PMF):\n\n$$\np(X = x) = \\mathcal{Categorical}(X \\mid p) = \\prod_{k=1}^K p_k^{I_{x=k}},\n$$\n\ndove:\n\n- $K$ è il numero di esiti possibili,\n- $p_k$ è la probabilità associata al $k$-esimo esito,\n- $I_{x=k}$ è una funzione indicatrice che vale 1 se $x = k$ e 0 altrimenti.\n\nLe probabilità $p_k$ formano un vettore:\n\n$$\np = \n\\begin{pmatrix}\np_1\\\\ \np_2\\\\\n\\dots \\\\ \np_K\n\\end{pmatrix},\n$$\n\nche soddisfa la condizione:\n\n$$\n\\sum_{k=1}^K p_k = 1.\n$$\n\nIn altre parole, la somma delle probabilità di tutti i possibili esiti è pari a 1, come richiesto da qualsiasi distribuzione di probabilità.\n\n### Proprietà Principali\n\n1. **Esiti Multipli**: La distribuzione categorica è adatta per modellare eventi con più di due esiti distinti. Un esempio classico è il lancio di un dado a sei facce, dove ciascun esito ha una probabilità di $\\frac{1}{6}$ nel caso di un dado equo.\n\n2. **Generalizzazione della Distribuzione Bernoulliana**: La distribuzione categorica è una generalizzazione della distribuzione Bernoulliana. In particolare, la distribuzione Bernoulliana rappresenta un caso speciale della distribuzione categorica con due sole categorie ($K = 2$), come il risultato di un lancio di una moneta (testa o croce).\n\n3. **Probabilità in Forma di Simplex**: Le probabilità degli esiti nella distribuzione categorica sono rappresentate da un vettore simplex. Un **simplex** è un vettore di probabilità non negative che sommano a 1, rispettando la condizione fondamentale delle distribuzioni di probabilità.\n\n### Implementazioni in R\n\n- **`sample`**: Permette di campionare da una distribuzione categorica, restituendo uno o più esiti in base alle probabilità specificate. Ad esempio:\n\n  ```r\n  sample(categories, size = n, replace = TRUE, prob = probabilities)\n  ```\n  \n  Dove `categories` è un vettore di esiti, `n` è il numero di campioni, e `probabilities` definisce le probabilità associate a ciascun esito.\n\n- **`rmultinom`**: Funzione per la distribuzione multinomiale. Può essere utilizzata per simulare una distribuzione categorica impostando il numero di prove \\( n = 1 \\). Ad esempio:\n\n  ```r\n  rmultinom(1, size = 1, prob = probabilities)\n  ```\n  Qui, `probabilities` specifica le probabilità per ciascun esito. Restituisce il numero di successi per ciascuna categoria in una matrice.\n\n## Riflessioni Conclusive\n\nIn questo capitolo, abbiamo esplorato diverse distribuzioni discrete fondamentali, ciascuna con le sue specifiche applicazioni e peculiarità. Abbiamo iniziato con la distribuzione Bernoulliana, che modella esperimenti con due possibili esiti, come il lancio di una moneta. Abbiamo poi approfondito la distribuzione Binomiale, una generalizzazione della Bernoulliana, che si focalizza sul conteggio del numero di successi in un dato numero di prove indipendenti.\n\nAbbiamo anche esaminato la distribuzione Beta-Binomiale, che estende ulteriormente il modello Binomiale incorporando la variabilità nella probabilità di successo, e la distribuzione di Poisson, utilizzata per modellare il numero di eventi che si verificano in un intervallo di tempo o spazio, quando questi eventi sono rari e indipendenti.\n\nInfine, abbiamo discusso la distribuzione Discreta Uniforme, che attribuisce la stessa probabilità a ogni evento in un insieme finito e discreto. Questa distribuzione è particolarmente utile quando non abbiamo ragioni per assegnare probabilità diverse ai diversi esiti.\n\nQueste distribuzioni formano il cuore dell'analisi statistica discreta e trovano applicazione in un'ampia gamma di settori. In particolare, nel contesto dell'analisi bayesiana, la comprensione della distribuzione Binomiale e Beta-Binomiale è cruciale, poiché queste distribuzioni forniscono le basi per l'aggiornamento bayesiano, un concetto chiave che sarà esplorato nei capitoli successivi.\n\nIn conclusione, le distribuzioni discrete forniscono strumenti essenziali e versatili per modellare e analizzare fenomeni caratterizzati da eventi distinti e quantificabili. La comprensione approfondita di queste distribuzioni è cruciale per chiunque desideri esplorare il vasto campo della probabilità e della statistica. \n\n## Esercizi\n\n::: {#exr-discr-rv-1}\n\nPer ciascuna delle distribuzioni di massa di probabilità discusse, utilizza R per:\n\n- creare un grafico della funzione, scegliendo opportunamente i parametri;\n- estrarre un campione di 1000 valori casuali dalla distribuzione e visualizzarlo con un istogramma;\n- calcolare la media e la deviazione standard dei campioni e confrontarle con i valori teorici attesi;\n- stimare l'intervallo centrale del 94% utilizzando i campioni simulati;\n- determinare i quantili della distribuzione per gli ordini 0.05, 0.25, 0.75 e 0.95;\n- scegliendo un valore della distribuzione pari alla media più una deviazione standard, calcolare la probabilità che la variabile aleatoria assuma un valore minore o uguale a questo valore.\n\n:::\n\n## Informazioni sull'Ambiente di Sviluppo {.unnumbered} \n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsessionInfo()\n#> R version 4.4.2 (2024-10-31)\n#> Platform: aarch64-apple-darwin20\n#> Running under: macOS Sequoia 15.2\n#> \n#> Matrix products: default\n#> BLAS:   /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRblas.0.dylib \n#> LAPACK: /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n#> \n#> locale:\n#> [1] C/UTF-8/C/C/C/C\n#> \n#> time zone: Europe/Rome\n#> tzcode source: internal\n#> \n#> attached base packages:\n#> [1] stats     graphics  grDevices utils     datasets  methods   base     \n#> \n#> other attached packages:\n#>  [1] mice_3.17.0       viridis_0.6.5     viridisLite_0.4.2 gridExtra_2.3    \n#>  [5] patchwork_1.3.0   bayesplot_1.11.1  psych_2.4.6.26    scales_1.3.0     \n#>  [9] markdown_1.13     knitr_1.49        lubridate_1.9.4   forcats_1.0.0    \n#> [13] stringr_1.5.1     dplyr_1.1.4       purrr_1.0.2       readr_2.1.5      \n#> [17] tidyr_1.3.1       tibble_3.2.1      ggplot2_3.5.1     tidyverse_2.0.0  \n#> [21] rio_1.2.3         here_1.0.1       \n#> \n#> loaded via a namespace (and not attached):\n#>  [1] gtable_0.3.6      shape_1.4.6.1     xfun_0.49         htmlwidgets_1.6.4\n#>  [5] lattice_0.22-6    tzdb_0.4.0        vctrs_0.6.5       tools_4.4.2      \n#>  [9] generics_0.1.3    parallel_4.4.2    fansi_1.0.6       pan_1.9          \n#> [13] pacman_0.5.1      jomo_2.7-6        pkgconfig_2.0.3   Matrix_1.7-1     \n#> [17] lifecycle_1.0.4   compiler_4.4.2    farver_2.1.2      munsell_0.5.1    \n#> [21] mnormt_2.1.1      codetools_0.2-20  htmltools_0.5.8.1 glmnet_4.1-8     \n#> [25] nloptr_2.1.1      pillar_1.9.0      MASS_7.3-61       iterators_1.0.14 \n#> [29] rpart_4.1.23      boot_1.3-31       mitml_0.4-5       foreach_1.5.2    \n#> [33] nlme_3.1-166      tidyselect_1.2.1  digest_0.6.37     stringi_1.8.4    \n#> [37] labeling_0.4.3    splines_4.4.2     rprojroot_2.0.4   fastmap_1.2.0    \n#> [41] grid_4.4.2        colorspace_2.1-1  cli_3.6.3         magrittr_2.0.3   \n#> [45] survival_3.7-0    utf8_1.2.4        broom_1.0.7       withr_3.0.2      \n#> [49] backports_1.5.0   timechange_0.3.0  rmarkdown_2.29    nnet_7.3-19      \n#> [53] lme4_1.1-35.5     hms_1.1.3         evaluate_1.0.1    rlang_1.1.4      \n#> [57] Rcpp_1.0.13-1     glue_1.8.0        minqa_1.2.8       jsonlite_1.8.9   \n#> [61] R6_2.5.1\n```\n:::\n",
    "supporting": [
      "11_discr_rv_distr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}