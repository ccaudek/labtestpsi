# Fondamenti della probabilità {#sec-prob-sigma-algebra}

::: callout-important
## In questo capitolo imparerai a:

- capire la $\sigma$-algebra e gli assiomi di Kolmogorov;
- applicare le regole fondamentali della probabilità;
- utilizzare elementi del calcolo combinatorio.
:::

::: callout-tip
## Prerequisiti

- Leggere il capitolo *Probability and counting* di **Introduction to Probability** [@blitzstein2019introduction]. 
- Leggere l'appendice sec-apx-combinatorics.
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(readr, lubridate, reshape2)
```
:::

## Introduzione 

Nel @sec-prob-spaces abbiamo introdotto la teoria della misura e della probabilità su insiemi con un numero finito di elementi. Tuttavia, molti degli spazi matematici che incontriamo nelle applicazioni pratiche, come gli interi e la retta reale, non hanno un numero finito di elementi, ma piuttosto un numero numerabile infinito o addirittura non numerabile infinito di elementi. Sfortunatamente, estendere la teoria della misura e della probabilità a spazi più generali come questi non è sempre semplice.

Senza entrare nei dettagli, è stato dimostrato che la forma più generale della teoria della misura e della probabilità applicabile a qualsiasi spazio matematico è chiamata $\sigma$-algebra. In questo capitolo, forniremo un'introduzione intuitiva ai vincoli delle $\sigma$-algebre ed esamineremo alcune notevoli applicazioni. In particolare, introdurremo i concetti di variabile casuale, funzioni di massa di probabilità e funzioni di ripartizione.

Questi concetti sono fondamentali per comprendere come la probabilità e la misura possono essere utilizzate in contesti più complessi, permettendo di estendere le nostre analisi a insiemi infiniti e spazi continui, che sono comuni nelle applicazioni psicologiche.

## $\sigma$-Algebra

Una **$\sigma$-algebra** è una struttura matematica che permette di definire in modo coerente quali sottoinsiemi di un insieme sono "misurabili". 

## Definizione di $\sigma$-Algebra

Una **$\sigma$-algebra** è una collezione di sottoinsiemi di uno spazio $X$ che soddisfa le seguenti proprietà:

- **Chiusura rispetto al complemento**: Se un sottoinsieme $A$ appartiene alla $\sigma$-algebra $\mathcal{F}$, allora anche il suo complemento $A^c$ appartiene a $\mathcal{F}$. Questo significa che se $\mathcal{F}$ contiene un certo sottoinsieme, deve contenere anche tutti gli elementi che non sono in quel sottoinsieme.

- **Chiusura rispetto alle unioni numerabili**: Se una sequenza numerabile di sottoinsiemi $A_1, A_2, A_3, \ldots$ appartiene alla $\sigma$-algebra $\mathcal{F}$, allora anche l'unione di tutti questi sottoinsiemi appartiene a $\mathcal{F}$. Questo implica che se $\mathcal{F}$ contiene una serie di sottoinsiemi, deve contenere anche il loro insieme unito.

- **Inclusione dello spazio campionario**: Lo spazio campionario $X$ stesso deve appartenere alla $\sigma$-algebra $\mathcal{F}$. In altre parole, l'intero insieme $X$ è considerato un sottoinsieme misurabile.

La *chiusura* in questo contesto significa che la collezione $\mathcal{F}$ è stabile rispetto a determinate operazioni insiemistiche. In particolare, se si applicano le operazioni di complemento o di unione numerabile a elementi della $\sigma$-algebra, i risultati di queste operazioni rimarranno all'interno della stessa $\sigma$-algebra. Questo garantisce che la $\sigma$-algebra non "perda" elementi a causa di queste operazioni, mantenendo così la coerenza e la completezza della collezione di sottoinsiemi.

### Spazio Misurabile

Un insieme dotato di una $\sigma$-algebra, $(X, \mathcal{X})$, è detto **spazio misurabile**. Gli elementi di una $\sigma$-algebra sono noti come **sottoinsiemi misurabili**, mentre i sottoinsiemi non appartenenti alla $\sigma$-algebra sono detti **non misurabili**. La distinzione tra sottoinsiemi misurabili e non misurabili è cruciale per evitare comportamenti anomali e controintuitivi nella teoria della misura e della probabilità.

## Gli Assiomi di Kolmogorov

Una volta definita una $\sigma$-algebra, è possibile introdurre la probabilità come una misura definita su questa collezione di sottoinsiemi. Gli assiomi di Kolmogorov stabiliscono le proprietà fondamentali che ogni funzione di probabilità deve soddisfare:

1. **Non negatività**: Per qualsiasi evento $A$ nello spazio campionario $\Omega$, la probabilità di $A$ è non negativa.

   $$
   P(A) \geq 0.
   $$

2. **Normalizzazione**: La probabilità dell'intero spazio campionario $\Omega$ è 1.

   $$
   P(\Omega) = 1.
   $$

3. **Additività numerabile**: Per qualsiasi sequenza di eventi mutuamente esclusivi ${A_i}_{i=1}^\infty \subset \mathcal{F}$ (cioè $A_i \cap A_j = \varnothing$ per $i \neq j$), la probabilità della loro unione è la somma delle loro probabilità.

   $$
   P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i).
   $$

### Connessione tra $\sigma$-Algebra e Probabilità

Gli assiomi di Kolmogorov richiedono una $\sigma$-algebra $\mathcal{F}$ come base per definire una misura di probabilità $P$. La $\sigma$-algebra delimita l'insieme di sottoinsiemi dello spazio $X$ per i quali la probabilità è ben definita.

1. **Non negatività** La misura di probabilità assegna un valore non negativo a ogni evento in $\mathcal{F}$.
2. **Normalizzazione** garantisce che $P(\Omega) = 1$, garantendo coerenza nella distribuzione della probabilità.
3. **Additività numerabile**: La chiusura della $\sigma$-algebra rispetto alle unioni numerabili consente di applicare l'additività anche a collezioni infinite di eventi.

In sintesi, gli assiomi di Kolmogorov richiedono una $\sigma$-algebra come struttura all'interno della quale queste proprietà valgono. La $\sigma$-algebra è quindi la collezione di eventi per i quali la misura di probabilità è ben definita e coerente con gli assiomi di Kolmogorov.

#### Esempio di $\sigma$-Algebra 

Consideriamo lo spazio campionario $\Omega = {1, 2, 3}$. Una possibile $\sigma$-algebra su $\Omega$ è:

$\mathcal{F}$ = {$\varnothing$,{1},{2,3}, {1, 2, 3}}.

Questa $\sigma$-algebra soddisfa tutte le proprietà richieste:

- Lo spazio campionario $\Omega$ e l'insieme vuoto $\varnothing$ appartengono a $\mathcal{F}$.
- Il complemento di ogni sottoinsieme in $\mathcal{F}$ appartiene ancora a $\mathcal{F}$.
- L'unione di qualsiasi collezione di sottoinsiemi in $\mathcal{F}$ appartiene a $\mathcal{F}$.

In conclusione, la $\sigma$-algebra è un concetto essenziale nella teoria della probabilità, poiché delimita quali eventi possono essere misurati e assegnati una probabilità. Attraverso gli assiomi di Kolmogorov, questa struttura consente di costruire un sistema probabilistico coerente, garantendo che la probabilità sia definita in modo rigoroso e che operazioni come complemento, unione e intersezione numerabili siano sempre ben poste.

## Probabilità

Una volta definiti gli assiomi di Kolmogorov, è possibile introdurre formalmente il concetto di **probabilità**.

La **probabilità** di un evento è una misura numerica che quantifica il grado di fiducia nel verificarsi di tale evento, in accordo con gli assiomi di Kolmogorov. Più precisamente:

- Se $P(A) = 0$, l'evento $A$ è impossibile.
- Se $P(A) = 1$, l'evento $A$ è certo.

Per indicare la probabilità che un evento $A$ **non** si verifichi, si usa la notazione $P(A^c)$, dove:

$$
P(A^c) = 1 - P(A).
$$

### Proprietà Derivate dagli Assiomi di Kolmogorov

Gli assiomi di Kolmogorov implicano alcune proprietà fondamentali, tra cui:

- $P(\varnothing) = 0$ (la probabilità dell'evento impossibile è nulla),
- $0 \leq P(A) \leq 1$ (la probabilità è sempre compresa tra 0 e 1),
- $P(A^c) = 1 - P(A)$ (probabilità del complemento),
- Se $A \subset B$, allora $P(A) \leq P(B)$ (monotonia),
- Se $A \cap B = \varnothing$, allora $P(A \cup B) = P(A) + P(B)$ (additività per eventi incompatibili).

### Regola della Somma

La **regola della somma** permette di calcolare la probabilità della disgiunzione logica ("$A \text{ oppure } B$") tra due eventi $A$ e $B$, considerando se questi siano incompatibili (mutuamente esclusivi) o meno.

#### Caso 1: Eventi incompatibili

Se due eventi $A$ e $B$ **non possono verificarsi contemporaneamente**, essi sono detti **incompatibili** o **mutuamente esclusivi**. In questo caso, la probabilità della loro disgiunzione è semplicemente la somma delle probabilità individuali:

$$
P(A \text{ oppure } B) = P(A) + P(B).
$$

Questa formula riflette il fatto che la probabilità della loro intersezione è nulla:

$$
P(A \cap B) = 0.
$$

Graficamente, gli eventi incompatibili possono essere rappresentati con diagrammi in cui le aree associate agli eventi non si sovrappongono. La probabilità dell'unione è quindi la somma delle aree.

**Esempio: Negazione di un evento**  
Un esempio comune di eventi incompatibili si verifica quando $B$ è il complemento di $A$, cioè $B = A^c$. In questo caso, poiché $A \text{ oppure } A^c$ è sempre vero, la somma delle loro probabilità deve essere 1:

$$
P(A) + P(A^c) = 1.
$$

Da cui segue che:

$$
P(A^c) = 1 - P(A).
$$

#### Caso 2: Eventi non incompatibili

Quando $A$ e $B$ **non sono incompatibili** (cioè possono verificarsi contemporaneamente), la regola della somma deve tenere conto della loro intersezione. La probabilità della disgiunzione si calcola come:

$$
P(A \text{ oppure } B) = P(A) + P(B) - P(A \cap B).
$$

Il termine $P(A \cap B)$ rappresenta la probabilità che entrambi gli eventi si verifichino, e viene sottratto per evitare di conteggiarlo due volte.

---

### Legge della Probabilità Totale

La **legge della probabilità totale** permette di esprimere la probabilità di un evento $A$ come somma delle probabilità condizionate rispetto a un'eventuale partizione dello spazio campione.

Se $B$ e $B^c$ formano una partizione dello spazio campione (ossia sono incompatibili e coprono l'intero spazio), allora:

$$
P(A) = P(A \cap B) + P(A \cap B^c).
$$

Utilizzando la definizione di probabilità condizionata, possiamo scrivere:

$$
P(A) = P(A \mid B) P(B) + P(A \mid B^c) P(B^c).
$$

Questo principio è particolarmente utile quando si conosce la probabilità condizionata di $A$ dato uno o più eventi $B$.

Analogamente, per l'evento $B$:

$$
P(B) = P(B \cap A) + P(B \cap A^c).
$$

In conclusione, gli assiomi di Kolmogorov forniscono una base rigorosa per il calcolo delle probabilità, garantendo coerenza e logica nella loro applicazione. La regola della somma e la legge della probabilità totale sono strumenti fondamentali per calcolare le probabilità in scenari più complessi, in cui gli eventi possono essere interdipendenti o condizionati. Questi principi permettono di affrontare problemi reali in modo strutturato e sistematico.

## Probabilità e Calcolo Combinatorio

I problemi scolastici più comuni sulle probabilità richiedono l'uso del calcolo combinatorio. La struttura generale di questi problemi è sempre la stessa: dobbiamo contare il numero di modi in cui un evento compatibile con l'evento di "successo" definito dal problema si realizza e poi trovare la proporzione di tali eventi rispetto a tutti gli eventi possibili (inclusi quelli di "insuccesso") che possono verificarsi nello spazio campionario. Questi problemi presentano due difficoltà principali:

1. Trasformare la descrizione verbale del problema in una formulazione matematica chiara, suddividendo gli eventi possibili nello spazio campionario in base alle condizioni di successo e insuccesso definite dal problema.
2. Contare il numero di successi e il numero totale di eventi.

Per risolvere questi problemi, dobbiamo utilizzare tecniche del calcolo combinatorio, come le permutazioni e le combinazioni, che ci permettono di contare in modo preciso il numero di possibilità.

Consideriamo un esempio semplice e intuitivo per chiarire il concetto. Supponiamo di avere una scatola con 10 palline numerate da 1 a 10. Vogliamo calcolare la probabilità di estrarre una pallina con un numero pari.

1. **Definizione degli eventi**: In questo caso, l'evento di "successo" è l'estrazione di una pallina con un numero pari.
   - Eventi di successo: {2, 4, 6, 8, 10}
   - Eventi totali: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}

2. **Conteggio delle possibilità**:
   - Numero di eventi di successo: 5
   - Numero totale di eventi: 10

3. **Calcolo della probabilità**:
   $$
   P(\text{numero pari}) = \frac{\text{numero di eventi di successo}}{\text{numero totale di eventi}} = \frac{5}{10} = 0.5.
   $$

Per problemi più complessi, come il calcolo della probabilità di ottenere una determinata combinazione di carte da un mazzo o di formare un particolare gruppo di persone da una popolazione più grande, utilizziamo strumenti del calcolo combinatorio. 

## Il Problema dei Fratelli Bernoulli

La soluzione dei problemi di probabilità non è sempre semplice e nella storia della matematica ci sono molti esempi di celebri matematici che hanno commesso errori. Uno di questi aneddoti riguarda Jakob Bernoulli, uno dei pionieri della teoria della probabilità.

Jakob Bernoulli si interessò al calcolo delle probabilità mentre cercava di formalizzare le leggi del caso nel suo libro "Ars Conjectandi", pubblicato postumo nel 1713. Uno dei problemi che affrontò riguardava il calcolo della probabilità di ottenere almeno una testa in 8 lanci di una moneta equa. Nonostante il suo approccio iniziale fosse corretto, Bernoulli commise un errore nel calcolo combinatorio durante il processo.

Per risolvere il problema di calcolare la probabilità di ottenere almeno una testa in 8 lanci, bisogna considerare la probabilità complementare, ovvero la probabilità di non ottenere alcuna testa (ottenere solo croci) in 8 lanci, e poi sottrarla da 1:

1. **Calcolo della probabilità complementare**: La probabilità di ottenere solo croci in un singolo lancio è $\frac{1}{2}$. La probabilità di ottenere solo croci in 8 lanci consecutivi è:
   $$
   \left(\frac{1}{2}\right)^8 = \frac{1}{256}.
   $$

2. **Calcolo della probabilità di ottenere almeno una testa**:
   $$
   P(\text{almeno una testa}) = 1 - P(\text{nessuna testa}) = 1 - \frac{1}{256} = \frac{255}{256}.
   $$

Jakob Bernoulli commise un errore nel calcolo combinatorio che lo portò a una soluzione errata. Egli sottostimò la probabilità di ottenere almeno una testa, probabilmente a causa di un errore nel conteggio delle possibili combinazioni di successi e insuccessi.

Questo errore fu successivamente corretto da altri matematici, tra cui suo nipote Daniel Bernoulli, che dimostrarono il metodo corretto per risolvere tali problemi utilizzando il calcolo combinatorio in modo appropriato.

La storia del calcolo combinatorio e della probabilità è ricca di aneddoti, come quello di Jakob Bernoulli, che mettono in luce quanto i problemi di probabilità possano essere estremamente controintuitivi, persino per i grandi matematici. Oggi, grazie al lavoro e alle correzioni apportate dai matematici del passato, siamo in grado di risolvere molti di questi problemi con maggiore facilità. La teoria della probabilità, come molte altre discipline scientifiche, è il risultato di un lungo processo di sviluppo e comprensione, che ha richiesto tempo e sforzi considerevoli.

Una delle sfide della probabilità è che spesso i problemi non si prestano a soluzioni immediate o intuitive. Tuttavia, esistono due approcci fondamentali per affrontarli. Il primo consiste nell'applicare i teoremi della teoria della probabilità, un metodo che, come abbiamo visto, può risultare controintuitivo. Il secondo approccio è quello della simulazione Monte Carlo, che consente di ottenere una soluzione approssimata, ma molto vicina al valore reale, seguendo una procedura più intuitiva. Il nome di questo metodo deriva dal famoso Casinò di Monte Carlo a Monaco, ma possiamo semplicemente riferirci ad esso come metodo di simulazione.

La simulazione Monte Carlo è una classe generale di metodi stocastici, in contrasto con i metodi deterministici, utilizzati per risolvere approssimativamente problemi analitici attraverso la generazione casuale delle quantità di interesse. Tra i metodi comunemente utilizzati troviamo il campionamento con reinserimento, in cui la stessa unità può essere selezionata più volte, e il campionamento senza reinserimento, in cui ogni unità può essere selezionata una sola volta. Questi strumenti offrono un potente mezzo per affrontare problemi complessi in modo pratico e accessibile.

::: {#exm-}

Consideriamo il seguente esercizio che presenta un "classico" problema di calcolo delle probabilità.

"Un'urna contiene 10 palline rosse, 10 palline blu e 20 palline verdi. Se si estraggono 5 palline a caso senza reinserimento, qual è la probabilità che venga selezionata almeno una pallina di ciascun colore?"

La soluzione al problema consiste nel contare il numero di modi in cui possono verificarsi gli eventi incompatibili con la condizione richiesta, dividere per il numero totale di modi in cui 5 palline possono essere estratte da un'urna con 40 palline, e sottrarre tale risultato da 1.

Iniziamo dal denominatore ("in quanti modi possono essere estratte 5 palline da un'urna che ne contiene 40"). La soluzione è data dal coefficiente binomiale: $\binom{40}{5}$.

Dobbiamo poi enumerare tutti i casi incompatibili con la condizione espressa dal problema; al numeratore avremo quindi: (modi di ottenere nessuna pallina rossa) + (modi di ottenere nessuna pallina blu) + (modi di ottenere nessuna pallina verde) - (modi di ottenere nessuna pallina rossa o blu) - (modi di ottenere nessuna pallina rossa o verde) - (modi di ottenere nessuna pallina blu o verde).

La soluzione è dunque:

$$
P(\text{almeno una rossa, blu e verde}) = \frac{
\binom{30}{5} + \binom{30}{5} + \binom{20}{5} - \binom{20}{5} - \binom{10}{5} - \binom{10}{5}
}{\binom{40}{5}} = 0.568.
$$

Lo stesso risultato si ottiene con una simulazione.

```{r}
set.seed(12345)

# Creare un'urna con le palline
urn <- c(rep("red", 10), rep("blue", 10), rep("green", 20))

# Numero di simulazioni
simulations <- 100000

count <- 0
for (i in 1:simulations) {
  # Estrarre 5 palline dall'urna
  draw <- sample(urn, 5, replace = FALSE)
  
  # Verificare se c'è almeno una pallina di ogni colore (red, blue, green)
  if ("red" %in% draw && "blue" %in% draw && "green" %in% draw) {
    count <- count + 1
  }
}

# Calcolare la probabilità simulata
prob_simulated <- count / simulations
prob_simulated
```

:::

Il metodo di simulazione consente di risolvere problemi che implicano il calcolo delle probabilità relative a vari eventi generati dal lancio dei dadi.

::: {#exm-}

Il problema dei compleanni, generalmente attribuito a Richard von Mises, è un noto esempio controintuitivo di calcolo delle probabilità che utilizza il calcolo combinatorio, in particolare le permutazioni. Il problema chiede quanti individui sono necessari affinché la probabilità che almeno due persone abbiano lo stesso compleanno superi il 50%, assumendo che ogni giorno dell'anno sia ugualmente probabile come compleanno. Sorprendentemente, la risposta è solo 23 persone, molto meno di quanto la maggior parte delle persone immagina.

Per risolvere il problema dei compleanni utilizzando le permutazioni, consideriamo la seguente relazione:

$$
\begin{align*}
P(\text{almeno due persone hanno lo stesso compleanno}) &= \\
1 - P(\text{nessuno ha lo stesso compleanno}).
\end{align*}
$$

Questa uguaglianza è valida perché l'evento "nessuno ha lo stesso compleanno" è il complemento dell'evento "almeno due persone hanno lo stesso compleanno". Pertanto, dobbiamo calcolare la probabilità che nessuno abbia lo stesso compleanno.

Sia $k$ il numero di persone. Per calcolare la probabilità che nessuno abbia lo stesso compleanno, dobbiamo contare il numero di modi in cui $k$ persone possono avere compleanni diversi. Poiché ogni compleanno è ugualmente probabile, possiamo usare le permutazioni per contare il numero di modi in cui $k$ compleanni unici possono essere disposti su 365 giorni:

$$
365P_k = \frac{365!}{(365 - k)!}.
$$

Dividiamo questo numero per il numero totale di elementi nello spazio campionario, che è il numero totale di modi in cui $k$ compleanni possono essere disposti su 365 giorni:

$$
365^k.
$$

Quindi, la probabilità che nessuno abbia lo stesso compleanno è:

$$
P(\text{nessuno ha lo stesso compleanno}) = \frac{365P_k}{365^k} = \frac{365!}{365^k (365 - k)!}.
$$

Usando questa formula, la probabilità che almeno due persone abbiano lo stesso compleanno è:

$$
P(\text{almeno due persone hanno lo stesso compleanno}) = 1 - \frac{365!}{365^k (365 - k)!}.
$$

In sintesi, calcolando questa probabilità, si scopre che bastano solo 23 persone affinché la probabilità che almeno due di loro abbiano lo stesso compleanno superi il 50%, un risultato sorprendente rispetto all'intuizione comune.

```{r}
# Funzione per calcolare la probabilità
birthday <- function(k) {
  logdenom <- k * log(365) + lgamma(365 - k + 1) # log denominatore
  lognumer <- lgamma(366) # log numeratore
  pr <- 1 - exp(lognumer - logdenom) # trasformazione inversa
  return(pr)
}

# Calcola la probabilità per k persone
k <- 1:50
bday <- sapply(k, birthday)

# Plot dei risultati
ggplot(data.frame(k = k, bday = bday), aes(x = k, y = bday)) +
  geom_line(marker = "o", alpha = 0.5) +
  geom_point(alpha = 0.5) +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "gray") +
  labs(
    x = "Numero di persone",
    y = "Probabilità che almeno due persone\nabbiano lo stesso compleanno",
    title = "Probabilità del Problema dei Compleanni"
  ) +
  xlim(0, 50) +
  ylim(0, 1)

# Probabilità per 20-25 persone
bday[20:25]
```

Osserviamo che quando il numero di persone è 23, la probabilità che almeno due persone abbiano lo stesso compleanno supera 0.5. Quando il numero di persone è più di 50, questa probabilità è quasi 1.

:::

::: {#exm-}

In precedenza, abbiamo derivato la soluzione analitica esatta per il problema dei compleanni, ma possiamo ottenere una soluzione approssimata in modo più intuitivo utilizzando il metodo della simulazione Monte Carlo.

La simulazione Monte Carlo si basa su un'idea semplice: simuliamo il processo descritto dal problema e analizziamo i risultati di molte prove per stimare la probabilità cercata.

Passi della simulazione sono i seguenti.

1. **Campionamento casuale:** Simuliamo i compleanni di $k$ persone scegliendo $k$ numeri interi casuali tra 1 e 365, dove ogni numero rappresenta un giorno dell'anno. Questo campionamento avviene con reinserimento: un giorno può essere estratto più volte, perché non esiste alcuna restrizione sul fatto che due o più persone possano nascere lo stesso giorno.

2. **Verifica della condizione:** Controlliamo se almeno due numeri campionati sono uguali, ovvero se almeno due persone condividono lo stesso compleanno.

3. **Ripetizione della simulazione:** Ripetiamo il processo di campionamento molte volte (ad esempio 1000, o anche 1 milione di volte per una stima più precisa).

4. **Stima della probabilità:** Calcoliamo la frazione di simulazioni in cui si è verificato l'evento "almeno due persone condividono un compleanno". Questa frazione è la nostra stima Monte Carlo della probabilità cercata.

Il seguente codice R implementa questo approccio:

```{r}
set.seed(12345) # Imposta il seme per la riproducibilità

k <- 23  # Numero di persone
sims <- 1000  # Numero di simulazioni
event <- 0  # Contatore eventi

# Simulazioni per stimare la probabilità
for (i in 1:sims) {
  days <- sample(1:365, k, replace = TRUE)
  unique_days <- unique(days)
  if (length(unique_days) < k) {
    event <- event + 1
  }
}

# Frazione di prove in cui almeno due compleanni sono uguali
answer <- event / sims
cat(sprintf("Stima della probabilità: %.6f\n", answer))

# Aumentare il numero di simulazioni a un milione per maggiore accuratezza
sims_large <- 1000000
event_large <- 0

for (i in 1:sims_large) {
  days <- sample(1:365, k, replace = TRUE)
  unique_days <- unique(days)
  if (length(unique_days) < k) {
    event_large <- event_large + 1
  }
}

answer_large <- event_large / sims_large
cat(sprintf("Stima con un milione di simulazioni: %.6f\n", answer_large))
```

Nel codice sopra, abbiamo impostato il numero di simulazioni a un milione. Osserviamo che quando il numero di persone è 23, la probabilità che almeno due persone abbiano lo stesso compleanno è superiore a 0.5. Quando il numero di persone supera 50, questa probabilità è vicina a 1.

:::

## Le Assunzioni nella Soluzione dei Problemi 

Nella realtà, i compleanni non seguono una distribuzione uniforme. Una soluzione migliore al problema dei compleanni sarebbe quella di estrarre i compleanni dalla distribuzione effettiva piuttosto che da una distribuzione uniforme in cui ogni giorno ha la stessa probabilità. Non esiste un metodo matematico standard per calcolare questa probabilità; l'unico modo per farlo è attraverso la simulazione.

Negli Stati Uniti, il CDC e la Social Security Administration monitorano il numero di nascite giornaliere. Nel 2016, [FiveThirtyEight](https://projects.fivethirtyeight.com/polls/) ha pubblicato un articolo sulle frequenze giornaliere di nascita e ha reso disponibili i dati in un file CSV su GitHub. Utilizzando il codice fornito da [Andrew Heiss](https://www.andrewheiss.com/blog/2024/05/03/birthday-spans-simulation-sans-math/), possiamo caricare quei dati e calcolare le probabilità giornaliere dei compleanni negli Stati Uniti.

```{r}
#| echo: false
#| message: false
#| 
# Leggi i dati
births_1994_1999 <- read_csv(
  "https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_1994-2003_CDC_NCHS.csv"
) %>%
  filter(year < 2000)

births_2000_2014 <- read_csv(
  "https://raw.githubusercontent.com/fivethirtyeight/data/master/births/US_births_2000-2014_SSA.csv"
)

# Unisci i dataset
births_combined <- bind_rows(births_1994_1999, births_2000_2014)

# Crea la colonna 'full_date' con un anno fittizio 2024 per mantenere la corretta relazione giorno/mese
births_combined <- births_combined %>%
  mutate(
    full_date = make_date(2024, month, date_of_month),
    day_of_year = yday(full_date),
    month_categorical = month.name[month]
  )

# Calcola la media delle nascite per ciascun giorno del mese per ogni mese
avg_births_month_day <- births_combined %>%
  group_by(month_categorical, date_of_month) %>%
  summarise(avg_births = mean(births, na.rm = TRUE)) %>%
  ungroup()

# Correggi l'ordine dei mesi per l'asse Y
avg_births_month_day <- avg_births_month_day %>%
  mutate(month_categorical = factor(month_categorical, levels = month.name))

# Crea una matrice per la heatmap
avg_births_pivot <- dcast(
  avg_births_month_day,
  month_categorical ~ date_of_month,
  value.var = "avg_births"
)

ggplot(avg_births_month_day, aes(x = date_of_month, y = month_categorical, fill = avg_births)) +
  geom_tile() +
  scale_fill_distiller(palette = "Spectral", name = "Average births") +
  labs(
    title = "Average births per day",
    subtitle = "1994–2014",
    x = "",
    y = ""
  ) +
  theme_minimal() +
  theme(axis.text.y = element_text(angle = 0, hjust = 1))
```

I dati mostrano alcuni pattern interessanti e chiaramente distinguibili:

- **Festività principali**: Durante il periodo natalizio e di Capodanno, le nascite sembrano essere meno frequenti. In particolare, il giorno di Natale, la vigilia di Natale e il giorno di Capodanno registrano il numero medio di nascite più basso.

- **Festività minori e date particolari**: Anche altre festività e occasioni speciali, come la vigilia di Capodanno, Halloween, il 4 luglio, il 1° aprile e l'intera settimana del Ringraziamento, mostrano una diminuzione significativa nelle medie delle nascite.

- **Il giorno 13**: Una leggera riduzione del numero medio di nascite si osserva il 13 di ogni mese. Questo pattern si evidenzia chiaramente nella colonna corrispondente ai giorni 13.

- **Picco di nascite a settembre**: I giorni con il numero medio di nascite più elevato si concentrano a metà settembre, dal 9 al 20, con l’eccezione dell’11 settembre, che mostra un valore inferiore.

È probabile che in Italia i pattern relativi alla distribuzione delle nascite siano almeno in parte diversi, a causa di variazioni culturali, sociali e legate alla gestione sanitaria.

---

Non è l’obiettivo qui riformulare la soluzione del problema dei compleanni utilizzando la distribuzione effettiva delle nascite invece di quella uniforme, ma piuttosto sottolineare un aspetto cruciale: ogni procedura di risoluzione dei problemi probabilistici si basa su **assunzioni**. Nel caso del problema dei compleanni, assumiamo che i compleanni siano distribuiti uniformemente durante l’anno. Tuttavia, questa ipotesi è solo un'approssimazione, poiché la distribuzione reale dei compleanni non è uniforme. 

Le soluzioni che otteniamo nei problemi probabilistici descrivono con maggiore precisione le regolarità del mondo reale quando le assunzioni su cui si basano sono ragionevoli e ben fondate. 

---

Questo principio si applica non solo al problema dei compleanni, ma a tutti i modelli probabilistici e, più in generale, a tutti i modelli scientifici. Ogni modello si basa su una serie di ipotesi che ne delimitano validità e applicabilità. Di conseguenza, è essenziale valutare attentamente la plausibilità di tali ipotesi per garantire che il modello offra una rappresentazione coerente e utile del fenomeno che si intende descrivere.

## Commenti e considerazioni finali

La teoria delle probabilità è un pilastro fondamentale della statistica, con applicazioni pratiche in numerosi campi, tra cui la psicologia. Comprendere le probabilità ci consente di prendere decisioni informate in situazioni di incertezza e di formulare previsioni affidabili. Una solida conoscenza delle basi della probabilità ci permette di affrontare una vasta gamma di problemi e di fare scelte ponderate basate sulla probabilità dei vari esiti possibili. Tuttavia, è importante ricordare che i modelli probabilistici sono solo approssimazioni della realtà e possono essere influenzati da semplificazioni o dalle limitazioni dei dati disponibili. Pertanto, è fondamentale interpretare i risultati con cautela e avere piena consapevolezza delle assunzioni che sottendono le analisi.

## Informazioni sull'Ambiente di Sviluppo {.unnumbered} 

```{r}
sessionInfo()
```



