---
execute:
  freeze: auto
---

# Distribuzioni di v.c. continue {#sec-prob-cont-prob-distr}


**Prerequisiti**

**Concetti e Competenze Chiave**

**Preparazione del Notebook**

```{r}
here::here("code", "_common.R") |> 
  source()

# Load packages
if (!requireNamespace("pacman")) install.packages("pacman")
pacman::p_load(mice)
```

## Introduzione

Analogamente a quanto avviene per le variabili casuali discrete, anche per le variabili casuali continue possiamo rappresentare la variabilità all'interno di una popolazione attraverso un modello statistico, ma in questo caso utilizziamo le densità di probabilità -- si veda il @sec-density-function. Mentre le distribuzioni di probabilità discrete si applicano a fenomeni con un numero finito o numerabile di esiti, le densità di probabilità sono fondamentali per descrivere variabili che possono assumere un continuum di valori.

La funzione di densità di probabilità $f(x)$ associata a una variabile casuale continua $X$ rappresenta la distribuzione della probabilità all'interno della popolazione. Questa funzione non fornisce la probabilità esatta di un singolo valore, ma piuttosto la probabilità di osservare valori di $X$ all'interno di un intervallo specifico. Così come per le distribuzioni discrete, anche le densità di probabilità costituiscono un modello della popolazione, una rappresentazione matematica che ci consente di fare previsioni e di comprendere meglio i fenomeni aleatori continui.

Iniziamo con la distribuzione continua uniforme.

## Distribuzione Uniforme

La distribuzione uniforme è una delle più semplici funzioni di densità di probabilità. Consideriamo di nuovo l'esperimento dello spinner introdotto in precedenza. Simuliamo 20 valori che potrebbero essere ottenuti facendo ruotare lo spinner e li rappresentiamo con un istogramma.

```{r}
# Simulazione di 20 valori
set.seed(123)
spinner_results <- runif(20, min = 0, max = 360)
print(spinner_results)

ggplot(data.frame(Valori = spinner_results), aes(x = Valori)) +
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black", alpha = 0.5) +
  labs(x = "Risultato dello spinner", y = "Frequenza relativa",
       title = "Istogramma dei risultati (20 simulazioni)") 
```

Nonostante possiamo pensare che ogni risultato tra 0 e 360 sia ugualmente probabile, l'istogramma non lo suggerisce chiaramente con solo 20 osservazioni. Simuliamo ora 100.000 ripetizioni.

```{r}
# Simulazione di 100.000 valori
spinner_results_large <- runif(100000, min = 0, max = 360)

# Istogramma
ggplot(data.frame(Valori = spinner_results_large), aes(x = Valori)) +
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black", alpha = 0.5) +
  labs(x = "Risultato dello spinner", y = "Frequenza relativa",
       title = "Istogramma dei risultati (100.000 simulazioni)") 
```

In questo caso, anche se ci sono variazioni nelle altezze delle barre (bin di ampiezza pari a 10), la forma generale dell'istogramma appare piuttosto uniforme su tutto l'intervallo $[0, 360]$. Con un numero enorme di risultati, l'istogramma si avvicinerebbe alla *funzione di densità uniforme* mostrata di seguito.

```{r}
# Curva della funzione di densità uniforme
x <- seq(0, 360, length.out = 100)
density_uniform <- dunif(x, min = 0, max = 360)

ggplot(data.frame(x = x, y = density_uniform), aes(x = x, y = y)) +
  geom_line(size = 1, color = "blue") +
  labs(x = "x", y = "p(x)", title = "Funzione di densità uniforme") 
```

Quando la variabile casuale $X$ è continua, come nel caso dello spinner, la probabilità è rappresentata da una curva, la *funzione di densità di probabilità*. Poiché lo spinner copre l'intervallo $[0, 360]$, la probabilità che $X$ sia compreso in questo intervallo è pari a 1. La densità costante è quindi:

```{r}
1 / 360
```

### Probabilità in un intervallo specifico

La probabilità di ottenere un valore tra 150 e 250, $P(150 < X < 250)$, è data dall'area sottesa alla curva in quell'intervallo. L'altezza della curva è $1/360$, mentre la base è $250 - 150 = 100$. Quindi:

```{r}
100 * (1 / 360)
```

Per calcolare la probabilità, si possono utilizzare le funzioni di distribuzione cumulative:

```{r}
punif(250, min = 0, max = 360) - punif(150, min = 0, max = 360)
```

Visualizzazione dell'intervallo di probabilità:

```{r}
# Visualizzazione della probabilità nell'intervallo [150, 250]
x <- seq(0, 360, length.out = 1000)
fx <- dunif(x, min = 0, max = 360)

ggplot(data.frame(x = x, fx = fx), aes(x = x, y = fx)) +
  geom_line(size = 1, color = "blue") +
  geom_area(data = subset(data.frame(x = x, fx = fx), x >= 150 & x <= 250),
            aes(x = x, y = fx), fill = "gray", alpha = 0.5) +
  labs(x = "x", y = "p(x)", title = "Probabilità per l'intervallo [150, 250]")
```

In maniera più formale possiamo dire che la distribuzione continua uniforme è una distribuzione di probabilità continua che assegna lo stesso grado di fiducia a tutti i possibili valori di una variabile definita in un certo intervallo $S=[a,b]\subset {\mathbb  {R}}$. La distribuzione continua uniforme viene indicata con ${\mathcal  {U}}(a,b)={\mathcal  {U}}([a,b])$. Come intervallo $[a,b]$ viene spesso preso l'intervallo unitario $I=[0,1]$.

La densità di probabilità di una variabile casuale continua uniforme ${\mathcal  {U}}(a,b)$ è

$$
f(x)={\frac  {1}{b-a}} \quad \text{su}\; [a, b].
$$

Il suo valore attesto è

$$
\displaystyle E(X)={\frac {1}{2}}(b+a).
$$

La sua varianza è

$$
V(X)={\frac {1}{12}}(b-a)^{2}.
$$

In R, è possibile manipolare la distribuzione uniforme utilizzando le funzioni della famiglia `runif`, `dunif`, `punif` e `qunif`. Di default, queste funzioni lavorano con la distribuzione uniforme standard $\mathcal{U}(0,1)$. 

### Funzione di densità di probabilità (PDF)

La funzione `dunif()` calcola l'ordinata della funzione di densità per i valori di input specificati. Per esempio, esaminiamo la densità di $\mathcal{U}(0,1)$ per i valori 0.5, 0.8 e 1.2. Ci aspettiamo di ottenere 1 per i primi due valori e 0 per 1.2, che è fuori dall'intervallo $[0, 1]$.

```{r}
dunif(c(0.5, 0.8, 1.2), min = 0, max = 1)
```

### Funzione di ripartizione (CDF)

La funzione `punif()` restituisce il valore della funzione di ripartizione. Per esempio, per $\mathcal{U}(0,1)$ nei punti 0.5 e 0.8:

```{r}
punif(c(0.5, 0.8), min = 0, max = 1)
```

### Calcolo della probabilità in un intervallo

Utilizzando la funzione di ripartizione, possiamo calcolare la probabilità che la variabile casuale continua assuma un valore in un intervallo specificato. Per esempio, per $\mathcal{U}(0,1)$ troviamo $P(0.5 < X < 0.8)$:

```{r}
punif(0.8, min = 0, max = 1) - punif(0.5, min = 0, max = 1)
```

### Calcolo dei quantili

La funzione `qunif()` restituisce i quantili della distribuzione uniforme, ovvero il valore della variabile casuale $X$ in corrispondenza del valore della funzione di ripartizione fornito in input. Per esempio, troviamo i quantili di ordine 0.5 e 0.8 di $\mathcal{U}(0,1)$:

```{r}
qunif(c(0.5, 0.8), min = 0, max = 1)
```

### Simulazione di valori casuali

La funzione `runif()` consente di generare numeri casuali dalla distribuzione uniforme. Per esempio, simuliamo 5 valori casuali da $\mathcal{U}(0,1)$:

```{r}
set.seed(123)  # Per la riproducibilità
runif(5, min = 0, max = 1)
```

### Valore atteso

Per verificare il valore atteso di 100,000 realizzazioni di $\mathcal{U}(0,1)$:

```{r}
mean(runif(100000, min = 0, max = 1))
```

### Varianza

Per calcolare la varianza di 100,000 realizzazioni di $\mathcal{U}(0,1)$:

```{r}
var(runif(100000, min = 0, max = 1))
```

Confrontiamo il valore teorico della varianza per $\mathcal{U}(0,1)$, che è $1/12$:

```{r}
1 / 12
```

In conclusione, le funzioni della famiglia `runif`, `dunif`, `punif` e `qunif` in R consentono di manipolare e analizzare la distribuzione uniforme.

## Distribuzione Gaussiana

La più importante distribuzione di densità è la Gaussiana. Non c'è un'unica distribuzione gaussiana (o Normale): la distribuzione gaussiana è una famiglia di distribuzioni. Tali distribuzioni sono dette "gaussiane" in onore di Carl Friedrich Gauss (uno dei più grandi matematici della storia il quale, tra le altre cose, scoprì l'utilità di tale funzione di densità per descrivere gli errori di misurazione). Adolphe Quetelet, il padre delle scienze sociali quantitative, fu il primo ad applicare tale funzione di densità alle misurazioni dell'uomo. Karl Pearson usò per primo il termine "distribuzione normale" anche se ammise che questa espressione "ha lo svantaggio di indurre le persone a credere che le altre distribuzioni, in un senso o nell'altro, non siano normali."

### Limite delle distribuzioni binomiali

Iniziamo con un un breve excursus storico. Nel 1733, Abraham de Moivre notò che, aumentando il numero di prove di una distribuzione binomiale, la distribuzione risultante diventava quasi simmetrica e a forma campanulare. Per esempio, con 10 prove e una probabilità di successo di 0.9, la distribuzione è chiaramente asimmetrica.

```{r}
# Parametri
n <- 10
p <- 0.9

# Calcolare la distribuzione binomiale
r_values <- 0:n
dist <- dbinom(r_values, size = n, prob = p)

# Grafico
ggplot(data.frame(Successi = r_values, Probabilità = dist), aes(x = Successi, y = Probabilità)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Distribuzione Binomiale: n = 10, p = 0.9", x = "Numero di Successi", y = "Probabilità") 
```

Quando il numero di prove *N* viene aumentato di un fattore di 100 a *N* = 1000, mantenendo costante la probabilità di successo del 90%, si osserva che la distribuzione assume una forma campanulare quasi simmetrica. Questa osservazione porta a una scoperta di de Moivre: quando *N* diventa grande, la funzione gaussiana, nonostante rappresenti la densità di variabili casuali continue, offre una buona approssimazione alla funzione di massa di probabilità binomiale.

```{r}
# Parametri aggiornati
n <- 1000

# Calcolare la distribuzione
r_values <- 850:950  # Intervallo per una migliore visualizzazione
dist <- dbinom(r_values, size = n, prob = p)

# Grafico
ggplot(data.frame(Successi = r_values, Probabilità = dist), aes(x = Successi, y = Probabilità)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(
    title = "Distribuzione Binomiale: n = 1000, p = 0.9", 
    x = "Numero di Successi", 
    y = "Probabilità"
  ) 
```

La distribuzione Normale fu scoperta da Gauss nel 1809. Il Paragrafo successivo illustra come si possa giungere alla Normale mediante una simulazione.

### La Normale prodotta con una simulazione

Il libro "Rethinking Statistics" di @McElreath_rethinking spiega come sia possibile ottenere la distribuzione normale attraverso una simulazione. Immaginiamo di avere duemila persone che si trovano allineate su una linea di partenza. Quando viene dato il segnale di partenza, ogni persona lancia una moneta e compie un passo avanti o indietro a seconda del risultato del lancio. La lunghezza di ogni passo può variare da 0 a 1 metro. Ogni persona lancia la moneta 16 volte e quindi compie 16 passi.

I risultati ottenuti da una serie di passeggiate casuali si traducono in varie distanze dall'origine, che è il punto da cui si parte, contrassegnato come zero, dopo un numero specificato di passi. Queste distanze sono rappresentate numericamente. Al termine di queste passeggiate, non è possibile determinare la posizione esatta di ogni individuo, ma è possibile descrivere accuratamente le caratteristiche della distribuzione delle 1000 distanze dall'origine.

Ad esempio, è possibile prevedere con precisione la frazione di individui che si sono mossi verso in avanti o indietro, o la proporzione di persone che si troveranno a una distanza specifica dal punto di partenza, come a 1.5 metri dall'origine. Queste previsioni sono fattibili perché la distribuzione delle distanze segue una distribuzione Normale.

Il codice presentato di seguito genera passeggiate casuali utilizzando un generatore di numeri casuali e ne traccia i percorsi risultanti. Il codice inizia inizializzando un oggetto generatore di numeri casuali con la funzione `np.random.default_rng()` della libreria `numpy`. Questo generatore sarà usato per produrre numeri casuali uniformemente distribuiti tra -1 e 1, simulando così il lancio di una moneta.

La variabile `steps` specifica il numero di passi per ogni passeggiata casuale, mentre `repetitions` indica il numero di passeggiate da generare. La variabile `show_steps` è un elenco di numeri di passi in cui il codice traccerà linee verticali sul grafico.

Successivamente, il codice crea un array bidimensionale di NumPy chiamato `x` con righe pari a `steps + 1` e colonne pari a `repetitions`. La prima colonna di questo array è riempita di zeri, e le colonne rimanenti sono riempite con la somma cumulativa dei passi, ottenuti da numeri casuali uniformemente distribuiti generati dal generatore di numeri casuali. Questo array verrà utilizzato per memorizzare le posizioni della passeggiata casuale ad ogni passo.

Il codice poi prepara una figura per tracciare tutte le passeggiate casuali. Il codice traccia anche la prima passeggiata casuale in nero.

```{r}
# Parametri
numero_passi <- 16
ripetizioni <- 1000
punti_da_evidenziare <- c(4, 8, 16)

# Generare passeggiate casuali
set.seed(123)
x <- matrix(0, nrow = numero_passi + 1, ncol = ripetizioni)

for (i in 1:ripetizioni) {
  passi <- runif(numero_passi, min = -1, max = 1)
  x[-1, i] <- cumsum(passi)
}

# Grafico delle passeggiate casuali
df <- data.frame(
  Passo = rep(0:numero_passi, times = ripetizioni), 
  Distanza = as.vector(x)
)

# Grafico delle passeggiate casuali
ggplot(
  df, 
  aes(
    x = Passo, 
    y = Distanza, 
    group = rep(1:ripetizioni, each = numero_passi + 1))
  ) +
  geom_line(color = "blue", alpha = 0.05) +
  geom_line(
    data = data.frame(Passo = 0:numero_passi, Distanza = x[, 1], group = 1), 
    aes(x = Passo, y = Distanza, group = group), color = "black") +
  geom_vline(
    xintercept = punti_da_evidenziare, 
    linetype = "dashed", 
    color = "black", 
    alpha = 0.5) +
  labs(
    title = "Passeggiate Casuali", 
    x = "Numero di Passi", 
    y = "Distanza dall'Origine"
  ) 
```

Il grafico riportato qui sotto visualizza la distribuzione dei passi a partire dalla linea mediana dopo 4, 8 e 16 lanci di moneta/passi. Quello che si nota è che, man mano che procediamo nel numero di passi, le densità iniziano a somigliare alla curva a campana associata alle distribuzioni Gaussiane.

```{r}
densities <- lapply(punti_da_evidenziare, function(step) {
  data.frame(Posizione = x[step + 1, ], Passo = step)
})

densities <- bind_rows(densities)

ggplot(densities, aes(x = Posizione, fill = as.factor(Passo))) +
  geom_density(alpha = 0.6) +
  facet_wrap(~ Passo, scales = "free") +
  labs(
    title = "Densità delle Posizioni",
    x = "Posizione",
    y = "Densità",
    fill = "Passo"  # Etichetta per la legenda
  ) +
  theme(
    legend.position = "bottom"  # Sposta la legenda in basso
  )
```

La chiarezza dell'informazione presentata nei grafici precedenti può essere migliorata utilizzando un KDE plot.

```{r}
# Generare i dati
posizioni <- apply(matrix(runif(numero_passi * ripetizioni, min = -1, max = 1), nrow = numero_passi), 2, sum)

# Calcolare media e deviazione standard
media <- mean(posizioni)
dev_std <- sd(posizioni)

# Generare la curva normale
valori <- seq(min(posizioni), max(posizioni), length.out = 1000)
densità_normale <- dnorm(valori, mean = media, sd = dev_std)

# Grafico
ggplot(data.frame(Posizione = posizioni), aes(x = Posizione)) +
  geom_density(fill = "skyblue", alpha = 0.5) +
  geom_line(data = data.frame(Posizione = valori, Densità = densità_normale), aes(x = Posizione, y = Densità), color = "red", linetype = "dashed") +
  labs(title = "Confronto tra Passeggiate Casuali e Normale", x = "Posizione", y = "Densità") 
```

Questa simulazione in luce un principio fondamentale della teoria delle probabilità: ogni processo che coinvolge la somma di una sequenza di valori casuali, tutti estratti dalla stessa distribuzione, inevitabilmente tende verso una distribuzione normale, comunemente conosciuta come curva gaussiana. Questa tendenza si verifica indipendentemente dalla configurazione iniziale della distribuzione di partenza, che può essere uniforme, come nell'esempio menzionato, o di qualsiasi altro tipo. La forma specifica della distribuzione iniziale influisce sulla velocità con cui si verifica questa convergenza verso il comportamento gaussiano, con variazioni significative nella velocità di convergenza: alcuni processi possono manifestare una convergenza lenta, mentre altri possono convergere estremamente rapidamente. Un esempio emblematico di questo fenomeno è rappresentato dal dispositivo conosciuto come [Galton box](https://en.wikipedia.org/wiki/Galton_board), il quale offre una rappresentazione visiva e fisica di come la somma di valori casuali generi una distribuzione normale.

Un modo per razionalizzare la distribuzione Gaussiana è quello di pensare alle medie. Qualunque sia il valore medio della distribuzione di origine, ogni campione da essa può essere considerato una fluttuazione rispetto a quel valore medio. Tuttavia, quando sommiamo queste fluttuazioni insieme, esse si annullano a vicenda. E, facendo ciò, queste fluttuazioni convergono eventualmente alla media delle osservazioni collettive. Non importa quale sia la forma della distribuzione sottostante. A seconda della forma, le somme cumulative convergeranno inevitabilmente sulla media, alcune distribuzioni più lentamente di altre.

Dal punto di vista formale, possiamo definire una variabile casuale continua $Y$ come avente una distribuzione normale se la sua densità di probabilità è distribuita secondo la seguente equazione

$$
f(y; \mu, \sigma) = {1 \over {\sigma\sqrt{2\pi} }} \exp \left\{-\frac{(y -  \mu)^2}{2 \sigma^2} \right\},
$$ {#eq-normal-formula}

dove $\mu \in \mathbb{R}$ e $\sigma > 0$ sono i parametri della distribuzione.

La densità normale è unimodale e simmetrica con una caratteristica forma a campana e con il punto di massima densità in corrispondenza di $\mu$.

Il significato dei parametri $\mu$ e $\sigma$ che appaiono nell'eq. {eq}`eq-normal-formula` viene chiarito dalla dimostrazione che

$$
\mathbb{E}(Y) = \mu, \qquad \mathbb{V}(Y) = \sigma^2.
$$

La rappresentazione grafica di quattro densità Normali con medie -1, -0.5, 0, 1 e con deviazioni standard 0.25, 0.5, 1 e 2 è fornita nella figura seguente.

```{r}
# Definire l'intervallo di x
x <- seq(-5, 6, by = 0.001)

# Parametri della distribuzione normale
mus <- c(-1.0, -0.5, 0.0, 1.0)
sigmas <- c(0.25, 0.5, 1, 2)

# Creare un data frame per tutte le combinazioni di mu e sigma
data <- do.call(rbind, lapply(1:length(mus), function(i) {
  data.frame(
    x = x,
    f_x = dnorm(x, mean = mus[i], sd = sigmas[i]),
    mu = mus[i],
    sigma = sigmas[i]
  )
}))

# Grafico
ggplot(data, aes(x = x, y = f_x, color = factor(mu), linetype = factor(sigma))) +
  geom_line(size = 1) +
  labs(
    x = "x",
    y = "f(x)",
    color = expression(mu),
    linetype = expression(sigma),
    title = "Distribuzioni Normali con Diversi Parametri"
  ) +
  theme(legend.position = "top")
```

### Concentrazione

È istruttivo osservare il grado di concentrazione della distribuzione Normale attorno alla media:

$$
\begin{align}
P(\mu - \sigma < Y < \mu + \sigma) &= P (-1 < Z < 1) \simeq 0.683, \notag\\
P(\mu - 2\sigma < Y < \mu + 2\sigma) &= P (-2 < Z < 2) \simeq 0.956, \notag\\
P(\mu - 3\sigma < Y < \mu + 3\sigma) &= P (-3 < Z < 3) \simeq 0.997. \notag
\end{align}
$$

Si noti come un dato la cui distanza dalla media è superiore a 3 volte la deviazione standard presenti un carattere di eccezionalità perché meno del 0.3% dei dati della distribuzione Normale presentano questa caratteristica.

Per indicare la distribuzione Normale si usa la notazione $\mathcal{N}(\mu, \sigma)$.

### Funzione di ripartizione

Il valore della funzione di ripartizione di $Y$ nel punto $y$ è l'area sottesa alla curva di densità $f(y)$ nella semiretta $(-\infty, y]$. Non esiste alcuna funzione elementare per la funzione di ripartizione

$$
F(y) = \int_{-\infty}^y {1 \over {\sigma\sqrt{2\pi} }} \exp \left\{-\frac{(y - \mu)^2}{2\sigma^2} \right\} dy, 
$$ (eq-gaussian-rip-formula)

pertanto le probabilità $P(Y < y)$ vengono calcolate mediante integrazione numerica approssimata. I valori della funzione di ripartizione di una variabile casuale Normale sono dunque forniti da un software.

Ecco l'equivalente in R utilizzando le funzioni per la distribuzione normale e il pacchetto `ggplot2` per i grafici.

### Generazione di Valori Casuali

In R, la funzione `rnorm()` genera valori casuali dalla distribuzione normale. Ad esempio, per ottenere un singolo valore casuale dalla $\mathcal{N}(100, 15)$:

```{r}
# Generare un singolo valore casuale
set.seed(123)  # Per la riproducibilità
rnorm(1, mean = 100, sd = 15)
```

Per estrarre 10 valori casuali dalla stessa distribuzione:

```{r}
# Generare 10 valori casuali
set.seed(123)
qi <- rnorm(10, mean = 100, sd = 15)
print(qi)
```

### Funzione di Ripartizione (CDF)

Per calcolare la probabilità che un'osservazione casuale abbia un valore minore o uguale a 115, utilizziamo `pnorm()`:

```{r}
# Probabilità che X <= 115
pnorm(115, mean = 100, sd = 15)
```

### Visualizzazione dell'Area Sottesa alla Funzione di Densità

Possiamo visualizzare l'area sottesa utilizzando `ggplot2`:

```{r}
# Parametri
mu <- 100
sigma <- 15

# Intervallo di x
x <- seq(mu - 3 * sigma, mu + 3 * sigma, length.out = 1000)

# Densità
fx <- dnorm(x, mean = mu, sd = sigma)

# Grafico
ggplot(data.frame(x, fx), aes(x = x, y = fx)) +
  geom_line(color = "blue") +
  geom_area(
    data = subset(data.frame(x, fx), x <= 115), 
    aes(x = x, y = fx), fill = "gray", alpha = 0.5
  ) +
  labs(title = "Funzione di Densità Normale", x = "x", y = "f(x)") 
```

### Calcolo dell'Integrale con `integrate`

Possiamo calcolare l'area sotto la curva manualmente utilizzando la funzione `integrate`:

```{r}
# Definizione della funzione gaussiana
gaussian <- function(x, mu, sigma) {
  (1 / (sqrt(2 * pi) * sigma)) * exp(-((x - mu)^2) / (2 * sigma^2))
}

# Calcolo dell'area
result <- integrate(gaussian, lower = -Inf, upper = 115, mu = 100, sigma = 15)
print(paste("Il risultato è", result$value, "con errore", result$abs.error))
```

### Proporzione di Valori Maggiori di 130

Calcoliamo $P(X > 130)$ utilizzando il complementare della funzione di ripartizione:

```{r}
# Probabilità che X > 130
1 - pnorm(130, mean = 100, sd = 15)
```

Possiamo anche utilizzare la funzione di sopravvivenza `1 - pnorm()`:

```{r}
# Funzione di sopravvivenza
pnorm(130, mean = 100, sd = 15, lower.tail = FALSE)
```

Visualizzazione:

```{r}
ggplot(data.frame(x, fx), aes(x = x, y = fx)) +
  geom_line(color = "blue") +
  geom_area(
    data = subset(data.frame(x, fx), x >= 130), 
    aes(x = x, y = fx), fill = "gray", alpha = 0.5
  ) +
  labs(title = "Area Sottesa per X >= 130", x = "x", y = "f(x)") 
```

### Funzione di Quantile (PPF)

La funzione `qnorm()` restituisce il quantile della distribuzione normale. Ad esempio:

```{r}
# Quantile corrispondente al 97.725%
qnorm(1 - 0.022750131948179195, mean = 100, sd = 15)
```

In conclusione, le funzioni `rnorm`, `dnorm`, `pnorm`, e `qnorm` in R forniscono gli strumenti necessari per manipolare la distribuzione normale.

### Distribuzione Normale standard

La distribuzione Normale di parametri $\mu = 0$ e $\sigma = 1$ viene detta *distribuzione Normale standard*. La famiglia Normale è l'insieme avente come elementi tutte le distribuzioni Normali con parametri $\mu$ e $\sigma$ diversi. Tutte le distribuzioni Normali si ottengono dalla Normale standard mediante una trasformazione lineare: se $Y \sim \mathcal{N}(\mu_Y, \sigma_Y)$ allora

$$
X = a + b Y \sim \mathcal{N}(\mu_X = a+b \mu_Y, \sigma_X = \left|b\right|\sigma_Y).
$$

L'area sottesa alla curva di densità di $\mathcal{N}(\mu, \sigma)$ nella semiretta $(-\infty, y]$ è uguale all'area sottesa alla densità Normale standard nella semiretta $(-\infty, z]$, in cui $z = (y -\mu_Y )/\sigma_Y$ è il punteggio standard di $Y$. Per la simmetria della distribuzione, l'area sottesa nella semiretta $[1, \infty)$ è uguale all'area sottesa nella semiretta $(-\infty, 1]$ e quest'ultima coincide con $F(-1)$. Analogamente, l'area sottesa nell'intervallo $[y_a, y_b]$, con $y_a < y_b$, è pari a $F(z_b) - F(z_a)$, dove $z_a$ e $z_b$ sono i punteggi standard di $y_a$ e $y_b$.

Si ha anche il problema inverso rispetto a quello del calcolo delle aree: dato un numero $0 \leq p \leq 1$, il problema è quello di determinare un numero $z \in \mathbb{R}$ tale che $P(Z < z) = p$. Il valore $z$ cercato è detto *quantile* di ordine $p$ della Normale standard e può essere trovato mediante un software.

Supponiamo che l'altezza degli individui adulti segua la distribuzione Normale di media $\mu = 1.7$ m e deviazione standard $\sigma = 0.1$ m. Vogliamo sapere la proporzione di individui adulti con un'altezza compresa tra $1.7$ e $1.8$ m.

Il problema ci chiede di trovare l'area sottesa alla distribuzione $\mathcal{N}(\mu = 1.7, \sigma = 0.1)$ nell'intervallo $[1.7, 1.8]$:

```{r}
# Parametri della distribuzione
mu <- 1.7
sigma <- 0.1

# Calcolare la probabilità cumulativa
prob <- pnorm(1.8, mean = mu, sd = sigma) - pnorm(1.7, mean = mu, sd = sigma)
print(prob)
```

```{r}
# Generare dati
x <- seq(mu - 3 * sigma, mu + 3 * sigma, length.out = 1000)
fx <- dnorm(x, mean = mu, sd = sigma)

# Creare il grafico
ggplot(data.frame(x, fx), aes(x = x, y = fx)) +
  geom_line(color = "blue") +
  geom_area(data = subset(data.frame(x, fx), x >= 1.7 & x <= 1.8), 
            aes(x = x, y = fx), fill = "gray", alpha = 0.5) +
  labs(title = "Funzione di Densità Normale", 
       x = "Altezza (m)", 
       y = "Densità")
```

In maniera equivalente, possiamo standardizzare i valori che delimitano l'intervallo considerato e utilizzare la funzione di ripartizione della normale standardizzata. I limiti inferiore e superiore dell'intervallo sono

$$
z_{\text{inf}} = \frac{1.7 - 1.7}{0.1} = 0, \quad z_{\text{sup}} = \frac{1.8 - 1.7}{0.1} = 1.0,
$$

quindi otteniamo

```{r}
# Standardizzazione
z_inf <- (1.7 - mu) / sigma
z_sup <- (1.8 - mu) / sigma

# Calcolo con la normale standardizzata
prob_standard <- pnorm(z_sup, mean = 0, sd = 1) - pnorm(z_inf, mean = 0, sd = 1)
print(prob_standard)
```

Il modo più semplice per risolvere questo problema resta comunque quello di rendersi conto che la probabilità richiesta non è altro che la metà dell'area sottesa dalle distribuzioni Normali nell'intervallo $[\mu - \sigma, \mu + \sigma]$, ovvero $0.683/2$.

Consideriamo ora la visualizzazione della PDF, la CDF e l'inverso della CDF della distribuzione normale.

```{r}
# Parametri della distribuzione
mu <- 100
sigma <- 15

# Generare intervalli di valori
x <- seq(mu - 3 * sigma, mu + 3 * sigma, length.out = 1000)
probabilities <- seq(0.01, 0.99, length.out = 100)

# Calcolo delle funzioni
pdf <- dnorm(x, mean = mu, sd = sigma)
cdf <- pnorm(x, mean = mu, sd = sigma)
ppf <- qnorm(probabilities, mean = mu, sd = sigma)

# Creare i grafici con ggplot2
library(gridExtra)

# Grafico della PDF
pdf_plot <- ggplot(data.frame(x, pdf), aes(x = x, y = pdf)) +
  geom_line(color = "blue") +
  labs(title = "PDF", x = "Valori", y = "Probabilità")

# Grafico della CDF
cdf_plot <- ggplot(data.frame(x, cdf), aes(x = x, y = cdf)) +
  geom_line(color = "orange") +
  labs(title = "CDF", x = "Valori", y = "Cumulativa")

# Grafico dell'inversa della CDF
ppf_plot <- ggplot(data.frame(Probabilità = probabilities, Valori = ppf), aes(x = Probabilità, y = Valori)) +
  geom_line(color = "green") +
  labs(title = "Inverse CDF", x = "Probabilità", y = "Valori") 

# Mostrare i grafici
grid.arrange(pdf_plot, cdf_plot, ppf_plot, ncol = 3)
```

Dovrebbe essere chiaro dalla figura che queste sono tre diverse modalità di osservare la stessa informazione.

## Distribuzione Chi-quadrato

Dalla Normale deriva la distribuzione $\chi^2$. La distribuzione $\chi^2_{~k}$ con $k$ gradi di libertà descrive la variabile casuale

$$
Z_1^2 + Z_2^2 + \dots + Z_k^2,
$$

dove $Z_1, Z_2, \dots, Z_k$ sono variabili casuali i.i.d. che seguono la distribuzione Normale standard $\mathcal{N}(0, 1)$. La variabile casuale chi-quadrato dipende dal parametro intero positivo $\nu = k$ che ne identifica il numero di gradi di libertà. La densità di probabilità di $\chi^2_{~\nu}$ è

$$
f(x) = C_{\nu} x^{\nu/2-1} \exp (-x/2), \qquad \text{se } x > 0,
$$

dove $C_{\nu}$ è una costante positiva.

### Grafico delle Distribuzioni Chi-Quadrato per Vari Valori di $\nu$

In R, utilizziamo `dchisq()` per calcolare la funzione di densità della distribuzione chi-quadrato e `ggplot2` per creare il grafico.

```{r}
# Intervallo di x
x <- seq(0, 40, by = 0.1)

# Valori di gradi di libertà
nus <- c(2, 4, 8, 16)

# Creazione del data frame per il grafico
data <- do.call(rbind, lapply(nus, function(nu) {
  data.frame(x = x, f_x = dchisq(x, df = nu), nu = as.factor(nu))
}))

# Grafico
ggplot(data, aes(x = x, y = f_x, color = nu)) +
  geom_line(size = 1) +
  labs(
    title = "Distribuzioni Chi-Quadrato per Diversi Valori di \u03bd",
    x = "x",
    y = "f(x)",
    color = expression(nu)
  ) 
```

### Proprietà della Distribuzione Chi-Quadrato

1. **Asimmetria**: La distribuzione $\chi^2_{\nu}$ è asimmetrica.
2. **Media**: Il valore atteso di una variabile $\chi^2_{\nu}$ è uguale a $\nu$.
3. **Varianza**: La varianza è pari a $2\nu$.
4. **Convergenza**: Per $k \to \infty$, $\chi^2_{\nu} \to \mathcal{N}(\nu, 2\nu)$.
5. **Somma di variabili**: La somma di variabili $\chi^2_{\nu}$ indipendenti con gradi di libertà diversi segue una distribuzione $\chi^2_{m}$, dove $m$ è la somma dei gradi di libertà.

### Esempio con $\chi^2_5$

Densità della Distribuzione $\chi^2_5$

```{r}
# Parametri
df <- 5
x <- seq(0, 20, length.out = 200)

# Calcolare la densità
pdf <- dchisq(x, df = df)

# Grafico
ggplot(data.frame(x, pdf), aes(x = x, y = pdf)) +
  geom_line(color = "blue", size = 1) +
  labs(
    title = "Distribuzione Chi-Quadrato (\u03bd=5)",
    x = "x",
    y = "PDF"
  ) 
```

### Generazione di Valori Casuali

In R, utilizziamo `rchisq()` per generare valori casuali dalla distribuzione chi-quadrato.

```{r}
# Generare 1.000.000 di valori casuali
set.seed(123)  # Per riproducibilità
x_samples <- rchisq(1000000, df = df)

# Mostrare i primi 20 valori
head(x_samples, 20)
```

### Calcolo della Media

La media teorica della distribuzione chi-quadrato è uguale a $\nu$. Verifichiamo empiricamente:

```{r}
# Calcolare la media
mean(x_samples)
```

### Calcolo della Varianza

La varianza teorica della distribuzione chi-quadrato è $2\nu$. Verifichiamo empiricamente:

```{r}
# Calcolare la varianza
var(x_samples)
```

In conclusione,

- la distribuzione chi-quadrato è asimmetrica e converge alla distribuzione normale per valori elevati di $\nu$,
- la media e la varianza empiriche dei valori generati sono vicine ai valori teorici, verificando le proprietà della distribuzione.


## Distribuzione $t$ di Student

Dalle distribuzioni Normale e Chi-quadrato deriva un'altra distribuzione molto nota, la $t$ di Student. Se $Z \sim \mathcal{N}(0, 1)$ e $W \sim \chi^2_{\nu}$ sono due variabili casuali indipendenti, allora il rapporto

$$
T = \frac{Z}{\Big( \frac{W}{\nu}\Big)^{\frac{1}{2}}}
$$

definisce la distribuzione $t$ di Student con $\nu$ gradi di libertà. Si usa scrivere $T \sim t_{\nu}$. L'andamento della distribuzione $t$ di Student è simile a quello della distribuzione Normale, ma ha una dispersione maggiore (ha le code più pesanti di una Normale, ovvero ha una varianza maggiore di 1).

La seguente mostra alcune distribuzioni $t$ di Student variando il parametro $\nu$.

```{r}
x <- seq(-5, 5, by = 0.1)
nus <- c(1, 2, 5, 30)

df <- data.frame(
  x = rep(x, length(nus) + 1),
  density = c(
    sapply(nus, function(nu) dt(x, df = nu)), dnorm(x, mean = 0, sd = 1)
  ),
  distribution = factor(
    rep(c(paste0("t (ν = ", nus, ")"), "N(μ = 0, σ = 1)"), each = length(x))
  )
)

ggplot(df, aes(x = x, y = density, color = distribution)) +
  geom_line(size = 1) +
  labs(
    x = "x", 
    y = "f(x)", 
    title = "Distribuzione t di Student e Normale"
  ) +
  theme(legend.position = "top")
```

### Proprietà

La variabile casuale $t$ di Student soddisfa le seguenti proprietà:

1.  Per $\nu \rightarrow \infty$, $t_{\nu}$ tende alla normale standard $\mathcal{N}(0, 1)$.
2.  La densità della $t_{\nu}$ è una funzione simmetrica con valore atteso nullo.
3.  Per $\nu > 2$, la varianza della $t_{\nu}$ vale $\nu / (\nu - 2)$; pertanto è sempre maggiore di 1 e tende a 1 per $\nu \rightarrow \infty$.

Calcoliamo il valore della funzione di ripartizione di ordine 0.025 nel caso di una $t_{30}$:

```{r}
qt(0.025, df = 30)
```

Aumentiamo i gradi di libertà ($\nu$ = 1000):

```{r}
qt(0.025, df = 1000)
```

Questo valore è quasi identico a quello della Normale standardizzata:

```{r}
qnorm(0.025, mean = 0, sd = 1)
```

La ragione per cui il quantile della distribuzione $t$ con $\nu = 30$ è maggiore (in valore assoluto) del quantile omotetico della distribuzione Normale Standard è che la distribuzione $t$ ha una varianza maggiore rispetto alla distribuzione Normale Standard.


## Generazione di Campioni

In R, possiamo generare campioni da diverse distribuzioni utilizzando le funzioni `rnorm`, `runif` e `rt`. Ad esempio:

**Distribuzione Normale**:
```r
set.seed(42)  # Per garantire la riproducibilità
media <- 0
deviazione_standard <- 1
campione_normale <- rnorm(100, mean = media, sd = deviazione_standard)
```

**Distribuzione Uniforme**:
```r
a <- 0
b <- 10
campione_uniforme <- runif(100, min = a, max = b)
```

**Distribuzione t di Student**:
```r
gradi_libertà <- 10
campione_t <- rt(100, df = gradi_libertà)
```

---

#### Calcolo della Densità

Possiamo calcolare la densità utilizzando le funzioni `dnorm`, `dunif` e `dt`. Ad esempio:

**Distribuzione Normale**:
```r
x <- seq(media - 4 * deviazione_standard, media + 4 * deviazione_standard, length.out = 100)
pdf_normale <- dnorm(x, mean = media, sd = deviazione_standard)
```

**Distribuzione Uniforme**:
```r
x <- seq(a, b, length.out = 100)
pdf_uniforme <- dunif(x, min = a, max = b)
```

**Distribuzione t di Student**:
```r
x <- seq(-5, 5, length.out = 100)
pdf_t <- dt(x, df = gradi_libertà)
```

---

#### Calcolo dei Quantili

I quantili si calcolano con le funzioni `qnorm`, `qunif` e `qt`. Ad esempio:

**Distribuzione Normale**:
```r
probabilità <- 0.5
quantile_normale <- qnorm(probabilità, mean = media, sd = deviazione_standard)
```

**Distribuzione Uniforme**:
```r
quantile_uniforme <- qunif(probabilità, min = a, max = b)
```

**Distribuzione t di Student**:
```r
quantile_t <- qt(probabilità, df = gradi_libertà)
```

---

#### Calcolo delle Probabilità Cumulate

Le probabilità cumulate si calcolano con le funzioni `pnorm`, `punif` e `pt`. Ad esempio:

**Distribuzione Normale**:
```r
quantile <- 0
probabilità_normale <- pnorm(quantile, mean = media, sd = deviazione_standard)
```

**Distribuzione Uniforme**:
```r
probabilità_uniforme <- punif(quantile, min = a, max = b)
```

**Distribuzione t di Student**:
```r
probabilità_t <- pt(quantile, df = gradi_libertà)
```

---

Con questi strumenti, R consente di generare, visualizzare e analizzare campioni da una vasta gamma di distribuzioni di probabilità, fornendo un potente supporto all'inferenza bayesiana e alla modellazione statistica.

## Esercizi

::: {#exr-cont_rv_distr-1}

Per ciascuna delle distribuzioni di massa di probabilità discusse, utilizza R per:

- creare un grafico della funzione, scegliendo opportunamente i parametri;
- estrarre un campione di 1000 valori casuali dalla distribuzione e visualizzarlo con un istogramma;
- calcolare la media e la deviazione standard dei campioni e confrontarle con i valori teorici attesi;
- stimare l'intervallo centrale del 94% utilizzando i campioni simulati;
- determinare i quantili della distribuzione per gli ordini 0.05, 0.25, 0.75 e 0.95;
- scegliendo un valore della distribuzione pari alla media più una deviazione standard, calcolare la probabilità che la variabile aleatoria assuma un valore minore o uguale a questo valore.

:::

## Informazioni sull'Ambiente di Sviluppo {.unnumbered}

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

