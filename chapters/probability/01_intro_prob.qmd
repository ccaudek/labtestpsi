# Interpretazione della probabilità {#sec-prob-interpretation}

::: callout-important
## In questo capitolo imparerai a

- a comprendere le diverse interpretazioni della probabilità.
:::

::: callout-tip
## Prerequisiti

- Leggere @sec-apx-sets.
:::

::: callout-caution
## Preparazione del Notebook

```{r}
here::here("code", "_common.R") |> 
  source()
```
:::

## Introduzione 

> “La probabilità è il concetto più importante nella scienza moderna, soprattutto considerando che nessuno ha la minima idea di cosa significhi davvero.” (Attribuito a Bertrand Russell, 1929)


Nel corso di questo capitolo, esploreremo varie concezioni della probabilità, tra cui la visione classica, frequentista e bayesiana. Inoltre, introdurremo la simulazione con Python per una migliore comprensione della legge dei grandi numeri, un concetto fondamentale nell'ambito della probabilità. Iniziamo introducendo il concetto di causalità.

### Il Concetto di Casualità e la Teoria della Probabilità

La casualità emerge ogni volta che ci troviamo in una situazione caratterizzata da incertezza, in cui non possiamo prevedere con certezza l’esito di un evento. Questo concetto è fondamentale in molteplici contesti, dai giochi d’azzardo alla ricerca scientifica, e rappresenta un modello che ci aiuta a gestire l’imprevedibilità intrinseca in molti fenomeni. La casualità è il nostro modo di comprendere ciò che è incerto, permettendoci di trattare e quantificare eventi che, pur non potendo essere previsti singolarmente, seguono comunque schemi riconoscibili.

### L’Urna come Modello di Casualità

Un modo semplice ma efficace per rappresentare la casualità è il classico modello dell’urna. Immaginiamo un’urna contenente numerose palline identiche, ciascuna numerata consecutivamente. Supponiamo che ogni pallina abbia la stessa probabilità di essere estratta. Definiremo quindi l’estrazione come "casuale", perché ogni pallina ha uguali possibilità di essere selezionata. In questo contesto, non possiamo anticipare quale pallina verrà estratta, ma sappiamo che ognuna ha la stessa probabilità di esserlo. 

Questo modello apparentemente semplice, basato sull’equivalenza delle probabilità, rappresenta in realtà l’essenza della casualità. Ci consente di estendere questo concetto per spiegare situazioni molto più complesse, dove possiamo applicare il principio della casualità a fenomeni ben oltre l’estrazione di palline, come il comportamento umano o i risultati di un esperimento scientifico.

### Applicazioni del Concetto di Casualità

La casualità trova applicazione in diversi ambiti, e il modello dell’urna offre una base di comprensione per i seguenti contesti:

1. **Giochi d’azzardo**: Per garantire un ambiente "equo" per i giocatori, si cerca di fare in modo che ogni numero o risultato abbia la stessa probabilità di verificarsi. La casualità è qui fondamentale per assicurare che nessun risultato sia predeterminato.

2. **Indagini statistiche**: Nei sondaggi o nelle ricerche demografiche, il campionamento casuale consente di ottenere un campione rappresentativo di una popolazione più ampia, riducendo il rischio di bias di selezione e offrendo inferenze generalizzabili.

3. **Sperimentazione scientifica**: La randomizzazione è utilizzata per distribuire casualmente i partecipanti tra i diversi gruppi sperimentali, permettendo così di controllare variabili confondenti e assicurare che le differenze osservate siano imputabili all’intervento e non ad altri fattori.

4. **Simulazioni**: In vari campi scientifici, come la fisica o la psicologia, i modelli basati sulla casualità permettono di simulare sistemi complessi e di fare previsioni sugli esiti possibili.

### Dalla Casualità alla Teoria della Probabilità

Il concetto di casualità rappresenta il fondamento della teoria della probabilità, che fornisce gli strumenti matematici per quantificare e analizzare rigorosamente l’incertezza. La teoria della probabilità, infatti, consente di trasformare la nostra intuizione della casualità in un modello matematico, attraverso il quale possiamo fare previsioni, calcolare rischi e prendere decisioni in condizioni di incertezza.

In particolare, la teoria della probabilità ci permette di:

1. **Quantificare l’incertezza**: Assegnando un valore numerico a ciascun esito possibile, possiamo esprimere in modo preciso quanto riteniamo probabile ciascun risultato.

2. **Combinare informazioni**: Attraverso regole matematiche come la somma e il prodotto delle probabilità, possiamo calcolare la probabilità di eventi complessi derivati da eventi più semplici.

3. **Aggiornare le credenze**: Quando emergono nuove informazioni, la teoria della probabilità (soprattutto in ambito bayesiano) ci fornisce metodi per aggiornare le nostre stime di probabilità in modo coerente e razionale.

4. **Prendere decisioni informate**: La probabilità ci aiuta a valutare rischi e benefici attesi in situazioni incerte, orientando le nostre scelte in maniera ottimale.

In sintesi, il concetto di casualità e la teoria della probabilità costituiscono strumenti potenti per navigare un mondo intrinsecamente incerto. Forniscono un linguaggio preciso per descrivere l’incertezza e un quadro rigoroso per ragionare su di essa. Comprendere questi concetti è essenziale non solo per matematici o statistici, ma per chiunque desideri prendere decisioni razionali in condizioni di incertezza, che si tratti di ricercatori, psicologi o cittadini comuni.

Nei capitoli seguenti, esploreremo in dettaglio come questi concetti si applicano all’analisi dei dati, con un focus particolare sull’approccio bayesiano. Questo metodo offre un modo naturale e intuitivo di ragionare sull’incertezza, aggiornando progressivamente le conoscenze alla luce di nuove evidenze.

### Storia e Definizioni della Probabilità

La probabilità è un concetto cardine nella matematica e nelle scienze, utilizzato per misurare l’incertezza e studiare fenomeni aleatori. Nel corso del tempo, la sua definizione si è evoluta, passando da intuizioni di tipo qualitativo a formulazioni formali e rigorose.

La probabilità nasce dal bisogno di distinguere gli eventi *deterministici*, il cui esito è prevedibile, da quelli *casuali*, caratterizzati dall'imprevedibilità. Un evento deterministico, almeno in teoria, produce sempre lo stesso risultato nelle stesse condizioni, mentre un evento casuale ha esiti che non possiamo prevedere con certezza. Questa distinzione ha portato alla necessità di quantificare l'incertezza associata agli eventi casuali, utilizzando il concetto di probabilità.

### Fonti dell’Incertezza

L'incertezza nei fenomeni casuali può derivare da due fonti principali:

- **Incertezza epistemica**: Questa forma di incertezza è legata alla nostra conoscenza limitata. Ad esempio, in un esperimento scientifico complesso, la nostra impossibilità di controllare tutte le variabili può introdurre incertezza nei risultati.

- **Incertezza ontologica**: Si riferisce alla casualità intrinseca di alcuni fenomeni, come in fisica quantistica, dove l'indeterminazione sembra essere una caratteristica fondamentale della realtà stessa. Un esempio intuitivo è il lancio di un dado: indipendentemente da quanto conosciamo le condizioni, non possiamo prevedere con assoluta precisione il risultato.

Il fisico danese Niels Bohr ha offerto un’interpretazione illuminante su questo tema: la fisica, secondo Bohr, non mira a rivelare una verità assoluta sulla natura, ma a capire cosa possiamo dire su di essa. Questa visione riconosce che l'incertezza – sia epistemica che ontologica – riflette i limiti del nostro linguaggio e delle nostre conoscenze. Questo approccio si allinea bene con l'interpretazione soggettiva della probabilità, secondo la quale la probabilità rappresenta il grado di fiducia che un individuo ha riguardo al verificarsi di un evento, basata sulle informazioni di cui dispone.

### Assiomatizzazione della Probabilità

Nel 1933, il matematico Andrey Kolmogorov fornì una definizione formale della probabilità, introducendo un sistema assiomatico che costituì la base della moderna teoria della probabilità. Questa formulazione ha trasformato la probabilità in una disciplina matematica rigorosa, offrendo uno strumento essenziale per quantificare l'incertezza in contesti scientifici. Da semplice metodo per analizzare i giochi d’azzardo nel XVII secolo, la probabilità è diventata una pietra miliare del ragionamento scientifico, fornendo un linguaggio universale per descrivere e analizzare l’incertezza in numerosi campi del sapere.

### Interpretazioni Frequentiste e Bayesiane

Le due principali interpretazioni della probabilità sono:

- **Interpretazione frequentista**: In questo approccio, la probabilità di un evento è definita come il limite della frequenza relativa con cui l’evento si verifica in una lunga serie di esperimenti identici. Questa visione oggettiva considera la probabilità come una proprietà intrinseca del fenomeno, indipendente dalle informazioni dell’osservatore.

- **Interpretazione bayesiana**: Al contrario, la probabilità è vista come una credenza soggettiva sul verificarsi di un evento. In questa visione, la probabilità rappresenta il grado di fiducia di un osservatore, dipendente dalle informazioni disponibili e dal contesto. L’approccio bayesiano permette quindi di aggiornare le stime probabilistiche man mano che nuove evidenze vengono acquisite, rendendo la probabilità una misura flessibile della conoscenza.

### La Storia della Probabilità

La probabilità moderna nacque da una domanda posta da Antoine Gombaud (Chevalier de Méré) a Blaise Pascal nel XVII secolo su come dividere equamente le puntate di un gioco d’azzardo interrotto. 

#### Il Problema dei Punti

Il problema può essere riassunto come segue:

> Immaginiamo due persone, A e B, che partecipano a un gioco in cui il primo che vince sei round consecutivi ottiene un premio. Dopo sei round, A ha vinto cinque round e B uno. Poiché il gioco si interrompe prima di assegnare il premio, come dovrebbero dividere il premio in modo equo?

Questa domanda diede origine a una corrispondenza tra Pascal e Fermat, che svilupparono una soluzione matematica basata sulle probabilità di vittoria per ciascun giocatore. Se, per esempio, A aveva una probabilità del 97% di vincere, mentre B una del 3%, sembrava equo assegnare il 97% del premio ad A. La loro corrispondenza ispirò l’opera di Christian Huygens, "De Ratiociniis in Ludo Aleae" (1657), che rimase un riferimento in probabilità per mezzo secolo.

#### Sviluppi Successivi

Nel 1713, Jacob Bernoulli pubblicò postumo "L’Arte della Congettura", introducendo la legge dei grandi numeri e ponendo le basi per l’applicazione della probabilità al di fuori dei giochi d’azzardo, ad esempio nello studio della mortalità e della giustizia penale.

### Interpretazione Classica

La definizione classica di probabilità fu proposta da Pierre-Simon Laplace (1749-1827), che basò il concetto sul calcolo combinatorio. Secondo Laplace, la probabilità di un evento è data dal rapporto tra i casi favorevoli e il numero totale di casi possibili, assumendo che tutti siano equiprobabili. Ad esempio, la probabilità di ottenere un “3” lanciando un dado è $\frac{1}{6}$, poiché solo uno dei sei risultati è favorevole. Tuttavia, questa definizione è limitata, poiché si basa sull’assunzione che ogni evento sia equiprobabile, il che non è sempre vero. Inoltre, è parzialmente circolare, poiché presuppone una conoscenza implicita del concetto di probabilità.

### Interpretazione Frequentista

L’approccio frequentista, nato dalla necessità di evitare le limitazioni dell’interpretazione classica, definisce la probabilità come il limite della frequenza relativa con cui un evento si verifica in una serie infinita di prove. Per esempio, la probabilità di ottenere "testa" in un lancio di moneta può essere stimata come la frequenza relativa di “testa” sul totale dei lanci, quando il numero di lanci tende all’infinito. Questa definizione è utile, ma impraticabile in molte situazioni, poiché richiede un numero infinito di ripetizioni e assume che gli eventi futuri siano identici a quelli passati.

```{r}
#| tags: [hide-input]
coin_flips <- function(n, run_label) {
  # Genera un vettore di 0 e 1 dove 1 rappresenta "testa" e 0 "croce"
  # usando una distribuzione binomiale.
  heads <- rbinom(n, 1, 0.5)
  
  # Calcola la proporzione cumulativa di teste.
  flips <- seq(1, n)
  proportion_heads <- cumsum(heads) / flips
  
  # Crea un data frame per un facile accesso e visualizzazione dei dati.
  df <- data.frame(flips = flips, proportion_heads = proportion_heads, run = run_label)
  
  return(df)
}

n <- 1000

df <- do.call(rbind, lapply(1:4, function(i) coin_flips(n, paste0("run", i))))

ggplot(df, aes(x = flips, y = proportion_heads, color = run)) +
  geom_line()
```

### La Legge dei Grandi Numeri

La simulazione precedente fornisce un esempio della Legge dei grandi numeri. La Legge dei Grandi Numeri afferma che, man mano che il numero di esperimenti casuali ripetuti aumenta, la stima della probabilità di un evento $P(Y=y)$ diventa sempre più accurata.

Il teorema sostiene che, con l'aumento del numero di ripetizioni di un esperimento casuale, la media dei risultati osservati tende a convergere al valore atteso teorico della variabile casuale. In altre parole, la media empirica dei risultati osservati si avvicina sempre di più al valore medio teorico.

Questa legge è cruciale perché garantisce che, con un numero sufficientemente grande di prove, la stima empirica della probabilità di un evento si avvicina al valore reale. Questo rende le stime probabilistiche più precise e affidabili.

Dal punto di vista pratico, la Legge dei Grandi Numeri consente di utilizzare modelli probabilistici per interpretare fenomeni reali. Anche se le osservazioni singole possono variare in modo casuale, la media delle osservazioni su un ampio numero di ripetizioni rifletterà fedelmente le probabilità teoriche.

Formalmente, data una serie di variabili casuali indipendenti $X_1, X_2, \ldots, X_n$, ciascuna con media $\mu$, la Legge dei Grandi Numeri è espressa come:

$$
\lim_{{n \to \infty}} P\left(\left|\frac{X_1 + X_2 + \ldots + X_n}{n} - \mu\right| < \epsilon\right) = 1,
$$

dove $\epsilon$ è un valore positivo arbitrariamente piccolo e $P(\cdot)$ indica la probabilità. Questo significa che, con un numero molto grande di ripetizioni, la media campionaria osservata sarà vicina alla media teorica attesa, permettendo inferenze affidabili sulla probabilità degli eventi.

In sintesi, la Legge dei Grandi Numeri assicura che, aumentando il numero di prove, le stime empiriche delle probabilità diventano sempre più precise, allineandosi con i valori teorici attesi.

#### Problema del caso singolo

Nell'ambito dell'approccio frequentista alla probabilità, basato sulla concezione delle frequenze relative di eventi osservati su lunghe serie di ripetizioni, emerge un limite concettuale nel trattare la probabilità di eventi singolari e non ripetibili. Secondo questa prospettiva, infatti, non risulta rigorosamente appropriato discutere di probabilità relative a eventi unici e non replicabili nel tempo. Esempi emblematici di tali eventi includono la possibilità che Alcaraz vinca contro Djokovic nella finale di Wimbledon del 2023 o che si verifichi pioggia a Firenze il giorno di Ferragosto del 2024. Questi scenari, essendo unici e circoscritti a un preciso momento storico, sfuggono alla logica frequentista che richiede, per definizione, la possibilità di osservazione ripetuta degli eventi per valutarne la probabilità. Nonostante ciò, nel linguaggio comune non specialistico, è comune l'uso del termine "probabilità" per riferirsi anche a tali eventi specifici e non ripetibili, evidenziando così una discrepanza tra l'uso tecnico e quello colloquiale del concetto di probabilità.

### Collegamento tra probabilità e statistica

Durante gli anni '20 del Novecento, Ronald A. Fisher propose un nuovo framework teorico per l'inferenza statistica, basato sulla concettualizzazione della frequenza. Fisher introdusse concetti chiave come la massima verosimiglianza, i test di significatività, i metodi di campionamento, l'analisi della varianza e il disegno sperimentale.

Negli anni '30, Jerzy Neyman ed Egon Pearson fecero ulteriori progressi nel campo con lo sviluppo di una teoria della decisione statistica, basata sul principio della verosimiglianza e sull'interpretazione frequentista della probabilità. Definirono due tipologie di errori decisionali e utilizzarono il test di significatività di Fisher, interpretando i valori-$p$ come indicatori dei tassi di errore a lungo termine.

### La riscoperta dei metodi Monte Carlo Markov chain

Fisher assunse una prospettiva critica nei confronti della "probabilità inversa" (ossia, i metodi bayesiani), nonostante questa fosse stata la metodologia predominante per l'inferenza statistica per quasi un secolo e mezzo. Il suo approccio frequentista ebbe un profondo impatto sullo sviluppo della statistica sia teorica che sperimentale, contribuendo a un decremento nell'utilizzo dell'inferenza basata sul metodo della probabilità inversa, originariamente proposto da Laplace.

Nel 1939, il libro di Harold Jeffreys intitolato "Theory of Probability" rappresentò una delle prime esposizioni moderne dei metodi bayesiani. Tuttavia, la rinascita del framework bayesiano fu rinviata fino alla scoperta dei metodi Monte Carlo Markov chain alla fine degli anni '80. Questi metodi hanno reso fattibile il calcolo di risultati precedentemente non ottenibili, consentendo un rinnovato interesse e sviluppo nei metodi bayesiani. Per una storia dell'approccio bayesiano, si veda [Bayesian Methods: General Background](https://bayes.wustl.edu/etj/articles/general.background.pdf) oppure [Philosophy of Statistics](https://plato.stanford.edu/entries/statistics/).

### Interpretazione soggettivista

L'interpretazione soggettiva della probabilità è vista come una misura della credenza personale, piuttosto che una proprietà intrinseca degli eventi. @de2017theory, un pioniere di questa visione, ha sostenuto radicalmente che la probabilità non ha una realtà oggettiva, affermando nel suo trattato sulla probabilità che "la probabilità non esiste". Per lui, parlare di probabilità oggettiva è come credere in concetti superati come l'"Etere cosmico" o "Fate e Streghe". Secondo de Finetti, le probabilità sono puramente soggettive, rappresentando il grado di convinzione di una persona basato sulle informazioni disponibili al momento.

Questa concezione ha radici storiche nella definizione di probabilità come grado di credenza individuale, introdotta per la prima volta da @ramsey1926truth. Questa nozione di probabilità come credenza soggettiva è stata inizialmente resistente nell'ambito delle idee bayesiane. Una discussione approfondita sugli assiomi della probabilità soggettiva è presentata da @fishburn1986axioms nel suo lavoro [vedi anche @press2009subjective].

La terminologia "soggettiva" associata alla probabilità può suggerire una certa imprecisione o un approccio non scientifico. @lindley2013understanding ha proposto il termine "probabilità personale" per evitare queste connotazioni negative. Allo stesso modo, @howson2006scientific preferiscono il termine "probabilità epistemica", che meglio descrive l'incertezza di un individuo di fronte a situazioni incerte. Il testo di @kaplan2023bayesian adotta questa terminologia, offrendo un linguaggio più neutro per discutere questi concetti.

Inoltre, l'interpretazione soggettiva si adatta particolarmente bene all'analisi di eventi singoli, consentendo di esprimere convinzioni su eventi specifici, come la probabilità di pioggia in un certo giorno o il risultato di una competizione sportiva.

Riassumendo, l'interpretazione soggettiva bayesiana propone che:

> la probabilità sia una misura del grado di fiducia che un soggetto razionale attribuisce alla validità di un'affermazione, basandosi su un insieme di informazioni disponibili, generalmente insufficienti per determinare con certezza la verità o la falsità dell'affermazione stessa.

Questa concezione non si riferisce a un soggetto specifico, bensì a un'idealizzazione di un'intelligenza razionale, priva di emozioni, istinti o altri elementi che potrebbero influenzare il giudizio. Tale idealizzazione consente di astrarre il calcolo probabilistico dalle limitazioni individuali, rendendolo applicabile in un contesto generale e rigoroso.

Facendo riferimento al lavoro di @jaynes2003probability, *Probability Theory: The Logic of Science*, il problema del calcolo della probabilità può essere formulato come segue: si immagini di fornire a un robot un'informazione, indicata con $I$, che è nota e considerata vera dal robot stesso, e un'affermazione, indicata con $A$, che nella realtà è esclusivamente vera o falsa, senza ambiguità. Il compito del robot consiste nel quantificare, nel modo più razionale possibile, il grado di incertezza riguardante la validità di $A$, basandosi esclusivamente sull'informazione $I$.

Questa misura, definita probabilità di $A$ dato $I$ (o condizionata a $I$), deve soddisfare due requisiti fondamentali: essere espressa come un numero reale compreso tra 0 e 1 e rispettare i principi della coerenza logica. La notazione standard per rappresentare tale probabilità è $P(A \mid I)$, dove il simbolo $\mid$ indica la dipendenza della probabilità di $A$ dall'informazione $I$.

::: {.callout-tip}
Per chi desidera approfondire, il primo capitolo del testo *Bernoulli's Fallacy* [@clayton2021bernoulli] offre un'introduzione molto leggibile alle tematiche della definizione della probabilità nella storia della scienza.
:::

## Riflessioni Conclusive

Nel presente capitolo, si è proceduto a un'analisi filosofica della nozione di probabilità, esplorando le sue interpretazioni sia come proprietà intrinseca degli eventi sia come espressione di convinzioni soggettive in condizioni di incertezza.

È stato, inoltre, introdotto il ruolo della simulazione come strumento metodologico fondamentale per l'approssimazione delle probabilità empiriche in contesti nei quali le soluzioni analitiche risultano impraticabili. Questa tecnica si rivela di estrema rilevanza in ambiti di recente sviluppo, dove la complessità dei modelli matematici richiede l'impiego di algoritmi numerici avanzati per la loro elaborazione e comprensione.

Con le premesse sopra esposte, il capitolo successivo sarà dedicato all'analisi matematica della probabilità. Si esaminerà il modo in cui gli statistici formulano e applicano teoremi e leggi probabilistici, estendendo l'applicabilità del concetto di probabilità al di là delle teorizzazioni puramente teoriche, verso implementazioni pratiche. Questo approccio quantitativo permetterà di quantificare e gestire l'incertezza con maggiore precisione e affidabilità.

## Informazioni sull'Ambiente di Sviluppo

```{r}
sessionInfo()
```

## Bibliografia {.unnumbered}

